{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi_string = open('./source/essence_text.txt', 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi_string[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi_string[:800].count('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chr_nums = list(map((lambda chr: ord(chr) - ord('\\n')), midi_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# chr_nums[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_dataset = torch.zeros(len(chr_nums), 129)\n",
    "# full_dataset.data[range(len(chr_nums)), chr_nums] = 1\n",
    "# full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_dataset.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_dataset.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_note = full_dataset[:, 0] != 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_dataset[is_note, :].sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transposed = full_dataset.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pitch_idx = full_dataset[is_note, :].argmax(dim=1)\n",
    "# transposed_idx = pitch_idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transposed.data[is_note, pitch_idx] = 0\n",
    "# transposed.data[is_note, transposed_idx] = 1\n",
    "# transposed.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = nn.LSTM(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(temp.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in temp.parameters():\n",
    "#     if len(param.shape) > 1:\n",
    "#         print(param.shape)\n",
    "#         nn.init.xavier_normal_(param, gain=5/3)\n",
    "# list(temp.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(slice)\n",
    "# print(slice(10))\n",
    "# print(slice(10).indices(5))\n",
    "# print([1,2,3,4,5][None:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SliceTester(object):\n",
    "#     def __getitem__(self, *indices):\n",
    "#         return indices\n",
    "\n",
    "# class SliceTester(object):\n",
    "#     def __getitem__(self, indices):\n",
    "#         return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sl = SliceTester()\n",
    "# print(sl[:5])\n",
    "# # len(sl[:5])\n",
    "# print(sl[:5, 5:10])\n",
    "# print(len(sl[:5, 5:10]))\n",
    "# print(sl[:,:,:,:])\n",
    "# print(len(sl[:,:,:,:]))\n",
    "# print(sl[5])\n",
    "# print(sl[5, 10])\n",
    "# list(sl[:].indices(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.randint(low=-3, high=3, size=(), dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = torch.zeros(1, len(chr_nums), 129)\n",
    "# df.data[0, range(len(chr_nums)), chr_nums] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, sequence_length, total_batch=-1, step_interval=1, random_choice=True, transpose=0):\n",
    "\n",
    "        # invalid argument handling ---------------------------------------------\n",
    "        if not isinstance(sequence_length, int):\n",
    "            raise TypeError(\"value for sequence_length must be an integer\")\n",
    "        if not isinstance(total_batch, int):\n",
    "            raise TypeError(\"value for total_batch must be an integer\")\n",
    "        if not isinstance(step_interval, int):\n",
    "            raise TypeError(\"value for skip_interval must be an integer\")\n",
    "        if not isinstance(random_choice, bool):\n",
    "            raise TypeError(\"value for random_choice must be a boolean\")\n",
    "        if not isinstance(transpose, int) or transpose < 0:\n",
    "            raise ValueError(\"value for transpose must be a non-negative integer\")\n",
    "        # -----------------------------------------------------------------------\n",
    "\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.data_string = open('./source/essence_text.txt', 'r', encoding='utf-8').read()\n",
    "        \n",
    "        # sequence_length check--------------------------------------------------\n",
    "        if sequence_length < 1:\n",
    "            self.sequence_length = len(self.data_string) - 1\n",
    "        else:\n",
    "            self.sequence_length = min(sequence_length, len(self.data_string) - 1)\n",
    "        # -----------------------------------------------------------------------\n",
    "\n",
    "        # total_batch and step_interval check------------------------------------\n",
    "        if total_batch > 0:\n",
    "            self.total_batch = min(total_batch, len(self.data_string) - self.sequence_length)\n",
    "            self.step_interval = int((len(self.data_string) - self.sequence_length) / self.total_batch)\n",
    "        elif step_interval > 1:\n",
    "            self.step_interval = step_interval\n",
    "            if (len(self.data_string) - self.sequence_length) % self.step_interval == 0:\n",
    "                self.total_batch = (len(self.data_string) - self.sequence_length) // self.step_interval\n",
    "            else:\n",
    "                self.total_batch = ((len(self.data_string) - self.sequence_length) // self.step_interval) + 1\n",
    "        else:\n",
    "            self.total_batch = len(self.data_string) - self.sequence_length\n",
    "            self.step_interval = 1\n",
    "        # -----------------------------------------------------------------------\n",
    "            \n",
    "        self.random_choice = random_choice\n",
    "        self.transpose = transpose\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total_batch\n",
    "    \n",
    "    def __getitem__(self, batch_idx):\n",
    "        \n",
    "        # invalid index handling -------------------------------------------------\n",
    "        if not isinstance(batch_idx, int):\n",
    "            raise IndexError(f\"this dataset only takes one integer value as the index, but {type(batch_idx)} was given\")\n",
    "        if not batch_idx < self.total_batch:\n",
    "            raise IndexError(f\"index out of bounds (index > len)\")\n",
    "        if batch_idx < 0:\n",
    "            batch_idx = self.total_batch + batch_idx\n",
    "            if batch_idx < 0:\n",
    "                raise IndexError(f\"index out of bounds (index < -len)\")\n",
    "        # ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        # index selection --------------------------------------------------------\n",
    "        if self.step_interval == 1:\n",
    "            idx = batch_idx\n",
    "        else:\n",
    "            if self.random_choice:\n",
    "                if batch_idx == self.total_batch - 1:\n",
    "                    # This last index might have a wider range than other indices\n",
    "                    idx = torch.randint(low=(self.step_interval * batch_idx),\n",
    "                                        high=(len(self.data_string) - self.sequence_length),\n",
    "                                        size=(),\n",
    "                                        dtype=torch.int).item()\n",
    "                else:\n",
    "                    idx = torch.randint(low=(self.step_interval * batch_idx),\n",
    "                                        high=(self.step_interval * (batch_idx + 1)),\n",
    "                                        size=(),\n",
    "                                        dtype=torch.int).item()\n",
    "            else:\n",
    "                idx = self.step_interval * batch_idx\n",
    "\n",
    "\n",
    "        string = self.data_string[idx : (idx + self.sequence_length + 1)]\n",
    "\n",
    "        input_chr_nums = [ord(chr) - ord('\\n') for chr in string[:-1]]\n",
    "        target_chr_nums = [ord(chr) - ord('\\n') for chr in string[1:]]\n",
    "        # -----------------------------------------------------------------------\n",
    "\n",
    "        # transpose -------------------------------------------------------------\n",
    "        if self.transpose > 0:\n",
    "            min_note = min([chr_num for chr_num in input_chr_nums if chr_num != 0])\n",
    "            lower_margin = min_note - 1\n",
    "            max_note = max([chr_num for chr_num in input_chr_nums if chr_num != 0])\n",
    "            upper_margin = 129 - max_note\n",
    "\n",
    "            # In case the transposed value might go out of bounds\n",
    "            if lower_margin < self.transpose:\n",
    "                if upper_margin < self.transpose:\n",
    "                    low = -lower_margin\n",
    "                    high = upper_margin\n",
    "                else:\n",
    "                    low = lower_margin\n",
    "                    high = min(upper_margin, 2 * self.transpose - lower_margin)\n",
    "            elif upper_margin < self.transpose:\n",
    "                low = -min(upper_margin, 2 * self.transpose - lower_margin)\n",
    "                high = upper_margin\n",
    "            # Else, everything is alright!\n",
    "            else:\n",
    "                low = -self.transpose\n",
    "                high = self.transpose\n",
    "                \n",
    "            transpose_val = torch.randint(low=low, high=high + 1, size=[], dtype=torch.int).item()\n",
    "            input_chr_nums = [chr_num + transpose_val if chr_num != 0 else chr_num for chr_num in input_chr_nums]\n",
    "            target_chr_nums = [chr_num + transpose_val if chr_num != 0 else chr_num for chr_num in target_chr_nums]\n",
    "        # ------------------------------------------------------------------------\n",
    "\n",
    "        input_tensor = torch.zeros(self.sequence_length, 129)\n",
    "        input_tensor[range(self.sequence_length), input_chr_nums] = 1\n",
    "\n",
    "        target_tensor = torch.tensor(target_chr_nums)\n",
    "\n",
    "        return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([73,  0, 54, 73,  0, 54, 73,  0, 54, 73,  0, 54, 73,  0, 54, 73,  0, 54,\n",
       "         73,  0, 54, 73,  0, 54, 73,  0, 54, 73,  0, 54, 73,  0, 54, 73,  0, 54,\n",
       "         73,  0, 54, 73,  0, 54,  0, 54, 85,  0, 54, 85,  0, 54, 85,  0, 54, 85,\n",
       "          0, 54, 85,  0, 54, 85,  0, 54, 85,  0, 54, 85,  0, 54, 85,  0, 54, 85,\n",
       "          0, 54, 85,  0, 54, 85,  0, 54, 85,  0,  0, 58, 80,  0, 58, 80,  0, 58,\n",
       "         80,  0, 58, 80,  0, 58, 80,  0, 58, 80,  0, 58, 80,  0, 58, 80,  0, 58,\n",
       "         80,  0, 58, 80,  0, 58, 80,  0, 58, 80,  0, 58, 80,  0, 58, 80,  0, 58,\n",
       "          0, 58,  0, 58,  0, 58,  0, 58,  0, 58,  0, 58,  0, 58,  0, 58,  0, 58,\n",
       "          0, 58,  0, 58,  0, 58,  0, 58,  0,  0, 57, 78,  0, 57, 78,  0, 57, 78,\n",
       "          0, 57, 78,  0, 57, 78,  0, 57, 78,  0, 57, 78,  0, 57, 78,  0, 57, 78,\n",
       "          0, 57, 78,  0, 57, 78,  0, 57, 78,  0, 57, 78,  0, 57, 78,  0, 57,  0,\n",
       "         57, 85,  0, 57, 85,  0, 57, 85,  0, 57, 85,  0, 57, 85,  0, 57, 85,  0,\n",
       "         57, 85,  0, 57, 85,  0, 57, 85,  0, 57, 85,  0, 57, 85,  0, 57, 85,  0,\n",
       "         57, 85,  0, 85,  0, 56, 73,  0, 56, 73,  0, 56, 73,  0, 56, 73,  0, 56,\n",
       "         73,  0, 56, 73,  0, 56, 73,  0, 56, 73,  0, 56, 73,  0, 56, 73,  0, 56,\n",
       "         73,  0, 56, 73,  0, 56, 73,  0, 56, 73,  0, 56,  0, 56,  0, 56,  0, 56,\n",
       "          0, 56,  0, 56,  0, 56,  0, 56,  0, 56,  0, 56,  0, 56,  0, 56,  0, 56,\n",
       "          0, 56,  0,  0, 54, 73,  0, 54, 73,  0, 54, 73,  0, 54, 73,  0, 54, 73,\n",
       "          0, 54, 73,  0, 54, 73,  0, 54, 73,  0, 54, 73,  0, 54, 73,  0, 54, 73,\n",
       "          0, 54, 73,  0, 54, 73,  0, 54, 73,  0, 54,  0, 54, 78,  0, 54, 78,  0,\n",
       "         54, 78,  0, 54, 78,  0, 54, 78,  0, 54, 78,  0, 54, 78,  0, 54, 78,  0,\n",
       "         54, 78,  0, 54, 78,  0, 54, 78,  0, 54, 78,  0, 54, 78,  0, 78,  0, 58,\n",
       "         85,  0, 58, 85]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MyDataset(400, total_batch=-1, step_interval=1, transpose=0)\n",
    "print(len(dataset))\n",
    "# dataset[:50]\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[73,  0, 54, 73,  0],\n",
       "          [ 0, 49, 68,  0, 49]])], torch.Size([2, 5, 129]), torch.Size([2, 5]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DataLoader(MyDataset(sequence_length=5, total_batch=-1, step_interval=1, transpose=5),\n",
    "                    batch_size=2, shuffle=False, drop_last=False)\n",
    "print(len(loader))\n",
    "(lambda list_of_tensors: (list_of_tensors, list_of_tensors[0].shape, list_of_tensors[1].shape))(next(iter(loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dataload(dataset, length, batch=1):\n",
    "#     max_idx = (len(dataset) - 1) - ((len(dataset) - 1) % length)\n",
    "#     ins = dataset[:max_idx].view(-1, length, 129)\n",
    "#     tar = dataset[1:max_idx + 1].argmax(dim=2).view(-1, length)\n",
    "#     for i in range(0, len(ins) // batch, batch):\n",
    "#         indices = range(i * batch, (i + 1) * batch)\n",
    "#         yield (ins[indices], tar[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list(dataload(dataset, 400, 2))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(129, 129, batch_first=True)\n",
    "        self.out_f = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.lstm(input, hidden)\n",
    "        output = self.out_f(output)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1, cuda=False):\n",
    "        h0 = torch.zeros(1, batch_size, 129)\n",
    "        c0 = torch.zeros(1, batch_size, 129)\n",
    "        \n",
    "        if cuda:\n",
    "            h0 = h0.cuda()\n",
    "            c0 = c0.cuda()\n",
    "        \n",
    "        return (h0, c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm = MyLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = [input_data for input_data, target_data in dataload(dataset, 500, batch=2)]\n",
    "# targets = [target_data for input_data, target_data in dataload(dataset, 500, batch=2)]\n",
    "# print(len(inputs), len(targets))\n",
    "# print(inputs[0].shape, inputs[1].shape)\n",
    "# print(targets[0].shape, targets[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden = lstm.init_hidden(batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(lstm.forward(inputs[0], hidden))\n",
    "# print(lstm.forward(inputs[0], hidden)[0].shape)\n",
    "# print(list(map(lambda tup: list(map(lambda t: t.shape, tup)), lstm.forward(inputs[0], hidden))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = lstm.forward(inputs[0], hidden)[0]\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def manual_loss(output, target, avg_over_seq=True):\n",
    "#     n_batch = output.shape[0]\n",
    "#     l_sequence = output.shape[1]\n",
    "#     total_loss = 0\n",
    "#     if output.shape[:2] == target.shape[:2]:\n",
    "#         for batch in range(n_batch):\n",
    "#             for step in range(l_sequence):\n",
    "#                 total_loss += nn.functional.nll_loss(output[batch, step, :].view(-1, 129).log(), target[batch, step].view(1))\n",
    "#         if avg_over_seq:\n",
    "#             avg_loss = total_loss / (n_batch * l_sequence)\n",
    "#         else:\n",
    "#             avg_loss = total_loss / n_batch\n",
    "        \n",
    "#         return avg_loss\n",
    "#     else:\n",
    "#         print(\"Dimension mismatch\")\n",
    "#         print(output.shape)\n",
    "#         print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual_loss(lstm(inputs[0], hidden)[0], targets[0], avg_over_seq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nn.NLLLoss(reduction='none')(torch.log(lstm.forward(inputs[0], hidden)[0])[:, 0], targets[0][:, 0]))\n",
    "# print(nn.NLLLoss(reduction='sum')(torch.log(lstm.forward(inputs[0], hidden)[0])[:, 0], targets[0][:, 0]))\n",
    "# print(nn.NLLLoss(reduction='elementwise_mean')(torch.log(lstm.forward(inputs[0], hidden)[0])[:, 0], targets[0][:, 0]))\n",
    "# print()\n",
    "# print(nn.NLLLoss(reduction='none')(torch.log(lstm.forward(inputs[0], hidden)[0]).view(-1, 129), targets[0].view(-1)))\n",
    "# print(nn.NLLLoss(reduction='sum')(torch.log(lstm.forward(inputs[0], hidden)[0]).view(-1, 129), targets[0].view(-1)))\n",
    "# print(nn.NLLLoss(reduction='elementwise_mean')(torch.log(lstm.forward(inputs[0], hidden)[0]).view(-1, 129), targets[0].view(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm.forward(inputs[0], hidden)[0].view(-1, 129).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0, batch_first=True):\n",
    "        super(SkipLSTM, self).__init__()\n",
    "        \n",
    "        # Hyperparameters to be kept (Others are used only for initialization)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        \n",
    "        self.layer_norm_0 = nn.LayerNorm(input_size)\n",
    "        \n",
    "        self.lstm_1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=batch_first)\n",
    "        self.layer_norm_1 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout_1 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.lstm_2 = nn.LSTM(input_size=hidden_size + input_size, hidden_size=hidden_size, batch_first=batch_first)\n",
    "        self.layer_norm_2 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout_2 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.lstm_3 = nn.LSTM(input_size=hidden_size + hidden_size, hidden_size=hidden_size, batch_first=batch_first)\n",
    "        self.layer_norm_3 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout_3 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.to_notes = nn.Linear(in_features=hidden_size + hidden_size, out_features=output_size)\n",
    "#         self.activation = nn.Sigmoid()\n",
    "        self.to_out = nn.LogSoftmax(dim=2)\n",
    "\n",
    "#         for param in self.parameters():\n",
    "#             if len(param.shape) == 2:\n",
    "#                 nn.init.xavier_normal_(param)\n",
    " \n",
    "    def forward(self, input, hiddens, temperature=1.0):\n",
    "        assert temperature > 0\n",
    "        \n",
    "        input_norm = self.layer_norm_0(input)\n",
    "        \n",
    "        output_1, hidden_1 = self.lstm_1(input_norm, hiddens[0])\n",
    "        output_1_norm = self.layer_norm_1(output_1)\n",
    "        concat_1 = torch.cat((output_1_norm, input_norm), dim=2)\n",
    "        dropped_1 = self.dropout_1(concat_1)\n",
    "        \n",
    "        output_2, hidden_2 = self.lstm_2(concat_1, hiddens[1])\n",
    "        output_2_norm = self.layer_norm_2(output_2)\n",
    "        concat_2 = torch.cat((output_2_norm, output_1_norm), dim=2)\n",
    "        dropped_2 = self.dropout_2(concat_2)\n",
    "        \n",
    "        output_3, hidden_3 = self.lstm_3(concat_2, hiddens[2])\n",
    "        output_3_norm = self.layer_norm_3(output_3)\n",
    "        concat_3 = torch.cat((output_3_norm, output_2_norm), dim=2)\n",
    "        dropped_3 = self.dropout_3(concat_3)\n",
    "        \n",
    "        output = self.to_notes(concat_3)\n",
    "#         output = self.activation(output)\n",
    "        output = output / temperature\n",
    "        output = self.to_out(output)\n",
    "        \n",
    "        return output, (hidden_1, hidden_2, hidden_3)\n",
    "    \n",
    "    def init_hidden(self, batch_size, cuda=False):\n",
    "        h0_1 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        c0_1 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        h0_2 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        c0_2 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        h0_3 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        c0_3 = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        if cuda:\n",
    "            h0_1 = h0_1.cuda()\n",
    "            c0_1 = c0_1.cuda()\n",
    "            h0_2 = h0_2.cuda()\n",
    "            c0_2 = c0_2.cuda()\n",
    "            h0_3 = h0_3.cuda()\n",
    "            c0_3 = c0_3.cuda()\n",
    "\n",
    "        return ((h0_1, c0_1), (h0_2, c0_2), (h0_3, c0_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProperLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0, batch_first=True):\n",
    "        super(ProperLSTM, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, batch_first=batch_first)\n",
    "        self.to_notes = nn.Linear(in_features=hidden_size, out_features=output_size)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.to_out = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            if len(param.shape) == 2:\n",
    "                nn.init.xavier_normal_(param)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.lstm(input, hidden)\n",
    "        output = self.to_notes(output)\n",
    "        output = self.activation(output)\n",
    "        output = self.to_out(output)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, cuda=False):\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        if cuda:\n",
    "            h0 = h0.cuda()\n",
    "            c0 = c0.cuda()\n",
    "        \n",
    "        return (h0, c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_lstm = ProperLSTM(input_size=129, hidden_size=300, output_size=129, num_layers=3, batch_first=True)\n",
    "my_lstm = SkipLSTM(input_size=129, hidden_size=300, output_size=129, dropout=0.5, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan)\n",
      "\n",
      "tensor(0.)\n",
      "\n",
      "tensor(inf)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(nn.functional.nll_loss(my_lstm(MyDataset(1600)[0][0].unsqueeze(dim=0), my_lstm.init_hidden(1))[0].view(-1, 129).log(), MyDataset(1600)[0][1].view(-1)))\n",
    "    # print(manual_loss(my_lstm(inputs[0], my_lstm.init_hidden(2))[0], targets[0]))\n",
    "\n",
    "    print()\n",
    "    temp = (torch.zeros(1,129, dtype=torch.float), torch.tensor([50]))\n",
    "    temp[0].data[0, 50] = 1.\n",
    "    print(nn.functional.nll_loss(temp[0].log(), temp[1]))\n",
    "\n",
    "    print()\n",
    "    temp[0].data[0, 50] = 0.\n",
    "    temp[0].data[0, 51] = 1.\n",
    "    print(nn.functional.nll_loss(temp[0].log(), temp[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(my_lstm.parameters(), lr=1.0)\n",
    "# optimizer = optim.SGD(my_lstm.parameters(), lr=0.01, momentum=0.9, weight_decay=1.0, nesterov=True)\n",
    "# optimizer = optim.RMSprop(my_lstm.parameters(), lr=0.05, weight_decay=0, momentum=0.9, centered=False)\n",
    "optimizer = optim.Adam(my_lstm.parameters(), lr=0.001, weight_decay=0.5, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights = torch.ones(129)\n",
    "# loss_weights.data[0] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 6019 : 4.859124183654785\n",
      "Overall Accuracy : 30.269%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6020 : 4.859155654907227\n",
      "Overall Accuracy : 28.250%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6021 : 4.859206199645996\n",
      "Overall Accuracy : 29.222%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6022 : 4.859200477600098\n",
      "Overall Accuracy : 30.041%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6023 : 4.859216213226318\n",
      "Overall Accuracy : 29.891%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6024 : 4.859172344207764\n",
      "Overall Accuracy : 24.116%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6025 : 4.859299182891846\n",
      "Overall Accuracy : 26.022%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6026 : 4.859194278717041\n",
      "Overall Accuracy : 25.222%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6027 : 4.859189987182617\n",
      "Overall Accuracy : 29.016%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6028 : 4.859191417694092\n",
      "Overall Accuracy : 26.234%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6029 : 4.859251499176025\n",
      "Overall Accuracy : 29.053%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6030 : 4.859131813049316\n",
      "Overall Accuracy : 29.741%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6031 : 4.859242916107178\n",
      "Overall Accuracy : 28.247%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6032 : 4.859193801879883\n",
      "Overall Accuracy : 26.813%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6033 : 4.859212398529053\n",
      "Overall Accuracy : 26.022%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6034 : 4.859167575836182\n",
      "Overall Accuracy : 26.941%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6035 : 4.859217643737793\n",
      "Overall Accuracy : 28.256%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6036 : 4.859163284301758\n",
      "Overall Accuracy : 28.253%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6037 : 4.859226226806641\n",
      "Overall Accuracy : 26.466%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6038 : 4.859256267547607\n",
      "Overall Accuracy : 26.969%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6039 : 4.859176158905029\n",
      "Overall Accuracy : 31.238%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6040 : 4.859175682067871\n",
      "Overall Accuracy : 27.816%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6041 : 4.859264373779297\n",
      "Overall Accuracy : 27.619%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6042 : 4.859261989593506\n",
      "Overall Accuracy : 26.188%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6043 : 4.859175205230713\n",
      "Overall Accuracy : 28.513%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6044 : 4.859200477600098\n",
      "Overall Accuracy : 27.434%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6045 : 4.859208583831787\n",
      "Overall Accuracy : 27.944%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6046 : 4.859187126159668\n",
      "Overall Accuracy : 25.291%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6047 : 4.859203815460205\n",
      "Overall Accuracy : 29.528%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6048 : 4.8591132164001465\n",
      "Overall Accuracy : 28.919%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6049 : 4.859213352203369\n",
      "Overall Accuracy : 27.538%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6050 : 4.859254837036133\n",
      "Overall Accuracy : 29.000%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6051 : 4.859164237976074\n",
      "Overall Accuracy : 27.194%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6052 : 4.8592000007629395\n",
      "Overall Accuracy : 28.738%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6053 : 4.859251976013184\n",
      "Overall Accuracy : 26.075%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6054 : 4.859245777130127\n",
      "Overall Accuracy : 26.647%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6055 : 4.859186172485352\n",
      "Overall Accuracy : 28.097%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6056 : 4.859243392944336\n",
      "Overall Accuracy : 29.016%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6057 : 4.85917329788208\n",
      "Overall Accuracy : 25.503%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6058 : 4.859177589416504\n",
      "Overall Accuracy : 26.053%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6059 : 4.859149932861328\n",
      "Overall Accuracy : 25.619%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6060 : 4.859227180480957\n",
      "Overall Accuracy : 29.544%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6061 : 4.859236240386963\n",
      "Overall Accuracy : 28.244%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6062 : 4.859199047088623\n",
      "Overall Accuracy : 27.194%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6063 : 4.85923433303833\n",
      "Overall Accuracy : 27.944%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6064 : 4.859164714813232\n",
      "Overall Accuracy : 28.691%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6065 : 4.859171390533447\n",
      "Overall Accuracy : 29.722%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6066 : 4.859207630157471\n",
      "Overall Accuracy : 27.288%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6067 : 4.859239101409912\n",
      "Overall Accuracy : 27.331%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6068 : 4.859129428863525\n",
      "Overall Accuracy : 26.144%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6069 : 4.859179973602295\n",
      "Overall Accuracy : 32.516%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6070 : 4.8591508865356445\n",
      "Overall Accuracy : 29.391%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6071 : 4.859206199645996\n",
      "Overall Accuracy : 26.203%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6072 : 4.859199523925781\n",
      "Overall Accuracy : 26.925%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6073 : 4.859243392944336\n",
      "Overall Accuracy : 27.178%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6074 : 4.859172344207764\n",
      "Overall Accuracy : 26.478%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6075 : 4.859222412109375\n",
      "Overall Accuracy : 27.191%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6076 : 4.8592095375061035\n",
      "Overall Accuracy : 27.966%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6077 : 4.859222412109375\n",
      "Overall Accuracy : 27.875%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6078 : 4.8592305183410645\n",
      "Overall Accuracy : 27.313%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6079 : 4.859187602996826\n",
      "Overall Accuracy : 28.984%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6080 : 4.859177589416504\n",
      "Overall Accuracy : 32.613%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6081 : 4.859256267547607\n",
      "Overall Accuracy : 26.569%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6082 : 4.859183311462402\n",
      "Overall Accuracy : 27.069%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6083 : 4.859195709228516\n",
      "Overall Accuracy : 28.681%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6084 : 4.859214782714844\n",
      "Overall Accuracy : 28.238%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6085 : 4.8591437339782715\n",
      "Overall Accuracy : 29.391%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6086 : 4.859221935272217\n",
      "Overall Accuracy : 28.150%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6087 : 4.859194278717041\n",
      "Overall Accuracy : 30.144%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6088 : 4.859208583831787\n",
      "Overall Accuracy : 29.197%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6089 : 4.85922384262085\n",
      "Overall Accuracy : 26.038%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6090 : 4.859147071838379\n",
      "Overall Accuracy : 29.425%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6091 : 4.859210014343262\n",
      "Overall Accuracy : 28.388%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 6092 : 4.859193801879883\n",
      "Overall Accuracy : 32.247%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6093 : 4.859257698059082\n",
      "Overall Accuracy : 29.769%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6094 : 4.859178066253662\n",
      "Overall Accuracy : 25.141%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6095 : 4.8591742515563965\n",
      "Overall Accuracy : 28.803%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6096 : 4.859167575836182\n",
      "Overall Accuracy : 25.716%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6097 : 4.8592000007629395\n",
      "Overall Accuracy : 26.119%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6098 : 4.859256744384766\n",
      "Overall Accuracy : 28.569%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6099 : 4.85922384262085\n",
      "Overall Accuracy : 29.966%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6100 : 4.8592400550842285\n",
      "Overall Accuracy : 27.156%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6101 : 4.859235763549805\n",
      "Overall Accuracy : 26.859%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6102 : 4.8591084480285645\n",
      "Overall Accuracy : 27.313%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6103 : 4.859247207641602\n",
      "Overall Accuracy : 26.019%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6104 : 4.859182834625244\n",
      "Overall Accuracy : 27.837%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6105 : 4.859281539916992\n",
      "Overall Accuracy : 28.481%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6106 : 4.859122276306152\n",
      "Overall Accuracy : 29.241%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6107 : 4.859158515930176\n",
      "Overall Accuracy : 27.119%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6108 : 4.859243392944336\n",
      "Overall Accuracy : 29.922%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6109 : 4.859209060668945\n",
      "Overall Accuracy : 25.884%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6110 : 4.859189510345459\n",
      "Overall Accuracy : 30.278%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6111 : 4.859198093414307\n",
      "Overall Accuracy : 27.906%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6112 : 4.859220504760742\n",
      "Overall Accuracy : 27.403%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6113 : 4.859156608581543\n",
      "Overall Accuracy : 27.634%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6114 : 4.859217166900635\n",
      "Overall Accuracy : 28.391%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6115 : 4.859148025512695\n",
      "Overall Accuracy : 25.434%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6116 : 4.859219074249268\n",
      "Overall Accuracy : 28.259%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6117 : 4.859189987182617\n",
      "Overall Accuracy : 25.875%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6118 : 4.8591766357421875\n",
      "Overall Accuracy : 31.263%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6119 : 4.859227180480957\n",
      "Overall Accuracy : 27.009%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6120 : 4.859153747558594\n",
      "Overall Accuracy : 28.922%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6121 : 4.859265327453613\n",
      "Overall Accuracy : 29.250%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6122 : 4.85915470123291\n",
      "Overall Accuracy : 26.094%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6123 : 4.859152317047119\n",
      "Overall Accuracy : 28.322%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6124 : 4.8591694831848145\n",
      "Overall Accuracy : 27.522%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6125 : 4.859185218811035\n",
      "Overall Accuracy : 27.353%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6126 : 4.859199523925781\n",
      "Overall Accuracy : 26.613%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6127 : 4.8592071533203125\n",
      "Overall Accuracy : 27.375%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6128 : 4.859241962432861\n",
      "Overall Accuracy : 25.331%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6129 : 4.859250545501709\n",
      "Overall Accuracy : 26.934%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6130 : 4.859205722808838\n",
      "Overall Accuracy : 28.725%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6131 : 4.859200477600098\n",
      "Overall Accuracy : 29.031%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6132 : 4.859198093414307\n",
      "Overall Accuracy : 24.247%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6133 : 4.859193325042725\n",
      "Overall Accuracy : 27.844%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6134 : 4.859162330627441\n",
      "Overall Accuracy : 27.069%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6135 : 4.859203815460205\n",
      "Overall Accuracy : 26.028%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6136 : 4.859196186065674\n",
      "Overall Accuracy : 27.884%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6137 : 4.859163761138916\n",
      "Overall Accuracy : 25.313%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6138 : 4.859248161315918\n",
      "Overall Accuracy : 26.928%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6139 : 4.859174728393555\n",
      "Overall Accuracy : 27.222%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6140 : 4.859204292297363\n",
      "Overall Accuracy : 22.488%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6141 : 4.859220504760742\n",
      "Overall Accuracy : 24.509%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6142 : 4.859198570251465\n",
      "Overall Accuracy : 28.628%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6143 : 4.859224319458008\n",
      "Overall Accuracy : 28.653%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6144 : 4.8592023849487305\n",
      "Overall Accuracy : 29.731%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6145 : 4.859212398529053\n",
      "Overall Accuracy : 28.913%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6146 : 4.859169960021973\n",
      "Overall Accuracy : 26.847%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6147 : 4.859206199645996\n",
      "Overall Accuracy : 27.081%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6148 : 4.859246253967285\n",
      "Overall Accuracy : 27.875%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6149 : 4.8591227531433105\n",
      "Overall Accuracy : 27.119%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6150 : 4.859241008758545\n",
      "Overall Accuracy : 29.119%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6151 : 4.859191417694092\n",
      "Overall Accuracy : 29.825%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6152 : 4.859255790710449\n",
      "Overall Accuracy : 26.122%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6153 : 4.859180450439453\n",
      "Overall Accuracy : 28.044%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6154 : 4.859218597412109\n",
      "Overall Accuracy : 28.831%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6155 : 4.859203815460205\n",
      "Overall Accuracy : 25.944%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6156 : 4.859192371368408\n",
      "Overall Accuracy : 26.253%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6157 : 4.85916805267334\n",
      "Overall Accuracy : 26.531%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6158 : 4.859251976013184\n",
      "Overall Accuracy : 27.238%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6159 : 4.8591837882995605\n",
      "Overall Accuracy : 27.359%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6160 : 4.859256267547607\n",
      "Overall Accuracy : 25.616%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6161 : 4.859191417694092\n",
      "Overall Accuracy : 29.775%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6162 : 4.8592753410339355\n",
      "Overall Accuracy : 27.466%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6163 : 4.859203338623047\n",
      "Overall Accuracy : 28.084%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6164 : 4.859170913696289\n",
      "Overall Accuracy : 29.375%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 6165 : 4.859234809875488\n",
      "Overall Accuracy : 27.334%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6166 : 4.859205722808838\n",
      "Overall Accuracy : 29.222%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6167 : 4.85921573638916\n",
      "Overall Accuracy : 26.503%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6168 : 4.859196662902832\n",
      "Overall Accuracy : 25.291%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6169 : 4.859262943267822\n",
      "Overall Accuracy : 27.250%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6170 : 4.859163284301758\n",
      "Overall Accuracy : 30.763%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6171 : 4.85922384262085\n",
      "Overall Accuracy : 26.778%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6172 : 4.859165191650391\n",
      "Overall Accuracy : 27.813%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6173 : 4.859229564666748\n",
      "Overall Accuracy : 24.719%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6174 : 4.859201431274414\n",
      "Overall Accuracy : 25.272%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6175 : 4.859261989593506\n",
      "Overall Accuracy : 31.219%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6176 : 4.859161853790283\n",
      "Overall Accuracy : 25.984%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6177 : 4.8592753410339355\n",
      "Overall Accuracy : 27.428%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6178 : 4.859229564666748\n",
      "Overall Accuracy : 24.463%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6179 : 4.859228134155273\n",
      "Overall Accuracy : 24.666%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6180 : 4.859177589416504\n",
      "Overall Accuracy : 28.647%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6181 : 4.859222412109375\n",
      "Overall Accuracy : 24.078%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6182 : 4.859165191650391\n",
      "Overall Accuracy : 26.203%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6183 : 4.859196186065674\n",
      "Overall Accuracy : 26.831%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6184 : 4.859177112579346\n",
      "Overall Accuracy : 32.078%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6185 : 4.859207630157471\n",
      "Overall Accuracy : 27.469%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6186 : 4.859203338623047\n",
      "Overall Accuracy : 27.356%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6187 : 4.859198570251465\n",
      "Overall Accuracy : 26.472%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6188 : 4.859216213226318\n",
      "Overall Accuracy : 25.794%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6189 : 4.859228134155273\n",
      "Overall Accuracy : 28.428%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6190 : 4.859248161315918\n",
      "Overall Accuracy : 25.869%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6191 : 4.859153747558594\n",
      "Overall Accuracy : 24.541%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6192 : 4.85922384262085\n",
      "Overall Accuracy : 28.397%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6193 : 4.859206199645996\n",
      "Overall Accuracy : 28.309%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6194 : 4.85917854309082\n",
      "Overall Accuracy : 24.350%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6195 : 4.85925817489624\n",
      "Overall Accuracy : 26.706%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6196 : 4.859173774719238\n",
      "Overall Accuracy : 28.606%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6197 : 4.859219551086426\n",
      "Overall Accuracy : 30.572%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n",
      "Loss at epoch 6198 : 4.859165191650391\n",
      "Overall Accuracy : 26.572%\n",
      "Note Accuracy (Except for timesteps) : 0.000%\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-e6ffecb31aaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m#         sequence = sequence.view(1, sequence_length, 129)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;31m#         target = target.view(1, sequence_length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0msequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mmy_lstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_lstm.train()\n",
    "my_lstm.cuda()\n",
    "\n",
    "loss_func = nn.NLLLoss(weight=loss_weights.cuda())\n",
    "\n",
    "learning_rate = 0.2\n",
    "optimizer.param_groups[0]['lr'] = learning_rate\n",
    "\n",
    "batch_size = 16\n",
    "sequence_length = 2000\n",
    "transpose = 7\n",
    "\n",
    "loader = DataLoader(MyDataset(sequence_length=sequence_length, total_batch=-1, step_interval=sequence_length, transpose=transpose),\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    drop_last=True)\n",
    "# short_data = MyDataset(sequence_length=1, total_batch=1, step_interval=1, random_choice=False, transpose=0)[0]\n",
    "# print(short_data[0], short_data[1])\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(loader) // 2)\n",
    "\n",
    "# best_loss = {'epoch': epoch, 'loss': 9999, 'recall': 0.0, 'acc': 0.0, 'state_dict': None, 'opt_dict': None}\n",
    "# best_recall = {'epoch': epoch, 'loss': 9999, 'recall': 0.0, 'acc': 0.0, 'state_dict': None, 'opt_dict': None}\n",
    "# best_acc = {'epoch': epoch, 'loss': 9999, 'recall': 0.0, 'acc': 0.0, 'state_dict': None, 'opt_dict': None}\n",
    "best_loss = torch.load('./3best_loss.pth', map_location='cuda')\n",
    "best_recall = torch.load('./3best_recall.pth', map_location='cuda')\n",
    "best_acc = torch.load('./3best_acc.pth', map_location='cuda')\n",
    "\n",
    "# n_epoch = 500\n",
    "# max_epoch = epoch + n_epoch\n",
    "\n",
    "# while epoch < max_epoch:\n",
    "#     for sequence, target in dataload(dataset, length=500, batch=batch_size):\n",
    "#         my_lstm.zero_grad()\n",
    "#         output, _ = my_lstm(sequence.cuda(), my_lstm.init_hidden(batch_size=batch_size, cuda=True))\n",
    "#         loss = loss_func(output.view(-1, 129).log(), target.view(-1).cuda())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(f\"Loss at epoch {epoch} : {loss}\")\n",
    "#     epoch += 1\n",
    "\n",
    "train_duration = 3600 * 8 + 900 # in seconds\n",
    "start_time = time()\n",
    "\n",
    "while time() - start_time < train_duration:\n",
    "    for sequence, target in loader:\n",
    "#     for sequence, target in (short_data,):\n",
    "#         sequence = sequence.view(1, sequence_length, 129)\n",
    "#         target = target.view(1, sequence_length)\n",
    "        sequence = sequence.cuda()\n",
    "        target = target.cuda()\n",
    "        my_lstm.zero_grad()\n",
    "        output = my_lstm(sequence, my_lstm.init_hidden(batch_size=batch_size, cuda=True))[0]\n",
    "        loss = loss_func(output.view(-1, 129), target.view(-1))\n",
    "        if not torch.isfinite(loss):\n",
    "            break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "    if not torch.isfinite(loss):\n",
    "        print(\"Loss Exploded!\")\n",
    "        break\n",
    "    overall_acc = (output.argmax(dim=2) == target).to(torch.float).mean().item()\n",
    "    acc = ((output.argmax(dim=2) == target) & (target != 0)).to(torch.float).mean().item()\n",
    "    with open('./train_log.txt', 'a') as log:\n",
    "        log.write(f\"Loss at epoch {epoch} : {loss}\\n\")\n",
    "        log.write(f\"Overall Accuracy : {overall_acc * 100:.3f}%\\n\")\n",
    "        log.write(f\"Note Accuracy (Except for timesteps) : {acc * 100:.3f}%\\n\\n\")\n",
    "    print(f\"Loss at epoch {epoch} : {loss}\")\n",
    "    print(f\"Overall Accuracy : {overall_acc * 100:.3f}%\")\n",
    "    print(f\"Note Accuracy (Except for timesteps) : {acc * 100:.3f}%\\n\")\n",
    "    \n",
    "    if loss < best_loss['loss']:\n",
    "        best_loss = {'epoch': epoch, 'loss': loss.item(), 'recall': acc, 'acc': overall_acc, 'state_dict': my_lstm.state_dict(), 'opt_dict': optimizer.state_dict}# , 'lr_dict': lr_scheduler.state_dict()}\n",
    "        torch.save(best_loss, './3best_loss.pth')\n",
    "    if acc > best_recall['recall']:\n",
    "        best_recall = {'epoch': epoch, 'loss': loss.item(), 'recall': acc, 'acc': overall_acc, 'state_dict': my_lstm.state_dict(), 'opt_dict': optimizer.state_dict}# , 'lr_dict': lr_scheduler.state_dict()}\n",
    "        torch.save(best_recall, './3best_recall.pth')\n",
    "    if overall_acc > best_acc['acc']:\n",
    "        best_acc = {'epoch': epoch, 'loss': loss.item(), 'recall': acc, 'acc': overall_acc, 'state_dict': my_lstm.state_dict(), 'opt_dict': optimizer.state_dict}# , 'lr_dict': lr_scheduler.state_dict()}\n",
    "        torch.save(best_acc, './3best_acc.pth')\n",
    "        \n",
    "    epoch += 1\n",
    "\n",
    "\n",
    "my_lstm.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 5617,\n",
       " 'loss': 3.8840205669403076,\n",
       " 'recall': 0.633809506893158,\n",
       " 'acc': 0.8883809447288513,\n",
       " 'state_dict': OrderedDict([('layer_norm_0.weight',\n",
       "               tensor([0.9376, 0.9211, 0.9197, 0.9209, 0.9194, 0.9191, 0.9204, 0.9202, 0.9197,\n",
       "                       0.9203, 0.9197, 0.9197, 0.9213, 0.9192, 0.9197, 0.9198, 0.9201, 0.9186,\n",
       "                       0.9201, 1.1771, 1.1676, 1.2862, 1.3071, 1.3103, 1.3289, 1.3573, 1.2834,\n",
       "                       1.2631, 1.2652, 1.2184, 1.2577, 1.1763, 1.2060, 1.1810, 1.1863, 1.1479,\n",
       "                       1.1786, 1.1539, 1.1146, 1.1040, 1.1189, 1.0869, 1.1486, 1.0739, 1.0943,\n",
       "                       1.1018, 1.0869, 1.0967, 1.0863, 1.0649, 1.0706, 1.1000, 1.1254, 1.0878,\n",
       "                       1.1235, 1.1177, 1.1070, 1.1432, 1.0984, 1.0547, 1.0867, 1.1031, 1.1141,\n",
       "                       1.0679, 1.0898, 1.0910, 1.0698, 1.0645, 1.0540, 1.0756, 1.0822, 1.0532,\n",
       "                       1.0463, 1.0586, 1.0357, 1.0992, 1.0544, 1.0512, 1.0561, 1.0807, 1.0954,\n",
       "                       1.0851, 1.0759, 1.1271, 1.1116, 1.1247, 1.1230, 1.1610, 1.1467, 1.1716,\n",
       "                       1.1514, 1.1808, 1.1661, 1.1857, 1.2108, 1.2249, 1.2274, 1.3047, 1.2688,\n",
       "                       1.2636, 1.2907, 1.2087, 1.1172, 1.1089, 1.0215, 0.9907, 0.9471, 0.9428,\n",
       "                       0.9510, 0.9197, 0.9205, 0.9192, 0.9200, 0.9190, 0.9198, 0.9201, 0.9197,\n",
       "                       0.9198, 0.9225, 0.9199, 0.9197, 0.9201, 0.9203, 0.9195, 0.9203, 0.9217,\n",
       "                       0.9194, 0.9209, 0.9196], device='cuda:0')),\n",
       "              ('layer_norm_0.bias',\n",
       "               tensor([-0.3393,  0.0790,  0.0804,  0.0791,  0.0806,  0.0809,  0.0797,  0.0798,\n",
       "                        0.0804,  0.0797,  0.0804,  0.0803,  0.0788,  0.0809,  0.0803,  0.0802,\n",
       "                        0.0799,  0.0815,  0.0799,  0.0918,  0.0862,  0.0996,  0.0929,  0.0976,\n",
       "                        0.0947,  0.0992,  0.0882,  0.0826,  0.0777,  0.0718,  0.0693,  0.0774,\n",
       "                        0.0600,  0.0667,  0.0606,  0.0541,  0.0548,  0.0476,  0.0318,  0.0432,\n",
       "                        0.0260,  0.0186,  0.0273,  0.0537,  0.0507,  0.0385,  0.0567,  0.0600,\n",
       "                        0.0324,  0.0390,  0.0199,  0.0411,  0.0335,  0.0394,  0.0299,  0.0521,\n",
       "                        0.0411,  0.0480,  0.0523,  0.0400,  0.0477,  0.0534,  0.0265,  0.0493,\n",
       "                        0.0430,  0.0512,  0.0465,  0.0512,  0.0610,  0.0491,  0.0439,  0.0659,\n",
       "                        0.0336,  0.0559,  0.0469,  0.0355,  0.0517,  0.0592,  0.0440,  0.0669,\n",
       "                        0.0783,  0.0752,  0.0537,  0.0749,  0.0781,  0.0813,  0.0804,  0.0770,\n",
       "                        0.0867,  0.0848,  0.0858,  0.0736,  0.0761,  0.0877,  0.0961,  0.0968,\n",
       "                        0.0976,  0.0976,  0.1002,  0.0848,  0.1043,  0.0977,  0.0872,  0.0911,\n",
       "                        0.0874,  0.0845,  0.0829,  0.0813,  0.0822,  0.0803,  0.0796,  0.0809,\n",
       "                        0.0800,  0.0811,  0.0802,  0.0799,  0.0803,  0.0803,  0.0775,  0.0801,\n",
       "                        0.0803,  0.0800,  0.0797,  0.0805,  0.0797,  0.0783,  0.0807,  0.0792,\n",
       "                        0.0804], device='cuda:0')),\n",
       "              ('lstm_1.weight_ih_l0',\n",
       "               tensor([[-0.0376, -0.0671, -0.0773,  ..., -0.0951, -0.0233, -0.1327],\n",
       "                       [-0.0037,  0.0481,  0.0670,  ...,  0.0407,  0.0363,  0.0603],\n",
       "                       [-0.0406,  0.0868,  0.0111,  ...,  0.0382,  0.0329,  0.0660],\n",
       "                       ...,\n",
       "                       [-0.0648,  0.0453, -0.0944,  ..., -0.0501, -0.0366,  0.0348],\n",
       "                       [ 0.0895, -0.0263,  0.0247,  ..., -0.0060, -0.0359,  0.0407],\n",
       "                       [ 0.1072, -0.0283,  0.0808,  ...,  0.0089,  0.0616,  0.0414]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_1.weight_hh_l0',\n",
       "               tensor([[ 0.1150, -0.2518, -0.1578,  ..., -0.0978,  0.0331, -0.0126],\n",
       "                       [-0.1107, -0.1586,  0.1512,  ..., -0.1026, -0.2214,  0.0198],\n",
       "                       [ 0.0995,  0.1143, -0.1046,  ...,  0.0898, -0.0679,  0.1485],\n",
       "                       ...,\n",
       "                       [-0.1777,  0.0009, -0.0351,  ..., -0.0530, -0.0178, -0.0064],\n",
       "                       [-0.1252, -0.1572,  0.0882,  ...,  0.0906, -0.0397,  0.0420],\n",
       "                       [-0.1123, -0.0123,  0.0758,  ...,  0.2054,  0.0717, -0.1316]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_1.bias_ih_l0',\n",
       "               tensor([ 0.0873, -0.0975,  0.0013,  ...,  0.0218,  0.0525, -0.0103], device='cuda:0')),\n",
       "              ('lstm_1.bias_hh_l0',\n",
       "               tensor([ 0.0608, -0.0705,  0.0155,  ...,  0.0627,  0.0501, -0.0790], device='cuda:0')),\n",
       "              ('layer_norm_1.weight',\n",
       "               tensor([0.8859, 0.8484, 0.8763, 0.9666, 0.8303, 0.8703, 0.9128, 0.8749, 0.9352,\n",
       "                       0.8354, 0.8434, 0.8683, 0.9076, 0.9420, 0.9295, 0.9062, 0.8873, 0.8969,\n",
       "                       0.8421, 0.8698, 0.9224, 0.8768, 0.8907, 0.8555, 0.9027, 0.8925, 0.8672,\n",
       "                       0.8886, 0.8508, 0.8606, 0.8990, 0.8692, 0.8567, 0.9703, 0.8575, 0.9312,\n",
       "                       0.9600, 0.8721, 0.8868, 0.9913, 0.9054, 0.9101, 0.8436, 0.9243, 0.8893,\n",
       "                       0.9157, 0.8323, 0.8532, 0.8528, 0.8926, 0.8602, 0.9041, 0.9527, 0.8684,\n",
       "                       0.8850, 0.8710, 0.8589, 0.9132, 0.8679, 0.8572, 0.9052, 0.8604, 0.9260,\n",
       "                       0.9024, 0.8479, 0.9499, 0.9570, 0.8473, 0.9964, 0.8772, 0.8946, 0.8449,\n",
       "                       0.9735, 0.8555, 0.9160, 0.9226, 0.8958, 0.9776, 0.8607, 0.8906, 0.9030,\n",
       "                       0.9410, 0.8931, 0.9196, 0.9112, 0.8419, 0.8602, 0.8930, 0.9112, 0.8718,\n",
       "                       0.8868, 0.8456, 0.9080, 0.9038, 0.9623, 0.9349, 0.9282, 0.9480, 0.9684,\n",
       "                       0.9334, 0.8949, 0.8606, 0.8781, 0.8876, 0.9114, 0.9189, 0.8948, 0.9441,\n",
       "                       0.9532, 0.9657, 0.8522, 0.8943, 0.8652, 0.8493, 0.9274, 0.9281, 0.8571,\n",
       "                       0.8367, 0.9058, 1.0641, 0.9283, 0.9306, 0.8635, 0.9305, 0.9213, 0.9778,\n",
       "                       0.9667, 0.9172, 0.8838, 0.9185, 0.9068, 0.8884, 0.8816, 0.8960, 0.9504,\n",
       "                       0.8536, 0.8855, 0.9179, 0.9152, 0.8967, 0.8492, 0.9019, 0.8832, 0.8718,\n",
       "                       0.8499, 0.8943, 0.8914, 0.9662, 0.7975, 0.9870, 0.9205, 0.9360, 0.9148,\n",
       "                       0.9287, 0.9436, 0.9111, 0.9379, 0.8611, 0.8810, 0.8639, 0.9577, 0.9027,\n",
       "                       0.8921, 0.8590, 0.8936, 0.9447, 0.8926, 0.8014, 0.9116, 0.9161, 0.9076,\n",
       "                       0.8470, 0.8889, 0.7821, 0.9184, 0.8523, 1.0039, 0.9547, 0.8375, 1.0153,\n",
       "                       0.8866, 0.8848, 0.8766, 0.8983, 0.9676, 0.9227, 0.8978, 0.9481, 0.8893,\n",
       "                       0.9177, 0.9326, 0.9973, 0.8909, 0.9173, 0.9226, 0.8402, 0.8911, 0.9426,\n",
       "                       0.9454, 0.9842, 0.9176, 0.8921, 0.8844, 0.9236, 0.9389, 0.8854, 0.8742,\n",
       "                       0.8335, 0.9211, 0.9495, 0.9564, 0.8850, 0.9335, 0.8943, 0.9018, 0.8386,\n",
       "                       0.9078, 0.8760, 0.8773, 0.9875, 0.8443, 0.9014, 0.9009, 0.8619, 0.9166,\n",
       "                       0.8852, 0.8463, 0.8408, 0.9491, 0.9513, 0.9863, 0.8914, 0.8429, 0.9115,\n",
       "                       0.9626, 0.8796, 0.8962, 0.9773, 0.9173, 0.8833, 0.8485, 0.8582, 0.9890,\n",
       "                       0.8182, 0.8911, 0.8807, 0.9130, 0.8040, 0.9230, 0.9266, 0.8889, 0.8344,\n",
       "                       0.8692, 0.9420, 0.9408, 0.9032, 0.8653, 0.9147, 0.9284, 0.8408, 1.0022,\n",
       "                       0.8133, 0.8929, 0.9033, 0.8762, 0.8555, 0.9176, 0.8983, 0.8622, 0.9136,\n",
       "                       0.9178, 0.9078, 0.8718, 0.9096, 0.9045, 0.8399, 0.9466, 0.9058, 0.9605,\n",
       "                       0.8741, 0.9198, 0.8829, 0.9366, 0.9332, 0.8570, 0.9438, 0.8838, 0.9264,\n",
       "                       0.9468, 0.9287, 0.8951, 0.9299, 0.8306, 0.8771, 0.8995, 0.9442, 0.9137,\n",
       "                       0.8880, 0.8263, 0.8273], device='cuda:0')),\n",
       "              ('layer_norm_1.bias',\n",
       "               tensor([ 0.0587, -0.0102, -0.0587, -0.0490,  0.0157, -0.0041,  0.0001, -0.0029,\n",
       "                        0.0013, -0.0217, -0.0069,  0.0026, -0.0235, -0.0539,  0.0408,  0.0645,\n",
       "                       -0.0247, -0.0269,  0.0179,  0.0244, -0.0250, -0.0228, -0.0069, -0.0077,\n",
       "                       -0.0370,  0.0134, -0.0241,  0.0088, -0.0421, -0.0008,  0.0500,  0.0892,\n",
       "                        0.0092, -0.0282,  0.0296,  0.0042,  0.0197,  0.0650, -0.0782, -0.0267,\n",
       "                        0.0306,  0.0174,  0.0088,  0.0466, -0.0285,  0.0158, -0.0121, -0.0073,\n",
       "                        0.0727, -0.0098, -0.0114, -0.0096, -0.0202, -0.0219, -0.0775,  0.0568,\n",
       "                        0.0055,  0.0450,  0.0073,  0.0743,  0.0271, -0.0254, -0.0679, -0.0251,\n",
       "                        0.0034, -0.0125, -0.0019,  0.0124,  0.0460,  0.0533,  0.0051, -0.0057,\n",
       "                        0.0703, -0.0138, -0.0120, -0.0279,  0.0147,  0.0100, -0.0031,  0.0302,\n",
       "                        0.0500,  0.0195, -0.0270,  0.0301,  0.0621,  0.0460, -0.0326, -0.0141,\n",
       "                       -0.0473,  0.0204, -0.0825,  0.0478,  0.0057, -0.0045, -0.0207,  0.0244,\n",
       "                       -0.0545,  0.0238,  0.0768, -0.0212, -0.0023,  0.0304,  0.0555, -0.0072,\n",
       "                       -0.0527, -0.0206, -0.0160,  0.0177, -0.0589,  0.0204,  0.0140,  0.0030,\n",
       "                        0.0058, -0.0368, -0.0244, -0.0023, -0.0571,  0.0188, -0.0351,  0.0164,\n",
       "                        0.0324,  0.0201, -0.0135,  0.0293, -0.0203, -0.0360,  0.0208,  0.0572,\n",
       "                        0.0040,  0.0046, -0.0125, -0.0467, -0.0022,  0.0114,  0.0014, -0.0016,\n",
       "                        0.0256,  0.0224,  0.0037, -0.0173,  0.0081,  0.0422, -0.0512,  0.0274,\n",
       "                        0.0121, -0.0212, -0.0069,  0.0038,  0.0396,  0.1417, -0.0108, -0.0371,\n",
       "                       -0.0553,  0.0235, -0.0136, -0.0647,  0.0445,  0.0004,  0.1194,  0.0344,\n",
       "                       -0.0321, -0.0231, -0.0941,  0.0330,  0.0610,  0.0087,  0.0260, -0.0288,\n",
       "                       -0.0255,  0.0232,  0.0167, -0.0051, -0.0113,  0.0051,  0.0017,  0.0448,\n",
       "                       -0.0342, -0.0056, -0.0050,  0.0106, -0.0571,  0.0093, -0.0336,  0.0211,\n",
       "                       -0.0239,  0.0340, -0.0285, -0.0314, -0.0003,  0.0397, -0.0182,  0.0010,\n",
       "                       -0.0162, -0.0519, -0.0296, -0.0192, -0.0209, -0.0504, -0.0142, -0.1107,\n",
       "                        0.0061,  0.0533,  0.0309,  0.0294,  0.0045, -0.0877, -0.0137,  0.0281,\n",
       "                       -0.0193, -0.0131,  0.0102, -0.0559, -0.0140, -0.0568, -0.0128,  0.0231,\n",
       "                        0.0377, -0.0500, -0.0239, -0.0366, -0.0192, -0.0478,  0.0081,  0.0286,\n",
       "                       -0.0144,  0.0111,  0.0216, -0.0110,  0.0235, -0.0475, -0.0216,  0.0030,\n",
       "                        0.0455, -0.0314,  0.0042,  0.0033,  0.0100,  0.0786,  0.0601, -0.0032,\n",
       "                        0.0220,  0.0407,  0.0604,  0.0365, -0.0020,  0.0488, -0.0207, -0.0255,\n",
       "                        0.0270, -0.0850,  0.0378, -0.0298, -0.0078,  0.0494,  0.0342, -0.0365,\n",
       "                       -0.0764,  0.0185,  0.0248,  0.0245,  0.0707,  0.0248, -0.0528,  0.0307,\n",
       "                       -0.0293, -0.0425, -0.0750,  0.0565,  0.0608,  0.0474,  0.0247,  0.0100,\n",
       "                       -0.0114,  0.0032,  0.0395,  0.0087, -0.0371, -0.0313, -0.0058, -0.0067,\n",
       "                        0.0024, -0.0596,  0.0027,  0.0217, -0.0291, -0.0145, -0.0277, -0.0076,\n",
       "                       -0.0527, -0.0356, -0.0630, -0.0380, -0.0142,  0.0791, -0.0298,  0.0246,\n",
       "                       -0.0377, -0.0394, -0.0082, -0.0091], device='cuda:0')),\n",
       "              ('lstm_2.weight_ih_l0',\n",
       "               tensor([[ 0.0413, -0.0491, -0.0529,  ..., -0.0106, -0.0262, -0.0676],\n",
       "                       [-0.0001,  0.0229,  0.0961,  ..., -0.0108,  0.1036,  0.0466],\n",
       "                       [-0.1381, -0.0240, -0.0814,  ...,  0.0448,  0.0233, -0.0558],\n",
       "                       ...,\n",
       "                       [ 0.0159,  0.0075,  0.0589,  ...,  0.0481,  0.0094, -0.0016],\n",
       "                       [-0.0864,  0.0086, -0.0111,  ..., -0.0105,  0.0384,  0.0368],\n",
       "                       [ 0.0752, -0.0475,  0.0699,  ..., -0.0121, -0.0001,  0.0038]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_2.weight_hh_l0',\n",
       "               tensor([[ 0.0971,  0.1841, -0.0141,  ..., -0.1744,  0.0450,  0.1605],\n",
       "                       [ 0.0373, -0.1067, -0.0851,  ..., -0.0315, -0.0966, -0.0718],\n",
       "                       [ 0.0605, -0.0382,  0.1819,  ..., -0.0629,  0.1784, -0.1745],\n",
       "                       ...,\n",
       "                       [-0.1052,  0.1042, -0.1510,  ...,  0.0547, -0.1310, -0.1147],\n",
       "                       [-0.0773, -0.1677, -0.0160,  ..., -0.0712, -0.1392, -0.0906],\n",
       "                       [ 0.0494, -0.1640,  0.0892,  ...,  0.0967,  0.0471,  0.0721]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_2.bias_ih_l0',\n",
       "               tensor([ 0.0259, -0.0254,  0.0298,  ..., -0.0413, -0.0662, -0.0226], device='cuda:0')),\n",
       "              ('lstm_2.bias_hh_l0',\n",
       "               tensor([-0.0556, -0.0904, -0.0234,  ..., -0.0070, -0.0354,  0.0484], device='cuda:0')),\n",
       "              ('layer_norm_2.weight',\n",
       "               tensor([1.3470, 1.3039, 1.2805, 1.2943, 1.2868, 1.3588, 1.2563, 1.2898, 1.2297,\n",
       "                       1.4940, 1.2980, 1.0825, 1.3062, 1.3978, 1.4312, 1.3852, 1.2819, 1.3241,\n",
       "                       1.4594, 1.2000, 1.3314, 1.2018, 1.3173, 1.3290, 1.3077, 1.2480, 1.3130,\n",
       "                       1.4043, 1.3197, 1.4204, 1.2645, 1.4295, 1.2752, 1.3305, 1.2952, 1.2274,\n",
       "                       1.1093, 1.4208, 1.2140, 1.2609, 1.4189, 1.2567, 1.1867, 1.2514, 1.2594,\n",
       "                       1.2869, 1.3304, 1.4057, 1.3416, 1.5157, 1.4201, 1.3403, 1.3654, 1.2107,\n",
       "                       1.4155, 1.2186, 1.2767, 1.4657, 1.4506, 1.4034, 1.2073, 1.2827, 1.3095,\n",
       "                       1.2114, 1.4107, 1.4065, 1.2589, 1.1196, 1.1924, 1.2261, 1.4631, 1.5498,\n",
       "                       1.3053, 1.3871, 1.4107, 1.5021, 1.2003, 1.3757, 1.3300, 1.3149, 1.3025,\n",
       "                       1.4285, 1.6130, 1.1576, 1.0765, 1.3374, 1.2330, 1.0956, 1.2364, 1.1773,\n",
       "                       1.2518, 1.2325, 1.4670, 1.2280, 1.2394, 1.3178, 1.2071, 1.2498, 1.4652,\n",
       "                       1.5192, 1.3617, 1.3704, 1.2226, 1.4209, 1.2458, 1.2852, 1.3234, 1.2936,\n",
       "                       1.4202, 1.1877, 1.2028, 1.3500, 1.3604, 1.2089, 1.3563, 1.2986, 1.3427,\n",
       "                       1.2321, 1.5246, 1.2774, 1.3499, 1.2355, 1.1909, 1.2583, 1.0837, 1.2663,\n",
       "                       1.3171, 1.2879, 1.1753, 1.3747, 1.3873, 1.2942, 1.0978, 1.2166, 1.3700,\n",
       "                       1.3471, 1.4579, 1.3380, 1.1410, 1.1913, 1.5623, 1.2943, 1.1688, 1.2469,\n",
       "                       1.3705, 1.1798, 1.2351, 1.3466, 1.2438, 1.1385, 1.3385, 1.3606, 1.3625,\n",
       "                       1.2842, 1.2658, 1.3390, 1.3749, 1.5207, 1.2861, 1.1565, 1.1921, 1.3378,\n",
       "                       1.3314, 1.2900, 1.3111, 1.2112, 1.3719, 1.3381, 1.3727, 1.2854, 1.2040,\n",
       "                       1.1910, 1.2334, 1.2220, 1.4580, 1.2861, 1.3758, 1.1419, 1.3705, 1.3168,\n",
       "                       1.2994, 1.4914, 1.2129, 1.2816, 1.2499, 1.2273, 1.2935, 1.3905, 1.3873,\n",
       "                       1.3953, 1.3380, 1.3472, 1.2399, 1.2872, 1.4366, 1.3594, 1.2960, 1.1475,\n",
       "                       1.2580, 1.2759, 1.3408, 1.3706, 1.4283, 1.5772, 1.2221, 1.2124, 1.2565,\n",
       "                       1.3508, 1.2664, 1.3678, 1.4191, 1.2331, 1.2912, 1.2585, 1.3433, 1.1970,\n",
       "                       1.3589, 1.2336, 1.2686, 1.2988, 1.1578, 1.2055, 1.1336, 1.3770, 1.4887,\n",
       "                       1.1949, 1.4358, 1.2506, 1.1829, 1.3693, 1.2209, 1.3411, 1.3435, 1.2579,\n",
       "                       1.1816, 1.3374, 1.5409, 1.3519, 1.4401, 1.4699, 1.1804, 1.3279, 1.3722,\n",
       "                       1.4026, 1.3145, 1.2082, 1.3520, 1.3028, 1.1479, 1.2843, 1.3138, 1.2510,\n",
       "                       1.1695, 1.2726, 1.3034, 1.3387, 1.1976, 1.3951, 1.2938, 1.2247, 1.3051,\n",
       "                       1.2622, 1.3800, 1.1731, 1.3312, 1.1071, 1.3871, 1.3006, 1.3289, 1.2432,\n",
       "                       1.4180, 1.5671, 1.4769, 1.3698, 1.2971, 1.2592, 1.3604, 1.2280, 1.3573,\n",
       "                       1.3143, 1.2471, 1.4825, 1.1735, 1.4040, 1.1344, 1.2901, 1.2631, 1.2926,\n",
       "                       1.2925, 1.2036, 1.2924, 1.3078, 1.0670, 1.2409, 1.3363, 1.2693, 1.2360,\n",
       "                       1.3671, 1.2864, 1.3214], device='cuda:0')),\n",
       "              ('layer_norm_2.bias',\n",
       "               tensor([-0.0235, -0.0190, -0.0420, -0.0349,  0.1533,  0.0773, -0.0178, -0.0470,\n",
       "                        0.0268,  0.1461, -0.0176, -0.0555,  0.0572, -0.1280,  0.0990, -0.0411,\n",
       "                       -0.0785, -0.0655, -0.0596,  0.0000,  0.1205, -0.0281, -0.0434, -0.1454,\n",
       "                       -0.0282,  0.0764, -0.1643, -0.1796,  0.0664,  0.1360,  0.0666, -0.0652,\n",
       "                       -0.0694, -0.1486,  0.0324,  0.0746, -0.0199, -0.0175,  0.0616, -0.0626,\n",
       "                       -0.0118,  0.0996, -0.0503, -0.0387, -0.0268,  0.2151, -0.0532, -0.0409,\n",
       "                       -0.1421, -0.1068,  0.0713, -0.1222, -0.0523, -0.0336,  0.0168, -0.0192,\n",
       "                       -0.1525,  0.0885,  0.0847,  0.0608, -0.0569, -0.1032,  0.0924,  0.0047,\n",
       "                        0.1737, -0.0417,  0.0216,  0.0335,  0.0325,  0.0197,  0.0062,  0.1481,\n",
       "                        0.0951, -0.1575, -0.0303, -0.0718,  0.0104, -0.0423,  0.0204,  0.0846,\n",
       "                       -0.0635,  0.0794, -0.0130, -0.0185, -0.0154,  0.0670,  0.0108, -0.0441,\n",
       "                        0.0573,  0.0483,  0.0772,  0.0107, -0.0221,  0.0208, -0.0245, -0.1626,\n",
       "                       -0.0747, -0.0846, -0.1053,  0.1348, -0.1047, -0.0683, -0.0200, -0.1603,\n",
       "                        0.0752,  0.0655,  0.1806,  0.1265, -0.1484, -0.0722,  0.0178,  0.2004,\n",
       "                       -0.0741,  0.1020, -0.1179,  0.1445,  0.0924,  0.0517, -0.0011, -0.0654,\n",
       "                        0.1023, -0.0520,  0.0834,  0.0893,  0.0398, -0.0910,  0.0828,  0.0496,\n",
       "                        0.0050,  0.0981, -0.2303,  0.0827, -0.0320, -0.0302, -0.0807, -0.1066,\n",
       "                       -0.0955,  0.0596,  0.0521, -0.1033,  0.0449, -0.0042, -0.0136,  0.0223,\n",
       "                        0.0312, -0.0278, -0.0139, -0.0012,  0.0712,  0.0143,  0.0056,  0.0951,\n",
       "                       -0.0870, -0.0242,  0.1012, -0.0637, -0.1060, -0.0482, -0.0346,  0.0581,\n",
       "                        0.0527, -0.0481,  0.0561,  0.1149,  0.0048,  0.0189,  0.2164, -0.0731,\n",
       "                        0.1440,  0.0265, -0.0540, -0.0640, -0.1343, -0.0149, -0.2577,  0.0757,\n",
       "                       -0.2030,  0.0194, -0.0877, -0.2282,  0.0016, -0.0685, -0.0630, -0.0053,\n",
       "                        0.0869, -0.0063, -0.1482,  0.0234, -0.0470, -0.0944, -0.0582,  0.0948,\n",
       "                        0.0956,  0.0336, -0.1585, -0.0610,  0.1327, -0.0362,  0.0820,  0.0496,\n",
       "                       -0.1308,  0.0078, -0.0809, -0.0804, -0.0872,  0.0198,  0.0435,  0.0275,\n",
       "                       -0.0387, -0.0380, -0.1143,  0.0125,  0.1268, -0.0663,  0.0672,  0.0305,\n",
       "                        0.1573,  0.0319,  0.1259, -0.0596, -0.0028,  0.0083, -0.0326, -0.1942,\n",
       "                       -0.0641,  0.0454,  0.0108,  0.0687,  0.0150,  0.1016, -0.0668, -0.0027,\n",
       "                        0.1212,  0.0217,  0.0361, -0.1639, -0.0348,  0.1002, -0.0746, -0.0998,\n",
       "                        0.0932,  0.0214,  0.0252, -0.0882,  0.0924, -0.0027,  0.0848,  0.1793,\n",
       "                        0.0354, -0.1722,  0.0313, -0.0887,  0.0189,  0.1821, -0.1051, -0.0458,\n",
       "                       -0.0291,  0.1052, -0.0239,  0.0653, -0.0647, -0.0025, -0.0148, -0.1444,\n",
       "                       -0.1000,  0.0512,  0.0465, -0.1158,  0.0702, -0.0566, -0.1201, -0.0431,\n",
       "                        0.0544,  0.0497,  0.0893,  0.0209, -0.0896, -0.0530, -0.0323,  0.1365,\n",
       "                        0.0410,  0.1885, -0.0093, -0.2409, -0.0882, -0.0296, -0.0709,  0.1090,\n",
       "                       -0.1118,  0.0018, -0.0876,  0.0677, -0.0503, -0.0619, -0.0674,  0.0018,\n",
       "                       -0.1319,  0.0651,  0.0161, -0.0485], device='cuda:0')),\n",
       "              ('lstm_3.weight_ih_l0',\n",
       "               tensor([[-0.0915,  0.0957,  0.0968,  ...,  0.0670,  0.0321, -0.0962],\n",
       "                       [ 0.0110, -0.0739,  0.0684,  ..., -0.0065, -0.1213,  0.0718],\n",
       "                       [ 0.2104, -0.0267,  0.0600,  ..., -0.0427, -0.0127, -0.1891],\n",
       "                       ...,\n",
       "                       [ 0.0452,  0.1038, -0.0694,  ..., -0.0569,  0.1659, -0.1020],\n",
       "                       [-0.0130, -0.0060, -0.0927,  ...,  0.0882, -0.1695, -0.0478],\n",
       "                       [ 0.0569, -0.1679, -0.0105,  ...,  0.1247,  0.1612,  0.0924]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_3.weight_hh_l0',\n",
       "               tensor([[-0.0001,  0.0060,  0.2133,  ...,  0.0509,  0.1289,  0.0924],\n",
       "                       [-0.1293,  0.2083, -0.0876,  ...,  0.1435, -0.0657,  0.1216],\n",
       "                       [-0.1366,  0.0993, -0.1197,  ..., -0.3895, -0.0623, -0.0829],\n",
       "                       ...,\n",
       "                       [-0.1329, -0.1090, -0.1562,  ..., -0.0495,  0.0252,  0.1244],\n",
       "                       [ 0.0006, -0.1849,  0.2989,  ...,  0.0784,  0.0352, -0.1007],\n",
       "                       [ 0.1854, -0.0440, -0.1167,  ...,  0.3650, -0.0791, -0.2948]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_3.bias_ih_l0',\n",
       "               tensor([ 0.0353,  0.0547,  0.0306,  ..., -0.0015,  0.0029,  0.0835], device='cuda:0')),\n",
       "              ('lstm_3.bias_hh_l0',\n",
       "               tensor([-0.0120,  0.0594,  0.0234,  ...,  0.0688, -0.0023,  0.1142], device='cuda:0')),\n",
       "              ('layer_norm_3.weight',\n",
       "               tensor([2.7861, 2.5514, 2.4302, 2.8483, 2.5676, 2.7074, 2.8396, 2.6863, 2.6305,\n",
       "                       2.8595, 2.4934, 2.4019, 2.0648, 2.7659, 2.4996, 2.1447, 2.7062, 2.7424,\n",
       "                       2.2866, 2.3552, 2.5307, 2.6044, 2.7246, 2.2145, 2.7853, 2.4286, 2.9016,\n",
       "                       2.9498, 2.4149, 2.9436, 2.7301, 2.9046, 2.1915, 2.7174, 2.4505, 2.8178,\n",
       "                       2.5756, 2.6821, 2.3747, 2.6375, 2.6259, 2.8109, 2.8568, 2.6569, 2.4529,\n",
       "                       2.8715, 2.3346, 2.8351, 2.8703, 2.2079, 2.4628, 2.8674, 2.5013, 2.7859,\n",
       "                       2.7587, 2.5143, 2.5320, 2.7443, 2.0421, 2.3331, 2.6865, 2.8866, 2.8967,\n",
       "                       2.6575, 2.7481, 2.3788, 2.7513, 2.8510, 2.3803, 2.7850, 2.8744, 2.4730,\n",
       "                       2.0956, 2.6620, 2.9415, 2.4238, 2.7623, 2.3921, 2.7253, 2.7127, 2.6545,\n",
       "                       2.8427, 2.7733, 2.7287, 2.3186, 2.2174, 2.8511, 2.9577, 2.9554, 2.8258,\n",
       "                       1.9439, 2.8278, 2.8250, 2.6144, 2.9830, 2.5979, 2.4581, 2.6351, 2.8014,\n",
       "                       2.8414, 2.8646, 2.5141, 2.4519, 2.9890, 3.1434, 1.7996, 2.9507, 2.6408,\n",
       "                       2.5390, 2.4288, 2.4303, 2.1333, 2.5497, 2.7376, 2.9133, 2.6976, 2.1966,\n",
       "                       2.1106, 2.7597, 2.4135, 2.7652, 2.7433, 2.6122, 2.8202, 2.7481, 2.6851,\n",
       "                       2.6704, 2.4564, 2.9993, 2.2390, 2.8124, 2.6874, 2.2012, 2.8387, 2.6639,\n",
       "                       2.6023, 2.8419, 2.5225, 2.2138, 2.6208, 2.7982, 1.8025, 2.7207, 2.6797,\n",
       "                       2.3863, 2.9074, 2.3260, 2.1764, 2.8818, 2.5521, 2.6519, 2.6356, 2.4827,\n",
       "                       2.1641, 2.9129, 2.6879, 2.7362, 2.6851, 2.4395, 2.6702, 2.1625, 2.7523,\n",
       "                       2.5113, 2.6258, 2.7155, 2.8251, 2.8027, 2.8268, 2.2813, 2.5568, 2.8490,\n",
       "                       2.7060, 2.6493, 2.6274, 2.2370, 2.6711, 2.9359, 2.9496, 2.7954, 2.9587,\n",
       "                       2.5318, 2.5861, 2.3032, 2.4777, 2.9600, 3.0213, 2.3790, 2.9707, 2.7167,\n",
       "                       2.4461, 2.8682, 2.8711, 2.3604, 2.7318, 2.7265, 2.7648, 2.6717, 2.6415,\n",
       "                       2.7635, 2.8551, 2.5950, 2.1198, 2.4046, 2.5460, 1.9237, 2.7184, 3.0670,\n",
       "                       2.5216, 2.7721, 2.7837, 2.6410, 2.9444, 2.7101, 2.6144, 2.4875, 2.6529,\n",
       "                       2.5081, 2.3492, 1.7350, 2.0463, 3.0005, 2.8571, 2.7087, 2.3149, 2.6450,\n",
       "                       2.8518, 2.9827, 2.6732, 2.7150, 2.3248, 2.6461, 2.8895, 2.5755, 3.0193,\n",
       "                       2.7161, 2.2415, 2.6195, 2.4470, 2.5605, 2.7074, 2.9011, 2.3374, 2.6366,\n",
       "                       2.0047, 2.9641, 2.9985, 2.3705, 2.0639, 2.5071, 2.6895, 2.4768, 2.3648,\n",
       "                       2.2792, 2.5808, 2.6110, 2.7465, 2.7537, 2.6257, 2.8361, 2.9409, 2.8487,\n",
       "                       2.8190, 2.7388, 2.7391, 2.4147, 3.0375, 2.4327, 2.5511, 2.8751, 2.4332,\n",
       "                       2.4355, 2.9902, 2.8326, 2.7719, 2.7507, 2.3966, 2.3687, 2.6521, 2.8414,\n",
       "                       2.7678, 2.7329, 2.7867, 2.7446, 2.3473, 2.7673, 2.5924, 3.0393, 2.5775,\n",
       "                       2.8600, 2.6553, 2.6602, 2.3431, 2.4286, 2.7239, 2.8288, 2.9490, 2.2626,\n",
       "                       2.7931, 2.7242, 2.6772], device='cuda:0')),\n",
       "              ('layer_norm_3.bias',\n",
       "               tensor([ 0.1421, -0.5530,  0.0833,  0.8813, -0.1588, -0.4887,  0.0819, -0.0943,\n",
       "                       -0.3901, -0.4041, -0.7882, -0.0659,  0.5999, -0.3974,  0.6469, -0.3405,\n",
       "                        0.5823, -0.0378,  0.6975,  0.5964, -0.1758, -0.4146,  0.8483, -0.2184,\n",
       "                        0.4934,  0.4502,  0.6877, -0.7350, -0.4212, -0.6097, -0.4444, -0.7848,\n",
       "                       -0.3791, -0.7037,  0.3782,  0.3017,  0.1026,  0.4641, -0.2871, -0.2207,\n",
       "                        0.6905, -0.1274, -0.8687, -0.5978, -0.0202, -0.8543,  0.1906, -0.9493,\n",
       "                        0.4122,  0.4465, -0.8148, -0.3514, -0.1951, -0.8474,  0.8312, -0.6149,\n",
       "                        0.6461, -0.8589, -0.3999, -0.5330, -0.4179, -0.5724,  0.7096, -0.8147,\n",
       "                        0.7794, -0.3335,  0.0537, -0.5398,  0.1911,  0.0041, -0.2493, -0.4857,\n",
       "                       -0.2359,  0.1082,  0.8463, -0.3022,  0.4975, -0.5174, -0.0609,  0.6062,\n",
       "                        0.5550, -0.2735,  0.2581,  1.0296, -0.5907,  0.3265, -0.8376,  0.7945,\n",
       "                        0.6094,  0.1272,  0.0976, -0.6673, -0.1339,  0.7326,  0.6974, -0.1618,\n",
       "                        0.7573,  0.4122,  0.3172, -0.1869, -0.5800,  0.0683, -0.5228,  0.7792,\n",
       "                        0.0602, -0.6133,  0.2017,  0.7783,  0.5904, -0.1315,  0.5530,  0.4186,\n",
       "                       -0.2869,  0.3729, -0.5723,  0.3618, -0.8043,  0.5328, -0.2170, -0.2510,\n",
       "                       -0.3504,  0.2232,  0.8814,  0.5014, -0.2550, -0.2875,  0.0966, -0.1874,\n",
       "                       -0.6613,  0.5322, -0.7388, -0.5893,  0.7222,  0.4036, -0.2701,  0.8341,\n",
       "                        0.5704, -0.3235, -0.5607,  0.2710, -0.4461, -0.4832, -0.7098,  0.2781,\n",
       "                        0.7319, -0.6038, -0.6454, -0.1931,  0.5898, -0.6209, -0.6963, -0.6401,\n",
       "                       -0.5743, -0.4715,  0.6721, -0.8502, -0.7337,  0.2714, -0.6088,  0.4824,\n",
       "                       -0.0602,  0.5423, -0.5984,  0.2559, -0.1937, -0.4843,  0.6137, -0.4124,\n",
       "                        1.0327, -0.6489, -0.5436, -0.1877, -0.6400,  0.5778, -0.5707,  0.5100,\n",
       "                        0.7976,  0.8457, -0.6276,  0.8089, -0.0711,  0.2319,  0.3373,  0.7214,\n",
       "                        0.4300,  0.6103, -0.0169, -0.7192, -0.5855,  0.8829, -0.6939,  0.7276,\n",
       "                       -0.5246, -0.2090, -0.3992, -0.9475, -0.0157, -0.2281, -0.8326, -0.8165,\n",
       "                        0.4475, -0.5239, -0.0333,  0.3096, -0.1169, -0.0011, -0.9905,  0.2401,\n",
       "                        0.6606,  0.5786, -0.4115,  0.6316, -0.7368,  0.8459, -0.3054,  0.5985,\n",
       "                       -0.1531,  0.2091,  0.5770,  0.8164,  0.9738, -0.4166,  0.5605, -0.0833,\n",
       "                       -0.7732, -0.0076,  0.7339,  0.7264,  0.1803, -0.6235, -0.6786,  0.0314,\n",
       "                       -0.7121,  1.0247, -0.2936, -0.7464,  0.6406, -0.1209,  0.4463,  0.5647,\n",
       "                        0.7066, -0.8603, -0.7391,  0.7536,  0.7317, -0.7395, -0.7837,  0.8296,\n",
       "                       -0.4457,  0.6112, -0.4681,  0.1066,  0.0443,  0.3376,  0.0264,  0.7634,\n",
       "                       -0.9637,  0.4823,  0.7626, -0.3996,  0.5829,  0.1011,  0.3532,  0.0883,\n",
       "                       -0.0788, -0.6191,  0.2529,  0.3987,  0.7219, -0.7256,  0.5536,  0.7039,\n",
       "                        0.4888,  0.4571, -0.9789, -0.5739,  0.5224, -0.5438, -0.2150,  0.7600,\n",
       "                       -0.5183,  0.2854,  0.2140,  0.7162,  0.0522, -0.0225,  0.4661, -0.4509,\n",
       "                        0.7879, -0.7636,  0.5663,  0.3993, -0.6485, -0.6430, -0.2401,  0.6437,\n",
       "                       -0.3888, -0.0652, -0.5947,  0.7915], device='cuda:0')),\n",
       "              ('to_notes.weight',\n",
       "               tensor([[ 0.3250, -0.3172,  0.0386,  ..., -0.1636, -0.1326, -0.1693],\n",
       "                       [ 0.0098,  0.0943, -0.0536,  ..., -0.0476, -0.0768,  0.0073],\n",
       "                       [-0.0530,  0.0374,  0.0408,  ...,  0.0103,  0.0100,  0.1209],\n",
       "                       ...,\n",
       "                       [-0.0688,  0.0445, -0.0454,  ..., -0.0992, -0.0448,  0.0809],\n",
       "                       [-0.0023,  0.0178, -0.0478,  ..., -0.0490,  0.0286,  0.0735],\n",
       "                       [ 0.0137,  0.0551, -0.0418,  ..., -0.0020,  0.0419,  0.0564]],\n",
       "                      device='cuda:0')),\n",
       "              ('to_notes.bias',\n",
       "               tensor([-0.1185, -0.0514, -0.0669, -0.0486, -0.0343, -0.0842, -0.0602, -0.0904,\n",
       "                       -0.0611, -0.0808, -0.0599, -0.0878, -0.0309, -0.0794, -0.0673, -0.0413,\n",
       "                       -0.0926, -0.1021, -0.0367, -0.0691, -0.0955, -0.1132, -0.1772, -0.1433,\n",
       "                       -0.2078, -0.1783, -0.1872, -0.1771, -0.1955, -0.2086, -0.2120, -0.2578,\n",
       "                       -0.1784, -0.1740, -0.2015, -0.2515, -0.2574, -0.1504, -0.1865, -0.2204,\n",
       "                       -0.2017, -0.2719, -0.1958, -0.2368, -0.1954, -0.2373, -0.2704, -0.2273,\n",
       "                       -0.2284, -0.2854, -0.2400, -0.2821, -0.2268, -0.2332, -0.2244, -0.2677,\n",
       "                       -0.2706, -0.2396, -0.2190, -0.2164, -0.2669, -0.1453, -0.2817, -0.2344,\n",
       "                       -0.2824, -0.2178, -0.2010, -0.2844, -0.2034, -0.2676, -0.1849, -0.2595,\n",
       "                       -0.2227, -0.2579, -0.2402, -0.1974, -0.1868, -0.2530, -0.2783, -0.2378,\n",
       "                       -0.2094, -0.1873, -0.1810, -0.2479, -0.2737, -0.2694, -0.1927, -0.2785,\n",
       "                       -0.3015, -0.3227, -0.2124, -0.2715, -0.2335, -0.3020, -0.1438, -0.1468,\n",
       "                       -0.2289, -0.1491, -0.1544, -0.1560, -0.1743, -0.0849, -0.1594, -0.0422,\n",
       "                       -0.0981, -0.0316, -0.0838, -0.1068, -0.0852, -0.0970, -0.0382, -0.0826,\n",
       "                       -0.0523, -0.0786, -0.0999, -0.0759, -0.1082, -0.0460, -0.0522, -0.0635,\n",
       "                       -0.0871, -0.0923, -0.0529, -0.0722, -0.0558, -0.0426, -0.0593, -0.0872,\n",
       "                       -0.0717], device='cuda:0'))]),\n",
       " 'opt_dict': <bound method Optimizer.state_dict of Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     initial_lr: 0.001\n",
       "     lr: 0.0005000000000000002\n",
       "     weight_decay: 0\n",
       " )>}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 5626,\n",
       " 'loss': 3.8841054439544678,\n",
       " 'recall': 0.6553428173065186,\n",
       " 'acc': 0.8917142748832703,\n",
       " 'state_dict': OrderedDict([('layer_norm_0.weight',\n",
       "               tensor([0.9375, 0.9203, 0.9186, 0.9182, 0.9192, 0.9201, 0.9201, 0.9200, 0.9200,\n",
       "                       0.9204, 0.9193, 0.9204, 0.9195, 0.9199, 0.9189, 0.9182, 0.9199, 0.9186,\n",
       "                       0.9196, 1.1785, 1.1666, 1.2874, 1.3082, 1.3089, 1.3274, 1.3575, 1.2807,\n",
       "                       1.2638, 1.2654, 1.2173, 1.2633, 1.1765, 1.2044, 1.1808, 1.1849, 1.1441,\n",
       "                       1.1773, 1.1537, 1.1165, 1.1004, 1.1180, 1.0843, 1.1471, 1.0708, 1.0931,\n",
       "                       1.1016, 1.0855, 1.0999, 1.0873, 1.0636, 1.0706, 1.1018, 1.1241, 1.0901,\n",
       "                       1.1227, 1.1155, 1.1073, 1.1444, 1.0963, 1.0567, 1.0885, 1.1025, 1.1120,\n",
       "                       1.0671, 1.0897, 1.0905, 1.0718, 1.0613, 1.0537, 1.0718, 1.0800, 1.0522,\n",
       "                       1.0461, 1.0594, 1.0341, 1.0985, 1.0550, 1.0504, 1.0569, 1.0808, 1.0950,\n",
       "                       1.0884, 1.0762, 1.1283, 1.1133, 1.1263, 1.1209, 1.1586, 1.1475, 1.1714,\n",
       "                       1.1485, 1.1812, 1.1628, 1.1865, 1.2066, 1.2253, 1.2277, 1.3039, 1.2708,\n",
       "                       1.2644, 1.2922, 1.2080, 1.1163, 1.1080, 1.0228, 0.9904, 0.9464, 0.9424,\n",
       "                       0.9497, 0.9203, 0.9205, 0.9196, 0.9207, 0.9186, 0.9197, 0.9205, 0.9191,\n",
       "                       0.9196, 0.9227, 0.9202, 0.9188, 0.9201, 0.9202, 0.9199, 0.9198, 0.9217,\n",
       "                       0.9191, 0.9201, 0.9195], device='cuda:0')),\n",
       "              ('layer_norm_0.bias',\n",
       "               tensor([-0.3404,  0.0797,  0.0815,  0.0818,  0.0809,  0.0799,  0.0799,  0.0800,\n",
       "                        0.0800,  0.0796,  0.0808,  0.0796,  0.0806,  0.0801,  0.0812,  0.0819,\n",
       "                        0.0801,  0.0814,  0.0804,  0.0907,  0.0865,  0.0985,  0.0918,  0.0975,\n",
       "                        0.0938,  0.0990,  0.0896,  0.0830,  0.0789,  0.0718,  0.0709,  0.0772,\n",
       "                        0.0595,  0.0663,  0.0603,  0.0529,  0.0544,  0.0465,  0.0318,  0.0456,\n",
       "                        0.0258,  0.0197,  0.0272,  0.0523,  0.0496,  0.0385,  0.0554,  0.0594,\n",
       "                        0.0321,  0.0397,  0.0200,  0.0432,  0.0334,  0.0387,  0.0309,  0.0524,\n",
       "                        0.0392,  0.0486,  0.0533,  0.0419,  0.0494,  0.0550,  0.0269,  0.0490,\n",
       "                        0.0425,  0.0518,  0.0471,  0.0524,  0.0602,  0.0479,  0.0435,  0.0666,\n",
       "                        0.0332,  0.0549,  0.0475,  0.0363,  0.0520,  0.0585,  0.0460,  0.0672,\n",
       "                        0.0797,  0.0753,  0.0554,  0.0728,  0.0772,  0.0804,  0.0800,  0.0784,\n",
       "                        0.0862,  0.0833,  0.0866,  0.0758,  0.0771,  0.0891,  0.0982,  0.0966,\n",
       "                        0.0975,  0.0978,  0.0998,  0.0844,  0.1039,  0.0985,  0.0889,  0.0918,\n",
       "                        0.0869,  0.0862,  0.0834,  0.0817,  0.0832,  0.0798,  0.0795,  0.0804,\n",
       "                        0.0794,  0.0814,  0.0804,  0.0796,  0.0809,  0.0804,  0.0774,  0.0799,\n",
       "                        0.0812,  0.0799,  0.0799,  0.0802,  0.0802,  0.0784,  0.0810,  0.0800,\n",
       "                        0.0806], device='cuda:0')),\n",
       "              ('lstm_1.weight_ih_l0',\n",
       "               tensor([[-0.0356, -0.0688, -0.0797,  ..., -0.0961, -0.0247, -0.1337],\n",
       "                       [-0.0026,  0.0470,  0.0655,  ...,  0.0393,  0.0355,  0.0587],\n",
       "                       [-0.0400,  0.0874,  0.0110,  ...,  0.0383,  0.0334,  0.0663],\n",
       "                       ...,\n",
       "                       [-0.0625,  0.0451, -0.0931,  ..., -0.0503, -0.0366,  0.0341],\n",
       "                       [ 0.0876, -0.0277,  0.0244,  ..., -0.0066, -0.0369,  0.0397],\n",
       "                       [ 0.1070, -0.0289,  0.0805,  ...,  0.0088,  0.0612,  0.0412]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_1.weight_hh_l0',\n",
       "               tensor([[ 0.1123, -0.2516, -0.1555,  ..., -0.1015,  0.0280, -0.0161],\n",
       "                       [-0.1132, -0.1591,  0.1509,  ..., -0.1026, -0.2198,  0.0183],\n",
       "                       [ 0.0998,  0.1191, -0.1102,  ...,  0.0883, -0.0694,  0.1486],\n",
       "                       ...,\n",
       "                       [-0.1763,  0.0005, -0.0389,  ..., -0.0535, -0.0190, -0.0099],\n",
       "                       [-0.1211, -0.1549,  0.0922,  ...,  0.0899, -0.0365,  0.0392],\n",
       "                       [-0.1099, -0.0133,  0.0756,  ...,  0.2075,  0.0687, -0.1306]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_1.bias_ih_l0',\n",
       "               tensor([ 0.0893, -0.0956,  0.0003,  ...,  0.0232,  0.0548, -0.0093], device='cuda:0')),\n",
       "              ('lstm_1.bias_hh_l0',\n",
       "               tensor([ 0.0628, -0.0687,  0.0146,  ...,  0.0641,  0.0525, -0.0781], device='cuda:0')),\n",
       "              ('layer_norm_1.weight',\n",
       "               tensor([0.8854, 0.8465, 0.8752, 0.9663, 0.8333, 0.8694, 0.9145, 0.8755, 0.9334,\n",
       "                       0.8335, 0.8412, 0.8678, 0.9078, 0.9432, 0.9289, 0.9072, 0.8858, 0.8970,\n",
       "                       0.8407, 0.8705, 0.9199, 0.8755, 0.8921, 0.8582, 0.9038, 0.8900, 0.8645,\n",
       "                       0.8910, 0.8520, 0.8631, 0.8996, 0.8689, 0.8583, 0.9709, 0.8577, 0.9290,\n",
       "                       0.9593, 0.8728, 0.8881, 0.9922, 0.9044, 0.9108, 0.8438, 0.9248, 0.8937,\n",
       "                       0.9136, 0.8332, 0.8520, 0.8495, 0.8937, 0.8577, 0.9066, 0.9519, 0.8675,\n",
       "                       0.8879, 0.8715, 0.8553, 0.9088, 0.8668, 0.8589, 0.9056, 0.8614, 0.9252,\n",
       "                       0.9059, 0.8515, 0.9513, 0.9549, 0.8503, 0.9952, 0.8795, 0.8986, 0.8477,\n",
       "                       0.9735, 0.8599, 0.9157, 0.9243, 0.8965, 0.9792, 0.8595, 0.8885, 0.9055,\n",
       "                       0.9401, 0.8895, 0.9208, 0.9126, 0.8399, 0.8600, 0.8927, 0.9128, 0.8686,\n",
       "                       0.8872, 0.8490, 0.9069, 0.9053, 0.9636, 0.9341, 0.9262, 0.9477, 0.9695,\n",
       "                       0.9329, 0.8954, 0.8595, 0.8759, 0.8912, 0.9115, 0.9204, 0.8922, 0.9430,\n",
       "                       0.9512, 0.9655, 0.8512, 0.8952, 0.8643, 0.8487, 0.9275, 0.9302, 0.8585,\n",
       "                       0.8379, 0.9100, 1.0635, 0.9279, 0.9303, 0.8637, 0.9281, 0.9193, 0.9794,\n",
       "                       0.9665, 0.9200, 0.8847, 0.9157, 0.9080, 0.8897, 0.8798, 0.8933, 0.9488,\n",
       "                       0.8533, 0.8848, 0.9170, 0.9167, 0.8955, 0.8512, 0.9005, 0.8782, 0.8691,\n",
       "                       0.8503, 0.8944, 0.8924, 0.9684, 0.7977, 0.9867, 0.9186, 0.9374, 0.9127,\n",
       "                       0.9274, 0.9458, 0.9131, 0.9386, 0.8579, 0.8814, 0.8644, 0.9548, 0.9016,\n",
       "                       0.8906, 0.8587, 0.8947, 0.9444, 0.8949, 0.8008, 0.9124, 0.9163, 0.9084,\n",
       "                       0.8458, 0.8930, 0.7865, 0.9163, 0.8518, 1.0032, 0.9573, 0.8361, 1.0170,\n",
       "                       0.8850, 0.8873, 0.8810, 0.8948, 0.9688, 0.9220, 0.8996, 0.9458, 0.8850,\n",
       "                       0.9173, 0.9352, 0.9939, 0.8904, 0.9156, 0.9213, 0.8401, 0.8884, 0.9424,\n",
       "                       0.9428, 0.9794, 0.9164, 0.8903, 0.8830, 0.9240, 0.9347, 0.8834, 0.8761,\n",
       "                       0.8329, 0.9234, 0.9481, 0.9580, 0.8833, 0.9380, 0.8930, 0.9008, 0.8377,\n",
       "                       0.9055, 0.8743, 0.8789, 0.9857, 0.8441, 0.9002, 0.9046, 0.8628, 0.9130,\n",
       "                       0.8855, 0.8486, 0.8417, 0.9488, 0.9530, 0.9815, 0.8915, 0.8411, 0.9088,\n",
       "                       0.9640, 0.8762, 0.8973, 0.9787, 0.9147, 0.8835, 0.8513, 0.8603, 0.9875,\n",
       "                       0.8180, 0.8894, 0.8806, 0.9119, 0.8015, 0.9233, 0.9227, 0.8894, 0.8318,\n",
       "                       0.8694, 0.9399, 0.9383, 0.9025, 0.8676, 0.9147, 0.9301, 0.8428, 1.0029,\n",
       "                       0.8089, 0.8916, 0.9052, 0.8741, 0.8566, 0.9159, 0.8980, 0.8647, 0.9127,\n",
       "                       0.9175, 0.9101, 0.8745, 0.9091, 0.9022, 0.8406, 0.9461, 0.9058, 0.9571,\n",
       "                       0.8724, 0.9193, 0.8828, 0.9366, 0.9322, 0.8596, 0.9445, 0.8839, 0.9244,\n",
       "                       0.9476, 0.9270, 0.8953, 0.9314, 0.8321, 0.8759, 0.8974, 0.9431, 0.9116,\n",
       "                       0.8895, 0.8300, 0.8284], device='cuda:0')),\n",
       "              ('layer_norm_1.bias',\n",
       "               tensor([ 0.0594, -0.0102, -0.0603, -0.0499,  0.0143, -0.0030, -0.0007, -0.0037,\n",
       "                        0.0032, -0.0220, -0.0071,  0.0027, -0.0229, -0.0539,  0.0409,  0.0659,\n",
       "                       -0.0232, -0.0282,  0.0196,  0.0234, -0.0246, -0.0226, -0.0064, -0.0080,\n",
       "                       -0.0369,  0.0126, -0.0232,  0.0091, -0.0431, -0.0029,  0.0508,  0.0901,\n",
       "                        0.0102, -0.0286,  0.0303,  0.0037,  0.0186,  0.0644, -0.0780, -0.0263,\n",
       "                        0.0296,  0.0155,  0.0097,  0.0451, -0.0288,  0.0153, -0.0120, -0.0075,\n",
       "                        0.0731, -0.0092, -0.0126, -0.0109, -0.0212, -0.0231, -0.0780,  0.0581,\n",
       "                        0.0040,  0.0439,  0.0055,  0.0741,  0.0280, -0.0239, -0.0660, -0.0264,\n",
       "                        0.0029, -0.0118, -0.0026,  0.0140,  0.0465,  0.0548,  0.0050, -0.0038,\n",
       "                        0.0707, -0.0142, -0.0128, -0.0271,  0.0167,  0.0087, -0.0028,  0.0304,\n",
       "                        0.0500,  0.0184, -0.0274,  0.0305,  0.0622,  0.0455, -0.0330, -0.0153,\n",
       "                       -0.0474,  0.0188, -0.0837,  0.0476,  0.0041, -0.0030, -0.0191,  0.0249,\n",
       "                       -0.0553,  0.0226,  0.0780, -0.0199, -0.0002,  0.0303,  0.0554, -0.0059,\n",
       "                       -0.0510, -0.0206, -0.0179,  0.0185, -0.0596,  0.0207,  0.0135,  0.0046,\n",
       "                        0.0063, -0.0353, -0.0246,  0.0006, -0.0575,  0.0183, -0.0376,  0.0175,\n",
       "                        0.0313,  0.0199, -0.0136,  0.0312, -0.0197, -0.0358,  0.0216,  0.0562,\n",
       "                        0.0036,  0.0060, -0.0128, -0.0479, -0.0011,  0.0126,  0.0005, -0.0005,\n",
       "                        0.0251,  0.0215,  0.0048, -0.0186,  0.0101,  0.0415, -0.0513,  0.0278,\n",
       "                        0.0103, -0.0213, -0.0054,  0.0038,  0.0403,  0.1435, -0.0103, -0.0359,\n",
       "                       -0.0550,  0.0235, -0.0139, -0.0660,  0.0452,  0.0014,  0.1191,  0.0337,\n",
       "                       -0.0318, -0.0240, -0.0940,  0.0329,  0.0605,  0.0084,  0.0250, -0.0275,\n",
       "                       -0.0256,  0.0229,  0.0185, -0.0046, -0.0112,  0.0043,  0.0015,  0.0463,\n",
       "                       -0.0334, -0.0052, -0.0046,  0.0096, -0.0567,  0.0084, -0.0345,  0.0187,\n",
       "                       -0.0253,  0.0333, -0.0286, -0.0314,  0.0013,  0.0399, -0.0194,  0.0026,\n",
       "                       -0.0182, -0.0512, -0.0286, -0.0180, -0.0225, -0.0515, -0.0162, -0.1116,\n",
       "                        0.0056,  0.0513,  0.0310,  0.0315,  0.0042, -0.0874, -0.0128,  0.0265,\n",
       "                       -0.0200, -0.0117,  0.0089, -0.0568, -0.0142, -0.0566, -0.0123,  0.0248,\n",
       "                        0.0396, -0.0479, -0.0243, -0.0360, -0.0187, -0.0492,  0.0067,  0.0281,\n",
       "                       -0.0155,  0.0118,  0.0229, -0.0115,  0.0223, -0.0470, -0.0229,  0.0047,\n",
       "                        0.0447, -0.0339,  0.0035,  0.0055,  0.0113,  0.0777,  0.0623, -0.0033,\n",
       "                        0.0235,  0.0420,  0.0608,  0.0375, -0.0015,  0.0473, -0.0192, -0.0259,\n",
       "                        0.0272, -0.0835,  0.0384, -0.0301, -0.0060,  0.0503,  0.0342, -0.0355,\n",
       "                       -0.0752,  0.0174,  0.0251,  0.0249,  0.0697,  0.0226, -0.0537,  0.0326,\n",
       "                       -0.0278, -0.0451, -0.0746,  0.0558,  0.0585,  0.0484,  0.0243,  0.0103,\n",
       "                       -0.0117,  0.0037,  0.0392,  0.0082, -0.0373, -0.0296, -0.0064, -0.0064,\n",
       "                        0.0029, -0.0607,  0.0012,  0.0237, -0.0284, -0.0159, -0.0285, -0.0080,\n",
       "                       -0.0518, -0.0366, -0.0630, -0.0358, -0.0135,  0.0801, -0.0296,  0.0236,\n",
       "                       -0.0375, -0.0382, -0.0088, -0.0102], device='cuda:0')),\n",
       "              ('lstm_2.weight_ih_l0',\n",
       "               tensor([[ 0.0406, -0.0488, -0.0549,  ..., -0.0103, -0.0262, -0.0680],\n",
       "                       [-0.0001,  0.0242,  0.0934,  ..., -0.0106,  0.1038,  0.0471],\n",
       "                       [-0.1399, -0.0255, -0.0815,  ...,  0.0444,  0.0246, -0.0550],\n",
       "                       ...,\n",
       "                       [ 0.0187,  0.0079,  0.0621,  ...,  0.0484,  0.0081, -0.0022],\n",
       "                       [-0.0834,  0.0074, -0.0097,  ..., -0.0113,  0.0391,  0.0371],\n",
       "                       [ 0.0771, -0.0469,  0.0735,  ..., -0.0128, -0.0002,  0.0035]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_2.weight_hh_l0',\n",
       "               tensor([[ 0.0955,  0.1851, -0.0151,  ..., -0.1724,  0.0460,  0.1599],\n",
       "                       [ 0.0404, -0.1077, -0.0882,  ..., -0.0328, -0.0960, -0.0734],\n",
       "                       [ 0.0619, -0.0377,  0.1833,  ..., -0.0641,  0.1835, -0.1741],\n",
       "                       ...,\n",
       "                       [-0.1051,  0.1047, -0.1588,  ...,  0.0516, -0.1311, -0.1143],\n",
       "                       [-0.0752, -0.1642, -0.0163,  ..., -0.0715, -0.1424, -0.0877],\n",
       "                       [ 0.0470, -0.1680,  0.0903,  ...,  0.0969,  0.0463,  0.0687]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_2.bias_ih_l0',\n",
       "               tensor([ 0.0269, -0.0258,  0.0267,  ..., -0.0385, -0.0687, -0.0215], device='cuda:0')),\n",
       "              ('lstm_2.bias_hh_l0',\n",
       "               tensor([-0.0546, -0.0909, -0.0265,  ..., -0.0041, -0.0379,  0.0495], device='cuda:0')),\n",
       "              ('layer_norm_2.weight',\n",
       "               tensor([1.3472, 1.3043, 1.2815, 1.2917, 1.2877, 1.3567, 1.2544, 1.2886, 1.2310,\n",
       "                       1.4916, 1.2939, 1.0844, 1.3053, 1.3990, 1.4309, 1.3854, 1.2848, 1.3256,\n",
       "                       1.4582, 1.1984, 1.3294, 1.2040, 1.3192, 1.3334, 1.3112, 1.2462, 1.3119,\n",
       "                       1.4051, 1.3198, 1.4217, 1.2631, 1.4286, 1.2725, 1.3296, 1.2964, 1.2258,\n",
       "                       1.1079, 1.4222, 1.2156, 1.2612, 1.4168, 1.2584, 1.1886, 1.2519, 1.2604,\n",
       "                       1.2861, 1.3288, 1.4041, 1.3410, 1.5133, 1.4152, 1.3428, 1.3660, 1.2104,\n",
       "                       1.4170, 1.2173, 1.2762, 1.4679, 1.4524, 1.4023, 1.2053, 1.2809, 1.3101,\n",
       "                       1.2113, 1.4104, 1.4036, 1.2582, 1.1174, 1.1957, 1.2237, 1.4636, 1.5505,\n",
       "                       1.3042, 1.3876, 1.4110, 1.4967, 1.1990, 1.3759, 1.3317, 1.3154, 1.3037,\n",
       "                       1.4283, 1.6130, 1.1546, 1.0802, 1.3404, 1.2328, 1.0923, 1.2389, 1.1776,\n",
       "                       1.2526, 1.2318, 1.4678, 1.2271, 1.2375, 1.3171, 1.2014, 1.2493, 1.4635,\n",
       "                       1.5187, 1.3616, 1.3712, 1.2247, 1.4178, 1.2452, 1.2866, 1.3224, 1.2902,\n",
       "                       1.4252, 1.1857, 1.2025, 1.3542, 1.3591, 1.2077, 1.3526, 1.2971, 1.3428,\n",
       "                       1.2316, 1.5246, 1.2773, 1.3493, 1.2353, 1.1903, 1.2590, 1.0841, 1.2652,\n",
       "                       1.3166, 1.2909, 1.1721, 1.3747, 1.3839, 1.2970, 1.0960, 1.2144, 1.3665,\n",
       "                       1.3448, 1.4588, 1.3389, 1.1365, 1.1907, 1.5600, 1.2958, 1.1705, 1.2476,\n",
       "                       1.3667, 1.1767, 1.2329, 1.3445, 1.2425, 1.1407, 1.3372, 1.3572, 1.3644,\n",
       "                       1.2837, 1.2648, 1.3379, 1.3767, 1.5234, 1.2918, 1.1508, 1.1891, 1.3351,\n",
       "                       1.3347, 1.2921, 1.3132, 1.2109, 1.3691, 1.3340, 1.3742, 1.2873, 1.2030,\n",
       "                       1.1912, 1.2344, 1.2198, 1.4559, 1.2885, 1.3734, 1.1453, 1.3688, 1.3178,\n",
       "                       1.2969, 1.4911, 1.2132, 1.2808, 1.2452, 1.2268, 1.2940, 1.3899, 1.3923,\n",
       "                       1.3927, 1.3334, 1.3458, 1.2399, 1.2878, 1.4363, 1.3576, 1.2976, 1.1452,\n",
       "                       1.2574, 1.2756, 1.3383, 1.3665, 1.4278, 1.5779, 1.2215, 1.2115, 1.2540,\n",
       "                       1.3517, 1.2635, 1.3680, 1.4203, 1.2361, 1.2916, 1.2554, 1.3399, 1.1942,\n",
       "                       1.3574, 1.2323, 1.2683, 1.3024, 1.1601, 1.2087, 1.1352, 1.3794, 1.4862,\n",
       "                       1.1951, 1.4373, 1.2516, 1.1818, 1.3704, 1.2219, 1.3408, 1.3436, 1.2591,\n",
       "                       1.1825, 1.3391, 1.5431, 1.3516, 1.4411, 1.4726, 1.1781, 1.3290, 1.3727,\n",
       "                       1.4014, 1.3145, 1.2101, 1.3521, 1.2995, 1.1460, 1.2836, 1.3151, 1.2514,\n",
       "                       1.1670, 1.2712, 1.3035, 1.3385, 1.1979, 1.3938, 1.2948, 1.2257, 1.3043,\n",
       "                       1.2609, 1.3789, 1.1724, 1.3299, 1.1055, 1.3871, 1.3002, 1.3298, 1.2445,\n",
       "                       1.4175, 1.5683, 1.4752, 1.3691, 1.2996, 1.2581, 1.3572, 1.2273, 1.3553,\n",
       "                       1.3110, 1.2471, 1.4835, 1.1751, 1.4002, 1.1340, 1.2912, 1.2621, 1.2939,\n",
       "                       1.2949, 1.2056, 1.2973, 1.3057, 1.0697, 1.2437, 1.3341, 1.2662, 1.2382,\n",
       "                       1.3667, 1.2837, 1.3232], device='cuda:0')),\n",
       "              ('layer_norm_2.bias',\n",
       "               tensor([-0.0242, -0.0203, -0.0432, -0.0362,  0.1517,  0.0764, -0.0173, -0.0470,\n",
       "                        0.0266,  0.1461, -0.0178, -0.0551,  0.0570, -0.1281,  0.1007, -0.0407,\n",
       "                       -0.0780, -0.0659, -0.0591,  0.0007,  0.1203, -0.0299, -0.0450, -0.1465,\n",
       "                       -0.0295,  0.0763, -0.1634, -0.1788,  0.0654,  0.1375,  0.0664, -0.0648,\n",
       "                       -0.0711, -0.1482,  0.0312,  0.0751, -0.0185, -0.0169,  0.0596, -0.0632,\n",
       "                       -0.0098,  0.1006, -0.0490, -0.0379, -0.0270,  0.2139, -0.0518, -0.0393,\n",
       "                       -0.1412, -0.1074,  0.0699, -0.1220, -0.0540, -0.0343,  0.0148, -0.0188,\n",
       "                       -0.1522,  0.0879,  0.0837,  0.0606, -0.0564, -0.1029,  0.0935,  0.0048,\n",
       "                        0.1733, -0.0434,  0.0216,  0.0342,  0.0335,  0.0183,  0.0059,  0.1482,\n",
       "                        0.0940, -0.1563, -0.0304, -0.0701,  0.0106, -0.0392,  0.0202,  0.0841,\n",
       "                       -0.0636,  0.0776, -0.0128, -0.0182, -0.0159,  0.0662,  0.0096, -0.0463,\n",
       "                        0.0589,  0.0485,  0.0774,  0.0103, -0.0231,  0.0217, -0.0254, -0.1626,\n",
       "                       -0.0751, -0.0840, -0.1049,  0.1340, -0.1035, -0.0671, -0.0196, -0.1605,\n",
       "                        0.0746,  0.0657,  0.1787,  0.1262, -0.1468, -0.0728,  0.0184,  0.2005,\n",
       "                       -0.0743,  0.1013, -0.1154,  0.1439,  0.0936,  0.0507, -0.0010, -0.0651,\n",
       "                        0.1018, -0.0527,  0.0848,  0.0876,  0.0390, -0.0909,  0.0811,  0.0472,\n",
       "                        0.0053,  0.0981, -0.2282,  0.0836, -0.0335, -0.0307, -0.0793, -0.1060,\n",
       "                       -0.0945,  0.0592,  0.0519, -0.1020,  0.0456, -0.0035, -0.0149,  0.0226,\n",
       "                        0.0316, -0.0270, -0.0164, -0.0011,  0.0707,  0.0147,  0.0069,  0.0942,\n",
       "                       -0.0869, -0.0227,  0.1006, -0.0642, -0.1063, -0.0476, -0.0328,  0.0593,\n",
       "                        0.0531, -0.0475,  0.0551,  0.1136,  0.0050,  0.0197,  0.2163, -0.0720,\n",
       "                        0.1436,  0.0249, -0.0529, -0.0644, -0.1352, -0.0135, -0.2577,  0.0739,\n",
       "                       -0.2032,  0.0191, -0.0867, -0.2281,  0.0019, -0.0675, -0.0624, -0.0046,\n",
       "                        0.0852, -0.0054, -0.1481,  0.0229, -0.0463, -0.0939, -0.0577,  0.0951,\n",
       "                        0.0955,  0.0342, -0.1566, -0.0595,  0.1322, -0.0357,  0.0827,  0.0481,\n",
       "                       -0.1304,  0.0086, -0.0810, -0.0791, -0.0862,  0.0196,  0.0441,  0.0271,\n",
       "                       -0.0390, -0.0379, -0.1145,  0.0110,  0.1246, -0.0648,  0.0672,  0.0315,\n",
       "                        0.1562,  0.0318,  0.1250, -0.0607, -0.0037,  0.0076, -0.0321, -0.1952,\n",
       "                       -0.0632,  0.0473,  0.0098,  0.0710,  0.0160,  0.1015, -0.0678, -0.0028,\n",
       "                        0.1215,  0.0226,  0.0352, -0.1649, -0.0345,  0.0996, -0.0744, -0.0976,\n",
       "                        0.0927,  0.0220,  0.0252, -0.0868,  0.0923, -0.0018,  0.0840,  0.1782,\n",
       "                        0.0367, -0.1716,  0.0315, -0.0890,  0.0164,  0.1819, -0.1044, -0.0459,\n",
       "                       -0.0273,  0.1056, -0.0238,  0.0640, -0.0640, -0.0023, -0.0143, -0.1429,\n",
       "                       -0.0994,  0.0498,  0.0469, -0.1142,  0.0701, -0.0572, -0.1187, -0.0430,\n",
       "                        0.0551,  0.0479,  0.0883,  0.0216, -0.0892, -0.0538, -0.0312,  0.1360,\n",
       "                        0.0442,  0.1886, -0.0108, -0.2413, -0.0886, -0.0289, -0.0722,  0.1090,\n",
       "                       -0.1112,  0.0009, -0.0880,  0.0671, -0.0500, -0.0608, -0.0653,  0.0012,\n",
       "                       -0.1329,  0.0642,  0.0157, -0.0499], device='cuda:0')),\n",
       "              ('lstm_3.weight_ih_l0',\n",
       "               tensor([[-0.0940,  0.0959,  0.0964,  ...,  0.0680,  0.0361, -0.0940],\n",
       "                       [ 0.0175, -0.0724,  0.0739,  ..., -0.0080, -0.1217,  0.0738],\n",
       "                       [ 0.2120, -0.0236,  0.0637,  ..., -0.0458, -0.0177, -0.1880],\n",
       "                       ...,\n",
       "                       [ 0.0438,  0.1002, -0.0683,  ..., -0.0597,  0.1692, -0.1014],\n",
       "                       [-0.0140, -0.0040, -0.0948,  ...,  0.0849, -0.1711, -0.0545],\n",
       "                       [ 0.0589, -0.1714, -0.0101,  ...,  0.1255,  0.1611,  0.0925]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_3.weight_hh_l0',\n",
       "               tensor([[ 0.0011,  0.0069,  0.2098,  ...,  0.0558,  0.1296,  0.0969],\n",
       "                       [-0.1337,  0.2094, -0.0862,  ...,  0.1432, -0.0665,  0.1223],\n",
       "                       [-0.1390,  0.0987, -0.1265,  ..., -0.3902, -0.0656, -0.0872],\n",
       "                       ...,\n",
       "                       [-0.1358, -0.1095, -0.1558,  ..., -0.0503,  0.0271,  0.1253],\n",
       "                       [-0.0001, -0.1839,  0.2957,  ...,  0.0808,  0.0387, -0.0956],\n",
       "                       [ 0.1856, -0.0443, -0.1145,  ...,  0.3614, -0.0825, -0.2965]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_3.bias_ih_l0',\n",
       "               tensor([ 0.0371,  0.0523,  0.0292,  ..., -0.0032,  0.0054,  0.0852], device='cuda:0')),\n",
       "              ('lstm_3.bias_hh_l0',\n",
       "               tensor([-0.0102,  0.0570,  0.0221,  ...,  0.0671,  0.0003,  0.1160], device='cuda:0')),\n",
       "              ('layer_norm_3.weight',\n",
       "               tensor([2.7883, 2.5522, 2.4310, 2.8497, 2.5681, 2.7070, 2.8433, 2.6859, 2.6295,\n",
       "                       2.8635, 2.4936, 2.4018, 2.0697, 2.7693, 2.5033, 2.1476, 2.7068, 2.7473,\n",
       "                       2.2899, 2.3561, 2.5293, 2.6096, 2.7271, 2.2159, 2.7854, 2.4322, 2.9037,\n",
       "                       2.9515, 2.4162, 2.9456, 2.7342, 2.9059, 2.1921, 2.7194, 2.4505, 2.8213,\n",
       "                       2.5797, 2.6829, 2.3775, 2.6387, 2.6272, 2.8114, 2.8621, 2.6602, 2.4542,\n",
       "                       2.8722, 2.3392, 2.8328, 2.8730, 2.2097, 2.4675, 2.8692, 2.5032, 2.7870,\n",
       "                       2.7614, 2.5183, 2.5330, 2.7465, 2.0383, 2.3333, 2.6900, 2.8869, 2.8935,\n",
       "                       2.6584, 2.7502, 2.3802, 2.7533, 2.8527, 2.3815, 2.7896, 2.8713, 2.4777,\n",
       "                       2.0963, 2.6609, 2.9467, 2.4246, 2.7686, 2.3936, 2.7290, 2.7139, 2.6561,\n",
       "                       2.8478, 2.7785, 2.7300, 2.3188, 2.2182, 2.8554, 2.9575, 2.9581, 2.8247,\n",
       "                       1.9482, 2.8306, 2.8270, 2.6165, 2.9828, 2.5976, 2.4584, 2.6343, 2.8051,\n",
       "                       2.8467, 2.8659, 2.5169, 2.4542, 2.9912, 3.1427, 1.7998, 2.9524, 2.6432,\n",
       "                       2.5398, 2.4288, 2.4324, 2.1336, 2.5462, 2.7362, 2.9156, 2.6978, 2.1972,\n",
       "                       2.1120, 2.7614, 2.4156, 2.7685, 2.7457, 2.6125, 2.8174, 2.7500, 2.6852,\n",
       "                       2.6735, 2.4590, 3.0016, 2.2400, 2.8122, 2.6888, 2.2019, 2.8406, 2.6646,\n",
       "                       2.6043, 2.8433, 2.5255, 2.2161, 2.6228, 2.8011, 1.8037, 2.7250, 2.6837,\n",
       "                       2.3893, 2.9084, 2.3282, 2.1835, 2.8824, 2.5550, 2.6533, 2.6377, 2.4841,\n",
       "                       2.1650, 2.9140, 2.6888, 2.7400, 2.6853, 2.4363, 2.6705, 2.1666, 2.7530,\n",
       "                       2.5125, 2.6265, 2.7154, 2.8271, 2.8055, 2.8262, 2.2834, 2.5567, 2.8500,\n",
       "                       2.7104, 2.6496, 2.6284, 2.2426, 2.6731, 2.9390, 2.9525, 2.7948, 2.9613,\n",
       "                       2.5340, 2.5897, 2.3071, 2.4779, 2.9624, 3.0232, 2.3781, 2.9720, 2.7177,\n",
       "                       2.4452, 2.8711, 2.8742, 2.3634, 2.7330, 2.7268, 2.7653, 2.6706, 2.6420,\n",
       "                       2.7644, 2.8561, 2.5996, 2.1235, 2.4065, 2.5482, 1.9280, 2.7185, 3.0686,\n",
       "                       2.5250, 2.7723, 2.7855, 2.6435, 2.9469, 2.7096, 2.6117, 2.4887, 2.6531,\n",
       "                       2.5079, 2.3532, 1.7350, 2.0474, 3.0015, 2.8583, 2.7071, 2.3141, 2.6471,\n",
       "                       2.8524, 2.9844, 2.6760, 2.7185, 2.3284, 2.6455, 2.8885, 2.5795, 3.0156,\n",
       "                       2.7165, 2.2451, 2.6247, 2.4469, 2.5634, 2.7109, 2.9012, 2.3392, 2.6322,\n",
       "                       2.0088, 2.9649, 3.0002, 2.3729, 2.0641, 2.5071, 2.6903, 2.4753, 2.3698,\n",
       "                       2.2828, 2.5823, 2.6145, 2.7452, 2.7563, 2.6268, 2.8376, 2.9417, 2.8482,\n",
       "                       2.8185, 2.7389, 2.7397, 2.4141, 3.0406, 2.4327, 2.5545, 2.8772, 2.4340,\n",
       "                       2.4370, 2.9923, 2.8364, 2.7705, 2.7524, 2.3981, 2.3715, 2.6566, 2.8455,\n",
       "                       2.7699, 2.7343, 2.7872, 2.7487, 2.3492, 2.7688, 2.5923, 3.0401, 2.5790,\n",
       "                       2.8631, 2.6564, 2.6596, 2.3437, 2.4311, 2.7251, 2.8325, 2.9482, 2.2630,\n",
       "                       2.7902, 2.7267, 2.6808], device='cuda:0')),\n",
       "              ('layer_norm_3.bias',\n",
       "               tensor([ 0.1425, -0.5524,  0.0843,  0.8817, -0.1620, -0.4896,  0.0830, -0.0943,\n",
       "                       -0.3905, -0.4043, -0.7885, -0.0667,  0.5991, -0.3978,  0.6458, -0.3397,\n",
       "                        0.5827, -0.0379,  0.6976,  0.5962, -0.1757, -0.4135,  0.8489, -0.2174,\n",
       "                        0.4945,  0.4505,  0.6889, -0.7360, -0.4205, -0.6097, -0.4440, -0.7850,\n",
       "                       -0.3791, -0.7039,  0.3785,  0.3011,  0.1019,  0.4643, -0.2878, -0.2193,\n",
       "                        0.6906, -0.1288, -0.8689, -0.5974, -0.0206, -0.8563,  0.1902, -0.9503,\n",
       "                        0.4138,  0.4478, -0.8154, -0.3519, -0.1948, -0.8473,  0.8321, -0.6150,\n",
       "                        0.6469, -0.8593, -0.3980, -0.5336, -0.4185, -0.5737,  0.7094, -0.8144,\n",
       "                        0.7793, -0.3330,  0.0545, -0.5398,  0.1911,  0.0050, -0.2493, -0.4847,\n",
       "                       -0.2353,  0.1094,  0.8469, -0.3013,  0.4967, -0.5164, -0.0601,  0.6050,\n",
       "                        0.5559, -0.2731,  0.2577,  1.0303, -0.5883,  0.3268, -0.8371,  0.7946,\n",
       "                        0.6122,  0.1265,  0.0981, -0.6675, -0.1342,  0.7321,  0.6970, -0.1636,\n",
       "                        0.7583,  0.4128,  0.3168, -0.1880, -0.5812,  0.0676, -0.5229,  0.7794,\n",
       "                        0.0604, -0.6127,  0.2009,  0.7788,  0.5906, -0.1321,  0.5521,  0.4178,\n",
       "                       -0.2870,  0.3734, -0.5712,  0.3607, -0.8039,  0.5326, -0.2173, -0.2515,\n",
       "                       -0.3504,  0.2236,  0.8810,  0.4998, -0.2552, -0.2873,  0.0954, -0.1879,\n",
       "                       -0.6619,  0.5317, -0.7396, -0.5891,  0.7221,  0.4027, -0.2689,  0.8346,\n",
       "                        0.5703, -0.3246, -0.5608,  0.2719, -0.4466, -0.4832, -0.7117,  0.2787,\n",
       "                        0.7326, -0.6048, -0.6448, -0.1934,  0.5902, -0.6206, -0.6969, -0.6402,\n",
       "                       -0.5729, -0.4710,  0.6719, -0.8497, -0.7341,  0.2714, -0.6083,  0.4820,\n",
       "                       -0.0606,  0.5424, -0.5985,  0.2545, -0.1940, -0.4835,  0.6129, -0.4119,\n",
       "                        1.0335, -0.6494, -0.5443, -0.1887, -0.6397,  0.5773, -0.5706,  0.5102,\n",
       "                        0.7985,  0.8454, -0.6271,  0.8083, -0.0703,  0.2310,  0.3374,  0.7223,\n",
       "                        0.4290,  0.6106, -0.0186, -0.7208, -0.5856,  0.8839, -0.6936,  0.7261,\n",
       "                       -0.5238, -0.2077, -0.4005, -0.9467, -0.0159, -0.2294, -0.8327, -0.8178,\n",
       "                        0.4471, -0.5239, -0.0319,  0.3094, -0.1171, -0.0015, -0.9903,  0.2408,\n",
       "                        0.6613,  0.5797, -0.4104,  0.6322, -0.7361,  0.8464, -0.3049,  0.5982,\n",
       "                       -0.1537,  0.2086,  0.5773,  0.8168,  0.9739, -0.4168,  0.5612, -0.0815,\n",
       "                       -0.7732, -0.0077,  0.7330,  0.7261,  0.1814, -0.6230, -0.6786,  0.0310,\n",
       "                       -0.7130,  1.0236, -0.2938, -0.7463,  0.6415, -0.1202,  0.4466,  0.5648,\n",
       "                        0.7070, -0.8606, -0.7388,  0.7533,  0.7319, -0.7396, -0.7841,  0.8298,\n",
       "                       -0.4440,  0.6114, -0.4680,  0.1059,  0.0442,  0.3383,  0.0259,  0.7640,\n",
       "                       -0.9628,  0.4837,  0.7621, -0.4009,  0.5841,  0.1010,  0.3524,  0.0884,\n",
       "                       -0.0792, -0.6196,  0.2516,  0.3982,  0.7226, -0.7259,  0.5531,  0.7048,\n",
       "                        0.4890,  0.4585, -0.9787, -0.5732,  0.5222, -0.5443, -0.2158,  0.7595,\n",
       "                       -0.5173,  0.2871,  0.2129,  0.7144,  0.0518, -0.0215,  0.4639, -0.4501,\n",
       "                        0.7871, -0.7626,  0.5670,  0.4003, -0.6493, -0.6445, -0.2397,  0.6438,\n",
       "                       -0.3883, -0.0655, -0.5928,  0.7917], device='cuda:0')),\n",
       "              ('to_notes.weight',\n",
       "               tensor([[ 0.3264, -0.3199,  0.0398,  ..., -0.1633, -0.1324, -0.1690],\n",
       "                       [ 0.0098,  0.0943, -0.0536,  ..., -0.0477, -0.0768,  0.0073],\n",
       "                       [-0.0529,  0.0374,  0.0408,  ...,  0.0103,  0.0099,  0.1209],\n",
       "                       ...,\n",
       "                       [-0.0688,  0.0445, -0.0454,  ..., -0.0992, -0.0448,  0.0808],\n",
       "                       [-0.0023,  0.0179, -0.0478,  ..., -0.0490,  0.0286,  0.0734],\n",
       "                       [ 0.0137,  0.0552, -0.0418,  ..., -0.0020,  0.0419,  0.0564]],\n",
       "                      device='cuda:0')),\n",
       "              ('to_notes.bias',\n",
       "               tensor([-0.1189, -0.0514, -0.0669, -0.0486, -0.0344, -0.0842, -0.0602, -0.0904,\n",
       "                       -0.0612, -0.0809, -0.0600, -0.0878, -0.0309, -0.0795, -0.0673, -0.0413,\n",
       "                       -0.0927, -0.1021, -0.0367, -0.0699, -0.0956, -0.1147, -0.1790, -0.1449,\n",
       "                       -0.2080, -0.1739, -0.1845, -0.1764, -0.1948, -0.2101, -0.2100, -0.2567,\n",
       "                       -0.1790, -0.1731, -0.1994, -0.2511, -0.2587, -0.1510, -0.1871, -0.2197,\n",
       "                       -0.2023, -0.2735, -0.1962, -0.2361, -0.1938, -0.2359, -0.2702, -0.2281,\n",
       "                       -0.2304, -0.2839, -0.2398, -0.2827, -0.2252, -0.2339, -0.2245, -0.2670,\n",
       "                       -0.2689, -0.2385, -0.2201, -0.2163, -0.2685, -0.1457, -0.2814, -0.2321,\n",
       "                       -0.2825, -0.2176, -0.2031, -0.2859, -0.2043, -0.2659, -0.1843, -0.2591,\n",
       "                       -0.2239, -0.2577, -0.2408, -0.1963, -0.1892, -0.2539, -0.2782, -0.2387,\n",
       "                       -0.2093, -0.1871, -0.1794, -0.2475, -0.2714, -0.2696, -0.1934, -0.2811,\n",
       "                       -0.3017, -0.3244, -0.2128, -0.2739, -0.2354, -0.3035, -0.1475, -0.1495,\n",
       "                       -0.2294, -0.1493, -0.1481, -0.1558, -0.1736, -0.0850, -0.1604, -0.0422,\n",
       "                       -0.0981, -0.0316, -0.0838, -0.1069, -0.0852, -0.0971, -0.0382, -0.0827,\n",
       "                       -0.0523, -0.0786, -0.0999, -0.0760, -0.1082, -0.0460, -0.0523, -0.0635,\n",
       "                       -0.0872, -0.0923, -0.0530, -0.0722, -0.0558, -0.0427, -0.0593, -0.0872,\n",
       "                       -0.0717], device='cuda:0'))]),\n",
       " 'opt_dict': <bound method Optimizer.state_dict of Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     initial_lr: 0.001\n",
       "     lr: 0.0005000000000000002\n",
       "     weight_decay: 0\n",
       " )>}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 5581,\n",
       " 'loss': 3.8881218433380127,\n",
       " 'recall': 0.6076475977897644,\n",
       " 'acc': 0.868152379989624,\n",
       " 'state_dict': OrderedDict([('layer_norm_0.weight',\n",
       "               tensor([0.9389, 0.9213, 0.9208, 0.9209, 0.9198, 0.9202, 0.9207, 0.9199, 0.9201,\n",
       "                       0.9204, 0.9199, 0.9204, 0.9205, 0.9196, 0.9199, 0.9200, 0.9205, 0.9199,\n",
       "                       0.9216, 1.1737, 1.1679, 1.2877, 1.3077, 1.3104, 1.3245, 1.3574, 1.2822,\n",
       "                       1.2615, 1.2682, 1.2247, 1.2582, 1.1765, 1.2059, 1.1812, 1.1833, 1.1510,\n",
       "                       1.1820, 1.1552, 1.1147, 1.1038, 1.1176, 1.0862, 1.1501, 1.0750, 1.0937,\n",
       "                       1.1020, 1.0895, 1.0995, 1.0853, 1.0648, 1.0727, 1.1059, 1.1244, 1.0899,\n",
       "                       1.1263, 1.1164, 1.1094, 1.1444, 1.0988, 1.0563, 1.0852, 1.1046, 1.1151,\n",
       "                       1.0668, 1.0891, 1.0897, 1.0712, 1.0643, 1.0564, 1.0768, 1.0830, 1.0512,\n",
       "                       1.0524, 1.0610, 1.0351, 1.1017, 1.0557, 1.0550, 1.0632, 1.0833, 1.0951,\n",
       "                       1.0863, 1.0788, 1.1274, 1.1143, 1.1234, 1.1247, 1.1527, 1.1452, 1.1709,\n",
       "                       1.1474, 1.1868, 1.1680, 1.1886, 1.2095, 1.2267, 1.2290, 1.3044, 1.2683,\n",
       "                       1.2613, 1.2903, 1.2156, 1.1164, 1.1106, 1.0253, 0.9925, 0.9471, 0.9433,\n",
       "                       0.9507, 0.9202, 0.9222, 0.9201, 0.9201, 0.9196, 0.9192, 0.9195, 0.9208,\n",
       "                       0.9202, 0.9227, 0.9197, 0.9197, 0.9198, 0.9211, 0.9205, 0.9207, 0.9219,\n",
       "                       0.9204, 0.9213, 0.9198], device='cuda:0')),\n",
       "              ('layer_norm_0.bias',\n",
       "               tensor([-0.3397,  0.0788,  0.0793,  0.0792,  0.0802,  0.0799,  0.0794,  0.0801,\n",
       "                        0.0800,  0.0796,  0.0801,  0.0797,  0.0795,  0.0805,  0.0801,  0.0801,\n",
       "                        0.0795,  0.0801,  0.0785,  0.0898,  0.0857,  0.1021,  0.0910,  0.0976,\n",
       "                        0.0947,  0.0994,  0.0887,  0.0834,  0.0790,  0.0714,  0.0720,  0.0757,\n",
       "                        0.0598,  0.0671,  0.0602,  0.0527,  0.0541,  0.0458,  0.0321,  0.0456,\n",
       "                        0.0270,  0.0192,  0.0252,  0.0517,  0.0496,  0.0384,  0.0570,  0.0594,\n",
       "                        0.0306,  0.0386,  0.0198,  0.0412,  0.0325,  0.0393,  0.0296,  0.0515,\n",
       "                        0.0416,  0.0496,  0.0549,  0.0396,  0.0473,  0.0539,  0.0268,  0.0497,\n",
       "                        0.0417,  0.0514,  0.0494,  0.0514,  0.0612,  0.0487,  0.0447,  0.0664,\n",
       "                        0.0324,  0.0566,  0.0469,  0.0381,  0.0507,  0.0596,  0.0456,  0.0664,\n",
       "                        0.0790,  0.0752,  0.0533,  0.0744,  0.0773,  0.0829,  0.0807,  0.0773,\n",
       "                        0.0866,  0.0838,  0.0871,  0.0737,  0.0777,  0.0873,  0.0981,  0.0953,\n",
       "                        0.0960,  0.0977,  0.0995,  0.0842,  0.1034,  0.0952,  0.0883,  0.0911,\n",
       "                        0.0873,  0.0813,  0.0829,  0.0808,  0.0823,  0.0798,  0.0779,  0.0800,\n",
       "                        0.0799,  0.0804,  0.0809,  0.0806,  0.0793,  0.0798,  0.0773,  0.0803,\n",
       "                        0.0804,  0.0802,  0.0790,  0.0795,  0.0793,  0.0782,  0.0796,  0.0789,\n",
       "                        0.0802], device='cuda:0')),\n",
       "              ('lstm_1.weight_ih_l0',\n",
       "               tensor([[-0.0351, -0.0700, -0.0800,  ..., -0.0977, -0.0258, -0.1355],\n",
       "                       [-0.0022,  0.0459,  0.0708,  ...,  0.0424,  0.0339,  0.0617],\n",
       "                       [-0.0444,  0.0879,  0.0121,  ...,  0.0378,  0.0342,  0.0658],\n",
       "                       ...,\n",
       "                       [-0.0644,  0.0448, -0.0952,  ..., -0.0516, -0.0368,  0.0337],\n",
       "                       [ 0.0860, -0.0261,  0.0257,  ..., -0.0062, -0.0360,  0.0421],\n",
       "                       [ 0.1072, -0.0296,  0.0754,  ...,  0.0051,  0.0600,  0.0367]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_1.weight_hh_l0',\n",
       "               tensor([[ 0.1106, -0.2495, -0.1522,  ..., -0.0976,  0.0371, -0.0160],\n",
       "                       [-0.1098, -0.1523,  0.1563,  ..., -0.0980, -0.2193,  0.0157],\n",
       "                       [ 0.0928,  0.1105, -0.1043,  ...,  0.0894, -0.0668,  0.1418],\n",
       "                       ...,\n",
       "                       [-0.1686,  0.0044, -0.0329,  ..., -0.0552, -0.0185,  0.0026],\n",
       "                       [-0.1216, -0.1545,  0.0904,  ...,  0.0906, -0.0462,  0.0399],\n",
       "                       [-0.1122, -0.0077,  0.0799,  ...,  0.2080,  0.0693, -0.1315]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_1.bias_ih_l0',\n",
       "               tensor([ 0.0923, -0.0916, -0.0048,  ...,  0.0209,  0.0513, -0.0074], device='cuda:0')),\n",
       "              ('lstm_1.bias_hh_l0',\n",
       "               tensor([ 0.0658, -0.0647,  0.0094,  ...,  0.0617,  0.0490, -0.0762], device='cuda:0')),\n",
       "              ('layer_norm_1.weight',\n",
       "               tensor([0.8880, 0.8467, 0.8686, 0.9710, 0.8323, 0.8708, 0.9135, 0.8796, 0.9370,\n",
       "                       0.8363, 0.8493, 0.8703, 0.9035, 0.9452, 0.9336, 0.9119, 0.8896, 0.8940,\n",
       "                       0.8449, 0.8685, 0.9286, 0.8794, 0.8950, 0.8557, 0.9041, 0.8987, 0.8730,\n",
       "                       0.8924, 0.8511, 0.8610, 0.9016, 0.8735, 0.8544, 0.9726, 0.8586, 0.9280,\n",
       "                       0.9579, 0.8731, 0.8808, 0.9895, 0.9115, 0.9108, 0.8453, 0.9280, 0.8938,\n",
       "                       0.9155, 0.8322, 0.8517, 0.8538, 0.8955, 0.8580, 0.9039, 0.9518, 0.8677,\n",
       "                       0.8900, 0.8753, 0.8623, 0.9159, 0.8703, 0.8607, 0.9057, 0.8598, 0.9236,\n",
       "                       0.9007, 0.8519, 0.9571, 0.9585, 0.8459, 0.9997, 0.8797, 0.8993, 0.8454,\n",
       "                       0.9734, 0.8574, 0.9176, 0.9220, 0.8973, 0.9751, 0.8611, 0.8896, 0.9023,\n",
       "                       0.9408, 0.8948, 0.9224, 0.9132, 0.8386, 0.8625, 0.8910, 0.9141, 0.8693,\n",
       "                       0.8920, 0.8491, 0.9085, 0.9068, 0.9664, 0.9380, 0.9252, 0.9495, 0.9707,\n",
       "                       0.9356, 0.8978, 0.8618, 0.8825, 0.8941, 0.9106, 0.9188, 0.8950, 0.9452,\n",
       "                       0.9542, 0.9685, 0.8491, 0.9010, 0.8662, 0.8535, 0.9305, 0.9317, 0.8553,\n",
       "                       0.8379, 0.9103, 1.0643, 0.9266, 0.9305, 0.8682, 0.9271, 0.9212, 0.9786,\n",
       "                       0.9674, 0.9186, 0.8878, 0.9213, 0.9115, 0.8881, 0.8826, 0.8942, 0.9510,\n",
       "                       0.8563, 0.8901, 0.9212, 0.9249, 0.8954, 0.8493, 0.9048, 0.8876, 0.8742,\n",
       "                       0.8490, 0.8907, 0.8943, 0.9701, 0.8012, 0.9886, 0.9244, 0.9389, 0.9150,\n",
       "                       0.9307, 0.9483, 0.9125, 0.9454, 0.8565, 0.8797, 0.8655, 0.9555, 0.9068,\n",
       "                       0.8951, 0.8635, 0.8921, 0.9429, 0.8955, 0.8053, 0.9162, 0.9174, 0.9112,\n",
       "                       0.8438, 0.8923, 0.7833, 0.9165, 0.8534, 1.0029, 0.9578, 0.8387, 1.0201,\n",
       "                       0.8883, 0.8907, 0.8745, 0.8976, 0.9711, 0.9237, 0.8976, 0.9478, 0.8876,\n",
       "                       0.9158, 0.9351, 0.9915, 0.8903, 0.9178, 0.9216, 0.8371, 0.8887, 0.9434,\n",
       "                       0.9403, 0.9839, 0.9132, 0.8925, 0.8905, 0.9234, 0.9414, 0.8807, 0.8723,\n",
       "                       0.8286, 0.9250, 0.9485, 0.9565, 0.8851, 0.9358, 0.8931, 0.9035, 0.8433,\n",
       "                       0.9078, 0.8763, 0.8821, 0.9883, 0.8473, 0.8958, 0.9071, 0.8635, 0.9096,\n",
       "                       0.8844, 0.8478, 0.8377, 0.9502, 0.9524, 0.9836, 0.8921, 0.8425, 0.9124,\n",
       "                       0.9637, 0.8803, 0.8976, 0.9784, 0.9182, 0.8838, 0.8464, 0.8623, 0.9927,\n",
       "                       0.8185, 0.8935, 0.8796, 0.9126, 0.8050, 0.9231, 0.9225, 0.8884, 0.8372,\n",
       "                       0.8703, 0.9376, 0.9430, 0.9059, 0.8725, 0.9143, 0.9303, 0.8439, 1.0095,\n",
       "                       0.8069, 0.8914, 0.9050, 0.8794, 0.8585, 0.9143, 0.9007, 0.8626, 0.9096,\n",
       "                       0.9156, 0.9093, 0.8791, 0.9155, 0.9055, 0.8446, 0.9420, 0.9080, 0.9596,\n",
       "                       0.8762, 0.9241, 0.8856, 0.9317, 0.9360, 0.8578, 0.9446, 0.8839, 0.9267,\n",
       "                       0.9464, 0.9322, 0.8924, 0.9301, 0.8336, 0.8787, 0.9006, 0.9473, 0.9151,\n",
       "                       0.8901, 0.8267, 0.8295], device='cuda:0')),\n",
       "              ('layer_norm_1.bias',\n",
       "               tensor([ 0.0593, -0.0108, -0.0600, -0.0502,  0.0140, -0.0041, -0.0003, -0.0024,\n",
       "                        0.0013, -0.0197, -0.0077,  0.0046, -0.0226, -0.0541,  0.0409,  0.0655,\n",
       "                       -0.0241, -0.0256,  0.0197,  0.0208, -0.0252, -0.0242, -0.0052, -0.0093,\n",
       "                       -0.0373,  0.0143, -0.0250,  0.0082, -0.0424, -0.0013,  0.0513,  0.0886,\n",
       "                        0.0092, -0.0294,  0.0301,  0.0045,  0.0191,  0.0658, -0.0755, -0.0256,\n",
       "                        0.0310,  0.0159,  0.0081,  0.0459, -0.0294,  0.0146, -0.0124, -0.0077,\n",
       "                        0.0730, -0.0109, -0.0129, -0.0085, -0.0200, -0.0256, -0.0771,  0.0579,\n",
       "                        0.0047,  0.0455,  0.0086,  0.0731,  0.0267, -0.0253, -0.0663, -0.0248,\n",
       "                        0.0032, -0.0119, -0.0024,  0.0117,  0.0466,  0.0561,  0.0041, -0.0053,\n",
       "                        0.0706, -0.0137, -0.0131, -0.0269,  0.0163,  0.0080, -0.0031,  0.0298,\n",
       "                        0.0509,  0.0177, -0.0282,  0.0300,  0.0620,  0.0468, -0.0336, -0.0150,\n",
       "                       -0.0460,  0.0204, -0.0852,  0.0493,  0.0054, -0.0050, -0.0198,  0.0246,\n",
       "                       -0.0544,  0.0241,  0.0756, -0.0221, -0.0017,  0.0311,  0.0581, -0.0048,\n",
       "                       -0.0508, -0.0200, -0.0189,  0.0208, -0.0609,  0.0196,  0.0131,  0.0024,\n",
       "                        0.0052, -0.0349, -0.0222, -0.0029, -0.0564,  0.0187, -0.0342,  0.0171,\n",
       "                        0.0308,  0.0222, -0.0130,  0.0316, -0.0184, -0.0342,  0.0202,  0.0576,\n",
       "                        0.0023,  0.0052, -0.0126, -0.0499, -0.0029,  0.0119,  0.0005, -0.0008,\n",
       "                        0.0262,  0.0201,  0.0031, -0.0175,  0.0076,  0.0423, -0.0515,  0.0285,\n",
       "                        0.0099, -0.0207, -0.0061,  0.0034,  0.0408,  0.1396, -0.0106, -0.0345,\n",
       "                       -0.0543,  0.0236, -0.0137, -0.0650,  0.0446,  0.0010,  0.1193,  0.0354,\n",
       "                       -0.0314, -0.0220, -0.0921,  0.0337,  0.0628,  0.0095,  0.0253, -0.0262,\n",
       "                       -0.0248,  0.0231,  0.0176, -0.0044, -0.0112,  0.0029,  0.0018,  0.0439,\n",
       "                       -0.0333, -0.0069, -0.0035,  0.0109, -0.0557,  0.0082, -0.0353,  0.0217,\n",
       "                       -0.0250,  0.0329, -0.0290, -0.0316, -0.0002,  0.0393, -0.0186, -0.0010,\n",
       "                       -0.0181, -0.0517, -0.0304, -0.0167, -0.0212, -0.0513, -0.0138, -0.1110,\n",
       "                        0.0042,  0.0519,  0.0298,  0.0308,  0.0032, -0.0874, -0.0155,  0.0277,\n",
       "                       -0.0199, -0.0136,  0.0093, -0.0550, -0.0146, -0.0573, -0.0106,  0.0223,\n",
       "                        0.0399, -0.0508, -0.0243, -0.0350, -0.0186, -0.0492,  0.0077,  0.0311,\n",
       "                       -0.0104,  0.0118,  0.0205, -0.0110,  0.0237, -0.0478, -0.0211,  0.0054,\n",
       "                        0.0443, -0.0303,  0.0016,  0.0059,  0.0118,  0.0790,  0.0614, -0.0041,\n",
       "                        0.0200,  0.0415,  0.0595,  0.0382, -0.0034,  0.0487, -0.0211, -0.0263,\n",
       "                        0.0262, -0.0839,  0.0368, -0.0304, -0.0070,  0.0498,  0.0351, -0.0363,\n",
       "                       -0.0753,  0.0199,  0.0244,  0.0240,  0.0698,  0.0248, -0.0529,  0.0306,\n",
       "                       -0.0284, -0.0426, -0.0735,  0.0551,  0.0597,  0.0480,  0.0265,  0.0083,\n",
       "                       -0.0106,  0.0037,  0.0377,  0.0093, -0.0387, -0.0306, -0.0060, -0.0068,\n",
       "                        0.0033, -0.0589,  0.0016,  0.0235, -0.0304, -0.0147, -0.0290, -0.0092,\n",
       "                       -0.0526, -0.0348, -0.0632, -0.0383, -0.0124,  0.0809, -0.0289,  0.0254,\n",
       "                       -0.0379, -0.0382, -0.0075, -0.0091], device='cuda:0')),\n",
       "              ('lstm_2.weight_ih_l0',\n",
       "               tensor([[ 0.0473, -0.0474, -0.0574,  ..., -0.0089, -0.0265, -0.0648],\n",
       "                       [-0.0005,  0.0200,  0.0871,  ..., -0.0124,  0.1046,  0.0424],\n",
       "                       [-0.1403, -0.0246, -0.0728,  ...,  0.0456,  0.0230, -0.0558],\n",
       "                       ...,\n",
       "                       [ 0.0159,  0.0092,  0.0622,  ...,  0.0480,  0.0082, -0.0015],\n",
       "                       [-0.0842,  0.0032, -0.0129,  ..., -0.0084,  0.0403,  0.0378],\n",
       "                       [ 0.0813, -0.0449,  0.0698,  ..., -0.0115,  0.0007,  0.0041]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_2.weight_hh_l0',\n",
       "               tensor([[ 0.0981,  0.1813, -0.0017,  ..., -0.1760,  0.0433,  0.1619],\n",
       "                       [ 0.0350, -0.1132, -0.0791,  ..., -0.0268, -0.1007, -0.0612],\n",
       "                       [ 0.0593, -0.0499,  0.1721,  ..., -0.0684,  0.1768, -0.1781],\n",
       "                       ...,\n",
       "                       [-0.1114,  0.1030, -0.1516,  ...,  0.0533, -0.1365, -0.1146],\n",
       "                       [-0.0756, -0.1713, -0.0103,  ..., -0.0718, -0.1379, -0.0932],\n",
       "                       [ 0.0592, -0.1616,  0.0840,  ...,  0.0941,  0.0507,  0.0763]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_2.bias_ih_l0',\n",
       "               tensor([ 0.0263, -0.0272,  0.0299,  ..., -0.0378, -0.0706, -0.0245], device='cuda:0')),\n",
       "              ('lstm_2.bias_hh_l0',\n",
       "               tensor([-0.0551, -0.0923, -0.0233,  ..., -0.0035, -0.0398,  0.0465], device='cuda:0')),\n",
       "              ('layer_norm_2.weight',\n",
       "               tensor([1.3432, 1.3098, 1.2825, 1.2929, 1.2884, 1.3570, 1.2602, 1.2915, 1.2277,\n",
       "                       1.4896, 1.2984, 1.0822, 1.3064, 1.3986, 1.4282, 1.3810, 1.2831, 1.3243,\n",
       "                       1.4581, 1.1992, 1.3394, 1.2023, 1.3203, 1.3362, 1.3093, 1.2437, 1.3167,\n",
       "                       1.4072, 1.3207, 1.4227, 1.2634, 1.4245, 1.2749, 1.3254, 1.2973, 1.2304,\n",
       "                       1.1084, 1.4197, 1.2155, 1.2535, 1.4174, 1.2556, 1.1878, 1.2536, 1.2630,\n",
       "                       1.2885, 1.3330, 1.4042, 1.3422, 1.5186, 1.4199, 1.3416, 1.3677, 1.2130,\n",
       "                       1.4153, 1.2202, 1.2789, 1.4610, 1.4525, 1.4027, 1.2099, 1.2852, 1.3070,\n",
       "                       1.2128, 1.4114, 1.4042, 1.2521, 1.1205, 1.1928, 1.2244, 1.4630, 1.5525,\n",
       "                       1.3065, 1.3913, 1.4183, 1.4988, 1.2019, 1.3764, 1.3282, 1.3144, 1.3043,\n",
       "                       1.4313, 1.6120, 1.1610, 1.0757, 1.3452, 1.2371, 1.0887, 1.2398, 1.1801,\n",
       "                       1.2545, 1.2273, 1.4675, 1.2221, 1.2349, 1.3168, 1.2020, 1.2475, 1.4674,\n",
       "                       1.5240, 1.3661, 1.3689, 1.2245, 1.4176, 1.2490, 1.2872, 1.3259, 1.2927,\n",
       "                       1.4222, 1.1889, 1.2058, 1.3543, 1.3659, 1.2098, 1.3500, 1.2999, 1.3436,\n",
       "                       1.2318, 1.5261, 1.2776, 1.3563, 1.2373, 1.1938, 1.2569, 1.0836, 1.2735,\n",
       "                       1.3148, 1.2834, 1.1752, 1.3788, 1.3899, 1.2946, 1.0916, 1.2203, 1.3662,\n",
       "                       1.3496, 1.4616, 1.3370, 1.1363, 1.1986, 1.5636, 1.2895, 1.1698, 1.2504,\n",
       "                       1.3660, 1.1799, 1.2301, 1.3502, 1.2399, 1.1404, 1.3420, 1.3552, 1.3644,\n",
       "                       1.2764, 1.2673, 1.3391, 1.3779, 1.5188, 1.2918, 1.1553, 1.1881, 1.3447,\n",
       "                       1.3370, 1.2893, 1.3111, 1.2120, 1.3706, 1.3355, 1.3792, 1.2837, 1.2050,\n",
       "                       1.1917, 1.2359, 1.2207, 1.4590, 1.2911, 1.3767, 1.1410, 1.3703, 1.3191,\n",
       "                       1.2998, 1.4940, 1.2133, 1.2809, 1.2444, 1.2331, 1.2992, 1.3880, 1.3865,\n",
       "                       1.3991, 1.3363, 1.3440, 1.2364, 1.2926, 1.4422, 1.3563, 1.3017, 1.1512,\n",
       "                       1.2605, 1.2786, 1.3453, 1.3703, 1.4294, 1.5775, 1.2206, 1.2102, 1.2600,\n",
       "                       1.3505, 1.2652, 1.3709, 1.4214, 1.2291, 1.2923, 1.2626, 1.3464, 1.1964,\n",
       "                       1.3620, 1.2349, 1.2674, 1.3038, 1.1628, 1.2119, 1.1394, 1.3823, 1.4844,\n",
       "                       1.1951, 1.4397, 1.2475, 1.1830, 1.3780, 1.2251, 1.3398, 1.3413, 1.2646,\n",
       "                       1.1749, 1.3406, 1.5434, 1.3481, 1.4373, 1.4740, 1.1797, 1.3307, 1.3740,\n",
       "                       1.4005, 1.3116, 1.2060, 1.3556, 1.3019, 1.1475, 1.2877, 1.3187, 1.2546,\n",
       "                       1.1730, 1.2729, 1.3096, 1.3389, 1.2012, 1.3946, 1.2930, 1.2291, 1.3050,\n",
       "                       1.2619, 1.3793, 1.1783, 1.3362, 1.1084, 1.3852, 1.3049, 1.3287, 1.2414,\n",
       "                       1.4237, 1.5673, 1.4767, 1.3703, 1.3038, 1.2590, 1.3564, 1.2286, 1.3617,\n",
       "                       1.3100, 1.2460, 1.4858, 1.1795, 1.4014, 1.1377, 1.2941, 1.2707, 1.2899,\n",
       "                       1.2927, 1.2053, 1.2946, 1.3042, 1.0653, 1.2431, 1.3363, 1.2632, 1.2418,\n",
       "                       1.3676, 1.2839, 1.3214], device='cuda:0')),\n",
       "              ('layer_norm_2.bias',\n",
       "               tensor([-0.0229, -0.0210, -0.0434, -0.0344,  0.1537,  0.0755, -0.0195, -0.0469,\n",
       "                        0.0264,  0.1463, -0.0183, -0.0545,  0.0575, -0.1266,  0.1020, -0.0406,\n",
       "                       -0.0794, -0.0664, -0.0600,  0.0010,  0.1227, -0.0296, -0.0430, -0.1458,\n",
       "                       -0.0275,  0.0771, -0.1647, -0.1802,  0.0673,  0.1364,  0.0676, -0.0657,\n",
       "                       -0.0712, -0.1499,  0.0321,  0.0748, -0.0216, -0.0177,  0.0602, -0.0622,\n",
       "                       -0.0123,  0.0988, -0.0514, -0.0381, -0.0316,  0.2137, -0.0533, -0.0409,\n",
       "                       -0.1416, -0.1070,  0.0714, -0.1218, -0.0525, -0.0352,  0.0146, -0.0187,\n",
       "                       -0.1521,  0.0880,  0.0863,  0.0610, -0.0582, -0.1041,  0.0923,  0.0048,\n",
       "                        0.1732, -0.0423,  0.0224,  0.0346,  0.0345,  0.0193,  0.0071,  0.1468,\n",
       "                        0.0949, -0.1572, -0.0290, -0.0691,  0.0132, -0.0411,  0.0213,  0.0855,\n",
       "                       -0.0627,  0.0807, -0.0124, -0.0185, -0.0155,  0.0658,  0.0091, -0.0469,\n",
       "                        0.0581,  0.0485,  0.0789,  0.0103, -0.0239,  0.0209, -0.0272, -0.1630,\n",
       "                       -0.0761, -0.0871, -0.1037,  0.1333, -0.1051, -0.0693, -0.0187, -0.1564,\n",
       "                        0.0745,  0.0658,  0.1782,  0.1267, -0.1481, -0.0722,  0.0169,  0.2002,\n",
       "                       -0.0753,  0.1029, -0.1165,  0.1444,  0.0942,  0.0502, -0.0022, -0.0656,\n",
       "                        0.1017, -0.0543,  0.0847,  0.0899,  0.0397, -0.0911,  0.0815,  0.0512,\n",
       "                        0.0063,  0.1003, -0.2309,  0.0853, -0.0327, -0.0283, -0.0779, -0.1093,\n",
       "                       -0.0950,  0.0604,  0.0523, -0.1028,  0.0452, -0.0037, -0.0133,  0.0234,\n",
       "                        0.0320, -0.0289, -0.0130,  0.0007,  0.0709,  0.0135,  0.0073,  0.0935,\n",
       "                       -0.0878, -0.0262,  0.1035, -0.0653, -0.1049, -0.0490, -0.0307,  0.0576,\n",
       "                        0.0508, -0.0473,  0.0557,  0.1147,  0.0040,  0.0214,  0.2164, -0.0716,\n",
       "                        0.1452,  0.0264, -0.0546, -0.0656, -0.1369, -0.0137, -0.2587,  0.0756,\n",
       "                       -0.2010,  0.0193, -0.0896, -0.2305,  0.0006, -0.0692, -0.0633, -0.0022,\n",
       "                        0.0877, -0.0081, -0.1478,  0.0222, -0.0481, -0.0932, -0.0570,  0.0941,\n",
       "                        0.0968,  0.0337, -0.1593, -0.0613,  0.1326, -0.0336,  0.0824,  0.0485,\n",
       "                       -0.1320,  0.0084, -0.0807, -0.0801, -0.0869,  0.0203,  0.0455,  0.0288,\n",
       "                       -0.0402, -0.0365, -0.1155,  0.0116,  0.1246, -0.0654,  0.0686,  0.0318,\n",
       "                        0.1575,  0.0299,  0.1261, -0.0612, -0.0061,  0.0096, -0.0328, -0.1942,\n",
       "                       -0.0636,  0.0464,  0.0102,  0.0684,  0.0150,  0.1004, -0.0688, -0.0021,\n",
       "                        0.1212,  0.0238,  0.0364, -0.1625, -0.0330,  0.1004, -0.0749, -0.0978,\n",
       "                        0.0944,  0.0240,  0.0273, -0.0869,  0.0937, -0.0008,  0.0854,  0.1793,\n",
       "                        0.0370, -0.1712,  0.0311, -0.0898,  0.0193,  0.1827, -0.1052, -0.0455,\n",
       "                       -0.0305,  0.1032, -0.0242,  0.0649, -0.0653, -0.0021, -0.0146, -0.1440,\n",
       "                       -0.0992,  0.0535,  0.0465, -0.1167,  0.0708, -0.0573, -0.1208, -0.0420,\n",
       "                        0.0535,  0.0493,  0.0894,  0.0248, -0.0906, -0.0538, -0.0319,  0.1368,\n",
       "                        0.0433,  0.1914, -0.0085, -0.2419, -0.0897, -0.0286, -0.0715,  0.1068,\n",
       "                       -0.1138,  0.0016, -0.0880,  0.0671, -0.0511, -0.0625, -0.0684,  0.0011,\n",
       "                       -0.1337,  0.0646,  0.0171, -0.0500], device='cuda:0')),\n",
       "              ('lstm_3.weight_ih_l0',\n",
       "               tensor([[-0.0948,  0.0972,  0.0896,  ...,  0.0706,  0.0337, -0.1056],\n",
       "                       [ 0.0092, -0.0684,  0.0709,  ..., -0.0110, -0.1167,  0.0596],\n",
       "                       [ 0.2175, -0.0229,  0.0566,  ..., -0.0362, -0.0108, -0.1864],\n",
       "                       ...,\n",
       "                       [ 0.0506,  0.0971, -0.0700,  ..., -0.0543,  0.1698, -0.1004],\n",
       "                       [-0.0170,  0.0034, -0.0869,  ...,  0.0943, -0.1669, -0.0486],\n",
       "                       [ 0.0625, -0.1633, -0.0147,  ...,  0.1259,  0.1586,  0.0885]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_3.weight_hh_l0',\n",
       "               tensor([[ 0.0028,  0.0067,  0.2176,  ...,  0.0527,  0.1238,  0.0926],\n",
       "                       [-0.1227,  0.2081, -0.0882,  ...,  0.1541, -0.0650,  0.1187],\n",
       "                       [-0.1372,  0.0907, -0.1201,  ..., -0.3930, -0.0737, -0.0749],\n",
       "                       ...,\n",
       "                       [-0.1381, -0.1073, -0.1516,  ..., -0.0546,  0.0235,  0.1293],\n",
       "                       [-0.0006, -0.1859,  0.3009,  ...,  0.0737,  0.0393, -0.0940],\n",
       "                       [ 0.1819, -0.0344, -0.1185,  ...,  0.3629, -0.0770, -0.2914]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_3.bias_ih_l0',\n",
       "               tensor([ 0.0370,  0.0545,  0.0284,  ..., -0.0035,  0.0021,  0.0880], device='cuda:0')),\n",
       "              ('lstm_3.bias_hh_l0',\n",
       "               tensor([-0.0103,  0.0591,  0.0213,  ...,  0.0668, -0.0031,  0.1188], device='cuda:0')),\n",
       "              ('layer_norm_3.weight',\n",
       "               tensor([2.7831, 2.5457, 2.4253, 2.8368, 2.5633, 2.7003, 2.8304, 2.6769, 2.6307,\n",
       "                       2.8568, 2.4815, 2.3989, 2.0548, 2.7587, 2.4905, 2.1350, 2.6987, 2.7363,\n",
       "                       2.2834, 2.3522, 2.5253, 2.6008, 2.7077, 2.2042, 2.7807, 2.4221, 2.8989,\n",
       "                       2.9406, 2.4033, 2.9366, 2.7244, 2.8968, 2.1900, 2.7125, 2.4475, 2.8099,\n",
       "                       2.5659, 2.6740, 2.3694, 2.6347, 2.6176, 2.8033, 2.8458, 2.6481, 2.4477,\n",
       "                       2.8692, 2.3301, 2.8260, 2.8650, 2.2033, 2.4596, 2.8647, 2.4895, 2.7862,\n",
       "                       2.7520, 2.4997, 2.5278, 2.7334, 2.0447, 2.3301, 2.6836, 2.8836, 2.8844,\n",
       "                       2.6545, 2.7370, 2.3793, 2.7480, 2.8474, 2.3805, 2.7848, 2.8645, 2.4728,\n",
       "                       2.0850, 2.6550, 2.9364, 2.4192, 2.7552, 2.3892, 2.7209, 2.7037, 2.6513,\n",
       "                       2.8385, 2.7678, 2.7150, 2.3137, 2.2114, 2.8542, 2.9458, 2.9552, 2.8270,\n",
       "                       1.9298, 2.8308, 2.8228, 2.6028, 2.9791, 2.5952, 2.4542, 2.6284, 2.7999,\n",
       "                       2.8357, 2.8606, 2.5064, 2.4463, 2.9838, 3.1341, 1.7858, 2.9405, 2.6422,\n",
       "                       2.5292, 2.4199, 2.4289, 2.1206, 2.5447, 2.7389, 2.9039, 2.6938, 2.1911,\n",
       "                       2.1103, 2.7529, 2.4073, 2.7615, 2.7358, 2.6085, 2.8156, 2.7448, 2.6759,\n",
       "                       2.6656, 2.4509, 2.9924, 2.2265, 2.8049, 2.6794, 2.2024, 2.8255, 2.6611,\n",
       "                       2.5950, 2.8371, 2.5230, 2.2127, 2.6156, 2.7955, 1.7968, 2.7178, 2.6766,\n",
       "                       2.3763, 2.9065, 2.3203, 2.1668, 2.8771, 2.5505, 2.6425, 2.6277, 2.4786,\n",
       "                       2.1551, 2.9064, 2.6798, 2.7284, 2.6795, 2.4375, 2.6646, 2.1541, 2.7496,\n",
       "                       2.5057, 2.6291, 2.7119, 2.8183, 2.7954, 2.8243, 2.2873, 2.5511, 2.8441,\n",
       "                       2.7024, 2.6454, 2.6199, 2.2251, 2.6622, 2.9297, 2.9409, 2.7953, 2.9511,\n",
       "                       2.5290, 2.5791, 2.2985, 2.4716, 2.9502, 3.0115, 2.3801, 2.9617, 2.7129,\n",
       "                       2.4397, 2.8567, 2.8693, 2.3538, 2.7304, 2.7271, 2.7669, 2.6740, 2.6392,\n",
       "                       2.7612, 2.8511, 2.5852, 2.1147, 2.3994, 2.5387, 1.9126, 2.7104, 3.0630,\n",
       "                       2.5176, 2.7701, 2.7755, 2.6367, 2.9373, 2.7008, 2.6143, 2.4830, 2.6518,\n",
       "                       2.4969, 2.3466, 1.7312, 2.0368, 2.9934, 2.8587, 2.7036, 2.3103, 2.6323,\n",
       "                       2.8472, 2.9802, 2.6658, 2.7085, 2.3195, 2.6361, 2.8832, 2.5760, 3.0169,\n",
       "                       2.7100, 2.2396, 2.6147, 2.4422, 2.5531, 2.7043, 2.8897, 2.3269, 2.6265,\n",
       "                       2.0022, 2.9559, 2.9972, 2.3586, 2.0588, 2.5021, 2.6839, 2.4726, 2.3620,\n",
       "                       2.2691, 2.5726, 2.6071, 2.7387, 2.7470, 2.6151, 2.8290, 2.9375, 2.8525,\n",
       "                       2.8168, 2.7345, 2.7346, 2.4060, 3.0331, 2.4227, 2.5489, 2.8675, 2.4239,\n",
       "                       2.4351, 2.9840, 2.8304, 2.7617, 2.7386, 2.3919, 2.3636, 2.6465, 2.8371,\n",
       "                       2.7637, 2.7220, 2.7838, 2.7458, 2.3371, 2.7622, 2.5897, 3.0293, 2.5722,\n",
       "                       2.8492, 2.6550, 2.6530, 2.3391, 2.4247, 2.7272, 2.8269, 2.9376, 2.2542,\n",
       "                       2.7924, 2.7236, 2.6771], device='cuda:0')),\n",
       "              ('layer_norm_3.bias',\n",
       "               tensor([ 0.1437, -0.5537,  0.0837,  0.8777, -0.1601, -0.4875,  0.0779, -0.0928,\n",
       "                       -0.3876, -0.4028, -0.7836, -0.0634,  0.5997, -0.3959,  0.6445, -0.3425,\n",
       "                        0.5786, -0.0406,  0.6953,  0.5927, -0.1748, -0.4118,  0.8437, -0.2184,\n",
       "                        0.4901,  0.4465,  0.6851, -0.7348, -0.4216, -0.6121, -0.4468, -0.7786,\n",
       "                       -0.3802, -0.6991,  0.3797,  0.3036,  0.1054,  0.4641, -0.2837, -0.2236,\n",
       "                        0.6872, -0.1283, -0.8620, -0.5991, -0.0196, -0.8485,  0.1881, -0.9458,\n",
       "                        0.4101,  0.4435, -0.8094, -0.3490, -0.1972, -0.8427,  0.8271, -0.6148,\n",
       "                        0.6416, -0.8566, -0.3973, -0.5302, -0.4167, -0.5694,  0.7098, -0.8163,\n",
       "                        0.7770, -0.3310,  0.0561, -0.5410,  0.1891,  0.0041, -0.2492, -0.4869,\n",
       "                       -0.2385,  0.1052,  0.8434, -0.3025,  0.4992, -0.5149, -0.0638,  0.6083,\n",
       "                        0.5513, -0.2716,  0.2565,  1.0248, -0.5899,  0.3265, -0.8319,  0.7881,\n",
       "                        0.6058,  0.1253,  0.0945, -0.6642, -0.1305,  0.7321,  0.6971, -0.1613,\n",
       "                        0.7538,  0.4082,  0.3194, -0.1863, -0.5809,  0.0693, -0.5179,  0.7770,\n",
       "                        0.0581, -0.6134,  0.2023,  0.7744,  0.5865, -0.1272,  0.5537,  0.4154,\n",
       "                       -0.2863,  0.3725, -0.5693,  0.3610, -0.8015,  0.5297, -0.2153, -0.2483,\n",
       "                       -0.3524,  0.2198,  0.8774,  0.4993, -0.2518, -0.2856,  0.0987, -0.1843,\n",
       "                       -0.6636,  0.5311, -0.7342, -0.5889,  0.7158,  0.4056, -0.2691,  0.8302,\n",
       "                        0.5721, -0.3200, -0.5599,  0.2738, -0.4422, -0.4839, -0.7056,  0.2779,\n",
       "                        0.7272, -0.6000, -0.6460, -0.1906,  0.5905, -0.6166, -0.6922, -0.6376,\n",
       "                       -0.5718, -0.4721,  0.6681, -0.8453, -0.7289,  0.2702, -0.6035,  0.4823,\n",
       "                       -0.0571,  0.5393, -0.6006,  0.2575, -0.1897, -0.4837,  0.6137, -0.4148,\n",
       "                        1.0287, -0.6501, -0.5390, -0.1851, -0.6375,  0.5783, -0.5710,  0.5098,\n",
       "                        0.7934,  0.8451, -0.6242,  0.8092, -0.0688,  0.2334,  0.3383,  0.7159,\n",
       "                        0.4309,  0.6121, -0.0149, -0.7156, -0.5873,  0.8783, -0.6950,  0.7258,\n",
       "                       -0.5253, -0.2105, -0.3969, -0.9440, -0.0162, -0.2258, -0.8288, -0.8114,\n",
       "                        0.4491, -0.5197, -0.0309,  0.3111, -0.1193, -0.0046, -0.9919,  0.2417,\n",
       "                        0.6564,  0.5766, -0.4080,  0.6274, -0.7326,  0.8424, -0.3022,  0.5956,\n",
       "                       -0.1529,  0.2068,  0.5753,  0.8128,  0.9714, -0.4114,  0.5589, -0.0808,\n",
       "                       -0.7754, -0.0053,  0.7302,  0.7214,  0.1778, -0.6245, -0.6771,  0.0319,\n",
       "                       -0.7076,  1.0186, -0.2910, -0.7400,  0.6387, -0.1240,  0.4457,  0.5605,\n",
       "                        0.7018, -0.8552, -0.7343,  0.7487,  0.7302, -0.7364, -0.7796,  0.8244,\n",
       "                       -0.4429,  0.6084, -0.4640,  0.1043,  0.0412,  0.3344,  0.0300,  0.7601,\n",
       "                       -0.9610,  0.4814,  0.7573, -0.3954,  0.5815,  0.1025,  0.3528,  0.0895,\n",
       "                       -0.0767, -0.6156,  0.2562,  0.4004,  0.7173, -0.7250,  0.5540,  0.7015,\n",
       "                        0.4847,  0.4526, -0.9744, -0.5705,  0.5191, -0.5403, -0.2131,  0.7578,\n",
       "                       -0.5178,  0.2827,  0.2147,  0.7126,  0.0530, -0.0241,  0.4669, -0.4480,\n",
       "                        0.7886, -0.7588,  0.5671,  0.3966, -0.6445, -0.6394, -0.2426,  0.6432,\n",
       "                       -0.3899, -0.0649, -0.5918,  0.7920], device='cuda:0')),\n",
       "              ('to_notes.weight',\n",
       "               tensor([[ 0.3195, -0.3148,  0.0436,  ..., -0.1616, -0.1362, -0.1711],\n",
       "                       [ 0.0097,  0.0941, -0.0537,  ..., -0.0476, -0.0768,  0.0073],\n",
       "                       [-0.0530,  0.0372,  0.0408,  ...,  0.0103,  0.0100,  0.1209],\n",
       "                       ...,\n",
       "                       [-0.0688,  0.0444, -0.0454,  ..., -0.0992, -0.0447,  0.0809],\n",
       "                       [-0.0023,  0.0175, -0.0478,  ..., -0.0489,  0.0286,  0.0735],\n",
       "                       [ 0.0137,  0.0550, -0.0419,  ..., -0.0020,  0.0419,  0.0563]],\n",
       "                      device='cuda:0')),\n",
       "              ('to_notes.bias',\n",
       "               tensor([-0.1159, -0.0512, -0.0667, -0.0484, -0.0341, -0.0840, -0.0600, -0.0903,\n",
       "                       -0.0610, -0.0807, -0.0598, -0.0877, -0.0307, -0.0793, -0.0671, -0.0411,\n",
       "                       -0.0924, -0.1020, -0.0366, -0.0675, -0.0954, -0.1142, -0.1752, -0.1431,\n",
       "                       -0.2074, -0.1754, -0.1831, -0.1754, -0.1914, -0.2118, -0.2077, -0.2556,\n",
       "                       -0.1790, -0.1743, -0.1981, -0.2470, -0.2536, -0.1485, -0.1863, -0.2205,\n",
       "                       -0.2031, -0.2704, -0.1959, -0.2360, -0.1954, -0.2360, -0.2714, -0.2259,\n",
       "                       -0.2263, -0.2834, -0.2386, -0.2788, -0.2243, -0.2322, -0.2221, -0.2651,\n",
       "                       -0.2696, -0.2389, -0.2188, -0.2149, -0.2666, -0.1452, -0.2813, -0.2339,\n",
       "                       -0.2787, -0.2152, -0.1990, -0.2843, -0.1998, -0.2658, -0.1857, -0.2588,\n",
       "                       -0.2233, -0.2565, -0.2382, -0.1956, -0.1869, -0.2541, -0.2797, -0.2370,\n",
       "                       -0.2065, -0.1870, -0.1804, -0.2428, -0.2725, -0.2657, -0.1944, -0.2774,\n",
       "                       -0.3005, -0.3190, -0.2091, -0.2701, -0.2297, -0.2966, -0.1444, -0.1453,\n",
       "                       -0.2272, -0.1471, -0.1540, -0.1504, -0.1740, -0.0848, -0.1562, -0.0420,\n",
       "                       -0.0979, -0.0314, -0.0836, -0.1067, -0.0850, -0.0969, -0.0381, -0.0825,\n",
       "                       -0.0521, -0.0784, -0.0997, -0.0758, -0.1080, -0.0459, -0.0521, -0.0633,\n",
       "                       -0.0870, -0.0922, -0.0527, -0.0720, -0.0557, -0.0425, -0.0592, -0.0870,\n",
       "                       -0.0715], device='cuda:0'))]),\n",
       " 'opt_dict': <bound method Optimizer.state_dict of Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     initial_lr: 0.001\n",
       "     lr: 0.0005000000000000002\n",
       "     weight_decay: 0\n",
       " )>}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 5852,\n",
       " 'loss': 4.267449378967285,\n",
       " 'recall': 0.11602499336004257,\n",
       " 'acc': 0.30198749899864197,\n",
       " 'state_dict': OrderedDict([('layer_norm_0.weight',\n",
       "               tensor([0.1823, 0.1674, 0.1664, 0.1662, 0.1668, 0.1673, 0.1673, 0.1673, 0.1673,\n",
       "                       0.1675, 0.1668, 0.1675, 0.1669, 0.1672, 0.1666, 0.1662, 0.1672, 0.1665,\n",
       "                       0.1671, 0.3389, 0.3302, 0.4213, 0.4375, 0.4380, 0.4526, 0.4766, 0.4160,\n",
       "                       0.4030, 0.4043, 0.3677, 0.4028, 0.3377, 0.3584, 0.3409, 0.3438, 0.3142,\n",
       "                       0.3382, 0.3211, 0.2945, 0.2833, 0.2957, 0.2721, 0.3163, 0.2629, 0.2783,\n",
       "                       0.2841, 0.2731, 0.2829, 0.2744, 0.2585, 0.2632, 0.2845, 0.3003, 0.2764,\n",
       "                       0.2993, 0.2945, 0.2887, 0.3151, 0.2816, 0.2545, 0.2761, 0.2854, 0.2921,\n",
       "                       0.2617, 0.2768, 0.2775, 0.2648, 0.2581, 0.2528, 0.2651, 0.2705, 0.2517,\n",
       "                       0.2477, 0.2565, 0.2398, 0.2829, 0.2537, 0.2502, 0.2547, 0.2707, 0.2805,\n",
       "                       0.2757, 0.2672, 0.3031, 0.2927, 0.3018, 0.2981, 0.3248, 0.3169, 0.3341,\n",
       "                       0.3175, 0.3413, 0.3279, 0.3451, 0.3600, 0.3737, 0.3756, 0.4342, 0.4084,\n",
       "                       0.4034, 0.4250, 0.3607, 0.2941, 0.2883, 0.2307, 0.2099, 0.1828, 0.1804,\n",
       "                       0.1848, 0.1674, 0.1676, 0.1670, 0.1676, 0.1664, 0.1671, 0.1675, 0.1667,\n",
       "                       0.1670, 0.1688, 0.1673, 0.1666, 0.1673, 0.1674, 0.1672, 0.1672, 0.1682,\n",
       "                       0.1667, 0.1673, 0.1669])),\n",
       "              ('layer_norm_0.bias',\n",
       "               tensor([-0.0062,  0.0019,  0.0020,  0.0020,  0.0020,  0.0020,  0.0019,  0.0019,\n",
       "                        0.0020,  0.0020,  0.0020,  0.0019,  0.0020,  0.0020,  0.0020,  0.0020,\n",
       "                        0.0020,  0.0020,  0.0020,  0.0022,  0.0020,  0.0023,  0.0022,  0.0021,\n",
       "                        0.0021,  0.0021,  0.0017,  0.0017,  0.0016,  0.0013,  0.0015,  0.0013,\n",
       "                        0.0012,  0.0009,  0.0008,  0.0003,  0.0004,  0.0003,  0.0004,  0.0003,\n",
       "                        0.0004, -0.0001, -0.0002, -0.0003, -0.0003, -0.0002, -0.0002, -0.0004,\n",
       "                       -0.0005, -0.0007, -0.0001, -0.0003, -0.0003, -0.0008, -0.0006, -0.0010,\n",
       "                       -0.0006, -0.0006, -0.0008, -0.0005, -0.0008, -0.0010, -0.0008, -0.0008,\n",
       "                       -0.0006, -0.0010, -0.0010, -0.0011, -0.0008, -0.0011, -0.0009, -0.0010,\n",
       "                       -0.0009, -0.0007, -0.0010, -0.0009, -0.0007, -0.0007, -0.0009, -0.0005,\n",
       "                       -0.0005, -0.0005, -0.0002, -0.0003,  0.0001,  0.0001,  0.0000,  0.0005,\n",
       "                        0.0007,  0.0006,  0.0009,  0.0012,  0.0014,  0.0012,  0.0015,  0.0015,\n",
       "                        0.0018,  0.0021,  0.0020,  0.0022,  0.0019,  0.0021,  0.0021,  0.0020,\n",
       "                        0.0020,  0.0018,  0.0019,  0.0020,  0.0020,  0.0020,  0.0019,  0.0019,\n",
       "                        0.0019,  0.0020,  0.0020,  0.0020,  0.0020,  0.0020,  0.0019,  0.0020,\n",
       "                        0.0020,  0.0020,  0.0020,  0.0020,  0.0019,  0.0020,  0.0020,  0.0020,\n",
       "                        0.0020])),\n",
       "              ('lstm_1.weight_ih_l0',\n",
       "               tensor([[-2.3683e-07,  1.7732e-09,  2.0852e-09,  ...,  2.6332e-09,\n",
       "                         7.2484e-09,  6.0558e-09],\n",
       "                       [-3.1175e-08,  1.2312e-09,  3.5258e-09,  ...,  1.1828e-10,\n",
       "                        -1.4482e-10,  2.8709e-09],\n",
       "                       [-6.1712e-07,  9.8494e-09, -2.3827e-09,  ...,  8.0424e-09,\n",
       "                         7.1718e-09,  1.0294e-08],\n",
       "                       ...,\n",
       "                       [-7.0849e-07,  9.8116e-09,  1.7399e-08,  ...,  1.2789e-08,\n",
       "                         4.2712e-09,  4.1008e-09],\n",
       "                       [-5.8090e-06,  1.0792e-07,  1.0381e-07,  ...,  1.9605e-07,\n",
       "                         1.1508e-07,  1.1572e-07],\n",
       "                       [-1.1606e-06,  3.3867e-08,  2.7984e-08,  ...,  3.5352e-08,\n",
       "                         3.2717e-08,  3.7669e-08]])),\n",
       "              ('lstm_1.weight_hh_l0',\n",
       "               tensor([[-1.4446e-11, -6.4354e-06, -2.6999e-11,  ..., -2.6850e-11,\n",
       "                        -1.0210e-10,  4.9163e-11],\n",
       "                       [-1.5385e-12, -6.8441e-11, -2.6860e-11,  ..., -4.2088e-12,\n",
       "                        -4.2496e-07,  2.4739e-11],\n",
       "                       [ 4.9668e-12, -4.0065e-11, -3.0180e-11,  ..., -9.1541e-12,\n",
       "                         5.6519e-11, -7.1215e-11],\n",
       "                       ...,\n",
       "                       [-9.0051e-10,  5.6079e-10,  1.1749e-10,  ..., -1.2114e-10,\n",
       "                         4.3357e-10, -8.3346e-11],\n",
       "                       [-2.7333e-10, -2.7241e-10, -2.2897e-10,  ..., -4.3405e-10,\n",
       "                        -7.6780e-10, -2.7184e-10],\n",
       "                       [-5.2204e-11, -1.7356e-10,  9.9315e-11,  ...,  1.0621e-07,\n",
       "                        -1.4083e-10,  7.6944e-12]])),\n",
       "              ('lstm_1.bias_ih_l0',\n",
       "               tensor([-1.7905e-07, -4.0978e-07,  2.5896e-06,  ..., -5.4285e-07,\n",
       "                       -8.8053e-06, -3.3008e-06])),\n",
       "              ('lstm_1.bias_hh_l0',\n",
       "               tensor([-1.2597e-07, -2.8679e-07,  8.2703e-08,  ..., -1.1418e-06,\n",
       "                       -8.9056e-06, -2.1597e-06])),\n",
       "              ('layer_norm_1.weight',\n",
       "               tensor([0.1477, 0.1268, 0.1421, 0.1950, 0.1199, 0.1390, 0.1659, 0.1424, 0.1752,\n",
       "                       0.1257, 0.1242, 0.1380, 0.1603, 0.1809, 0.1727, 0.1601, 0.1482, 0.1570,\n",
       "                       0.1238, 0.1395, 0.1677, 0.1424, 0.1513, 0.1329, 0.1580, 0.1502, 0.1364,\n",
       "                       0.1508, 0.1297, 0.1355, 0.1556, 0.1387, 0.1330, 0.1979, 0.1327, 0.1726,\n",
       "                       0.1915, 0.1406, 0.1492, 0.2113, 0.1584, 0.1620, 0.1255, 0.1701, 0.1524,\n",
       "                       0.1637, 0.1201, 0.1297, 0.1283, 0.1524, 0.1327, 0.1595, 0.1866, 0.1379,\n",
       "                       0.1490, 0.1401, 0.1315, 0.1608, 0.1376, 0.1333, 0.1591, 0.1346, 0.1703,\n",
       "                       0.1593, 0.1295, 0.1858, 0.1880, 0.1290, 0.2140, 0.1444, 0.1551, 0.1274,\n",
       "                       0.2000, 0.1339, 0.1649, 0.1700, 0.1539, 0.2031, 0.1337, 0.1494, 0.1591,\n",
       "                       0.1795, 0.1501, 0.1678, 0.1630, 0.1234, 0.1339, 0.1517, 0.1635, 0.1386,\n",
       "                       0.1592, 0.1282, 0.1597, 0.1588, 0.1934, 0.1756, 0.1713, 0.1837, 0.1971,\n",
       "                       0.1770, 0.1532, 0.1337, 0.1425, 0.1510, 0.1629, 0.1676, 0.1516, 0.1819,\n",
       "                       0.1856, 0.1945, 0.1293, 0.1531, 0.1362, 0.1280, 0.1718, 0.1732, 0.1331,\n",
       "                       0.1224, 0.1615, 0.2601, 0.1720, 0.1733, 0.1358, 0.1720, 0.1669, 0.2034,\n",
       "                       0.1951, 0.1673, 0.1473, 0.1649, 0.1604, 0.1501, 0.1446, 0.1521, 0.1842,\n",
       "                       0.1304, 0.1475, 0.1656, 0.1654, 0.1533, 0.1294, 0.1562, 0.1438, 0.1388,\n",
       "                       0.1288, 0.1528, 0.1519, 0.1979, 0.1026, 0.2082, 0.1666, 0.1792, 0.1631,\n",
       "                       0.1716, 0.1835, 0.1633, 0.1783, 0.1329, 0.1455, 0.1364, 0.1881, 0.1568,\n",
       "                       0.1506, 0.1333, 0.1529, 0.1816, 0.1530, 0.1040, 0.1629, 0.1652, 0.1607,\n",
       "                       0.1265, 0.1520, 0.0973, 0.1652, 0.1308, 0.2189, 0.1901, 0.1215, 0.2283,\n",
       "                       0.1476, 0.1487, 0.1525, 0.1530, 0.1967, 0.1685, 0.1557, 0.1826, 0.1475,\n",
       "                       0.1657, 0.1762, 0.2133, 0.1506, 0.1661, 0.1686, 0.1236, 0.1494, 0.1804,\n",
       "                       0.1808, 0.2032, 0.1652, 0.1505, 0.1464, 0.1696, 0.1769, 0.1467, 0.1427,\n",
       "                       0.1199, 0.1694, 0.1841, 0.1898, 0.1465, 0.1779, 0.1519, 0.1563, 0.1223,\n",
       "                       0.1590, 0.1417, 0.1442, 0.2072, 0.1257, 0.1560, 0.1586, 0.1355, 0.1633,\n",
       "                       0.1478, 0.1280, 0.1244, 0.1843, 0.1869, 0.2050, 0.1512, 0.1241, 0.1608,\n",
       "                       0.1941, 0.1427, 0.1545, 0.2047, 0.1641, 0.1478, 0.1293, 0.1341, 0.2115,\n",
       "                       0.1125, 0.1498, 0.1452, 0.1626, 0.1044, 0.1692, 0.1689, 0.1500, 0.1193,\n",
       "                       0.1390, 0.1790, 0.1780, 0.1572, 0.1381, 0.1642, 0.1732, 0.1249, 0.2195,\n",
       "                       0.1080, 0.1511, 0.1587, 0.1415, 0.1322, 0.1650, 0.1547, 0.1365, 0.1631,\n",
       "                       0.1659, 0.1617, 0.1418, 0.1610, 0.1570, 0.1238, 0.1856, 0.1595, 0.1900,\n",
       "                       0.1406, 0.1667, 0.1464, 0.1770, 0.1745, 0.1337, 0.1822, 0.1468, 0.1699,\n",
       "                       0.1838, 0.1714, 0.1535, 0.1740, 0.1195, 0.1425, 0.1543, 0.1832, 0.1624,\n",
       "                       0.1500, 0.1184, 0.1176])),\n",
       "              ('layer_norm_1.bias',\n",
       "               tensor([ 1.3580e-05, -2.4943e-05,  6.4471e-06, -7.7499e-05, -7.1030e-04,\n",
       "                        9.7173e-05, -7.6917e-05,  1.4128e-04, -3.4013e-04, -7.8458e-05,\n",
       "                       -1.8770e-05, -8.0378e-04,  5.9656e-05,  5.0716e-05,  5.3900e-05,\n",
       "                       -2.1458e-06,  2.0618e-05,  1.8339e-04,  2.7000e-04,  5.3630e-05,\n",
       "                        1.6108e-04, -6.2677e-05, -2.3365e-04, -8.6391e-04,  6.4344e-05,\n",
       "                        6.1821e-05,  2.1092e-04, -2.2869e-04,  7.8844e-05, -1.1778e-03,\n",
       "                        4.4516e-05,  2.1089e-07,  3.9103e-04,  4.6563e-04,  1.5587e-06,\n",
       "                       -1.2597e-03, -1.8994e-04, -7.0760e-05, -1.7765e-05,  5.6004e-04,\n",
       "                        1.1164e-04, -1.1763e-03, -6.3118e-05, -7.4023e-05,  6.0557e-05,\n",
       "                        3.5111e-05, -6.1077e-05,  1.1819e-03,  3.0165e-06,  3.9904e-05,\n",
       "                       -1.7685e-04,  1.5233e-04, -1.8935e-04, -1.0170e-05,  3.1373e-05,\n",
       "                        1.8641e-05,  1.0607e-04, -1.8872e-05,  1.7525e-05,  9.1840e-06,\n",
       "                       -2.7883e-04,  5.5047e-05,  8.5629e-07, -8.4563e-05, -7.7192e-05,\n",
       "                       -8.6863e-05,  2.4467e-04, -1.1426e-04,  5.3410e-05, -4.0810e-06,\n",
       "                        1.8840e-04, -9.7873e-05,  6.4551e-05,  5.2604e-05,  4.6871e-05,\n",
       "                       -1.8747e-04,  1.1138e-04, -5.2004e-04,  7.9687e-04,  2.3098e-05,\n",
       "                       -6.0141e-06, -4.5189e-04,  5.3120e-05,  2.2331e-05,  6.8113e-06,\n",
       "                       -1.0048e-04, -1.2437e-04, -2.5269e-05,  1.1820e-05, -3.8837e-04,\n",
       "                        1.0249e-03, -4.2515e-06,  1.1517e-04, -1.8103e-04,  8.9765e-05,\n",
       "                       -1.1329e-04, -5.8961e-05,  2.3056e-04, -2.0582e-06,  2.3374e-05,\n",
       "                        8.2708e-05,  2.4203e-05,  5.0013e-06,  1.0499e-04,  4.4615e-05,\n",
       "                       -8.4954e-05, -8.1851e-05,  2.7166e-04,  3.7879e-05,  2.7665e-04,\n",
       "                        8.5277e-06, -4.9305e-04,  3.5091e-04, -2.4529e-05, -1.3537e-05,\n",
       "                       -5.4635e-05,  6.1954e-06, -2.1486e-05,  3.1184e-05,  7.0459e-04,\n",
       "                        1.2787e-04, -8.1486e-04, -1.6642e-05,  2.6769e-06, -1.1970e-04,\n",
       "                       -2.7206e-05, -1.1197e-04, -4.5256e-05,  1.1870e-04,  1.1684e-04,\n",
       "                        1.1166e-04, -9.8386e-06,  1.7275e-04,  4.4243e-05, -2.3446e-04,\n",
       "                       -4.3747e-04, -5.9718e-05, -1.8249e-04,  1.8154e-04,  2.4793e-04,\n",
       "                       -4.4925e-05,  5.2178e-05,  2.9927e-05, -1.0440e-04,  6.5820e-05,\n",
       "                        3.2365e-05, -1.5486e-04,  2.5903e-04,  1.3089e-05,  2.2711e-05,\n",
       "                        1.1332e-04, -7.4823e-05, -2.5763e-06,  2.0265e-05,  2.4347e-04,\n",
       "                        1.1169e-05, -2.1067e-05, -2.7535e-04, -4.9302e-06,  8.8473e-05,\n",
       "                       -2.0371e-04,  1.8115e-04,  8.8873e-06,  2.6697e-05,  3.0363e-05,\n",
       "                       -3.7191e-04, -3.8762e-05,  2.4659e-05, -5.7379e-06,  2.4806e-05,\n",
       "                        5.4932e-05,  4.6466e-04, -1.5132e-05,  6.3454e-04, -5.9772e-05,\n",
       "                       -1.7389e-04, -2.4435e-04,  5.7539e-04,  6.8558e-05,  2.7170e-04,\n",
       "                       -1.7045e-05,  8.1962e-05, -5.5219e-04,  4.3695e-04, -4.6432e-04,\n",
       "                        3.0344e-06,  3.0821e-06, -1.0927e-04,  4.2988e-05, -4.4462e-05,\n",
       "                       -1.6812e-04,  3.6908e-04,  2.1823e-05,  1.0054e-04, -1.1068e-04,\n",
       "                        3.3808e-05, -3.4213e-04,  7.2496e-05, -2.8412e-04, -7.7362e-05,\n",
       "                        1.9539e-04,  1.2412e-05,  1.3663e-05,  3.8462e-05, -8.8417e-05,\n",
       "                        8.2121e-06,  2.0258e-05,  6.4279e-05, -4.5388e-05, -1.8121e-04,\n",
       "                        4.9270e-04, -2.1767e-06,  2.3506e-04,  1.2757e-05,  3.4603e-05,\n",
       "                        7.2773e-05,  8.4307e-05,  5.4448e-06, -3.5368e-05,  4.5429e-04,\n",
       "                       -1.2348e-05,  5.0399e-05, -8.4212e-06,  8.1243e-05,  4.7472e-05,\n",
       "                        2.0230e-04,  1.2966e-04,  3.5566e-04, -1.8266e-04, -1.0507e-04,\n",
       "                        2.3368e-04,  8.0319e-05,  2.2406e-05, -4.4854e-05, -1.3146e-04,\n",
       "                        2.1599e-04, -2.5795e-05, -5.4924e-05,  8.5297e-06,  4.7605e-05,\n",
       "                        2.7981e-05,  1.7149e-04, -4.5011e-04,  1.8865e-05, -3.9427e-04,\n",
       "                        1.9963e-05,  4.0730e-04,  3.6124e-05, -1.3909e-04, -1.6553e-05,\n",
       "                        9.7594e-05,  2.7899e-04, -7.4492e-05,  1.2891e-04, -2.4289e-05,\n",
       "                       -2.1234e-04, -8.0893e-06, -1.5030e-04,  4.8525e-05,  5.6055e-05,\n",
       "                        1.5325e-04,  1.8171e-05,  5.2240e-06,  5.7218e-05,  5.0946e-05,\n",
       "                        4.5665e-06, -2.3730e-05,  1.5424e-05, -2.5201e-05,  7.3394e-06,\n",
       "                        1.4147e-04, -9.0729e-05, -8.7155e-05,  3.4171e-04, -1.9193e-04,\n",
       "                        4.0015e-05,  1.9904e-04,  1.7090e-04,  2.0174e-04,  2.5005e-04,\n",
       "                        3.2625e-04, -1.3835e-06,  4.9399e-05, -7.6656e-05,  1.2094e-04,\n",
       "                       -2.4029e-04, -6.1387e-05,  5.7378e-05, -4.6039e-05, -5.0939e-05,\n",
       "                       -1.5776e-05, -1.2004e-04,  2.2914e-05,  1.9987e-05,  4.7327e-05,\n",
       "                       -1.9522e-04,  3.7098e-05,  7.2957e-05, -1.9247e-04,  2.4261e-04])),\n",
       "              ('lstm_2.weight_ih_l0',\n",
       "               tensor([[ 1.0396e-08,  9.5034e-09,  1.3405e-08,  ...,  2.7168e-08,\n",
       "                         2.8242e-10,  5.6181e-08],\n",
       "                       [-6.1773e-08,  4.0848e-09,  9.3818e-10,  ...,  6.2880e-08,\n",
       "                         1.5728e-07,  1.7080e-07],\n",
       "                       [ 4.9949e-11, -1.2961e-09, -1.3686e-08,  ...,  1.3645e-07,\n",
       "                         7.9163e-08,  1.4703e-07],\n",
       "                       ...,\n",
       "                       [ 6.5448e-08,  4.8818e-08,  7.1260e-09,  ...,  1.1887e-07,\n",
       "                        -3.0311e-07,  3.7637e-07],\n",
       "                       [ 1.3615e-08, -6.9946e-08,  1.3419e-07,  ..., -1.3044e-07,\n",
       "                         6.0063e-08,  5.3923e-08],\n",
       "                       [ 7.1924e-09,  4.3725e-10, -2.3599e-09,  ...,  7.6000e-08,\n",
       "                         1.1203e-07,  1.2898e-07]])),\n",
       "              ('lstm_2.weight_hh_l0',\n",
       "               tensor([[-8.5456e-10,  3.6888e-09,  2.1896e-09,  ...,  1.4602e-10,\n",
       "                         6.1122e-10,  6.0369e-10],\n",
       "                       [ 6.4818e-10, -8.1428e-10,  1.9989e-09,  ...,  2.2138e-09,\n",
       "                         4.9718e-10,  1.0300e-10],\n",
       "                       [ 6.3526e-10, -3.9257e-09,  2.7079e-09,  ...,  7.0326e-10,\n",
       "                         3.4194e-09, -5.0134e-10],\n",
       "                       ...,\n",
       "                       [-9.6094e-10,  2.6306e-09, -3.1089e-10,  ...,  1.1081e-08,\n",
       "                        -2.5315e-09,  3.1707e-10],\n",
       "                       [-3.1079e-10, -3.5726e-10,  8.0967e-09,  ...,  1.0967e-09,\n",
       "                        -9.1972e-10, -2.6987e-10],\n",
       "                       [ 4.4211e-10, -6.1749e-10,  1.4369e-10,  ...,  1.0453e-09,\n",
       "                        -1.6166e-10, -1.0645e-09]])),\n",
       "              ('lstm_2.bias_ih_l0',\n",
       "               tensor([-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000])),\n",
       "              ('lstm_2.bias_hh_l0',\n",
       "               tensor([-0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000])),\n",
       "              ('layer_norm_2.weight',\n",
       "               tensor([0.4685, 0.4345, 0.4169, 0.4246, 0.4216, 0.4761, 0.3959, 0.4223, 0.3782,\n",
       "                       0.5867, 0.4264, 0.2721, 0.4354, 0.5108, 0.5365, 0.4993, 0.4193, 0.4512,\n",
       "                       0.5593, 0.3537, 0.4543, 0.3579, 0.4463, 0.4575, 0.4400, 0.3897, 0.4405,\n",
       "                       0.5152, 0.4468, 0.5288, 0.4026, 0.5352, 0.4098, 0.4545, 0.4283, 0.3742,\n",
       "                       0.2883, 0.5293, 0.3665, 0.4011, 0.5249, 0.3990, 0.3465, 0.3941, 0.4005,\n",
       "                       0.4204, 0.4538, 0.5144, 0.4635, 0.6049, 0.5234, 0.4649, 0.4835, 0.3627,\n",
       "                       0.5250, 0.3678, 0.4127, 0.5668, 0.5544, 0.5129, 0.3589, 0.4163, 0.4391,\n",
       "                       0.3632, 0.5195, 0.5139, 0.3988, 0.2950, 0.3516, 0.3726, 0.5636, 0.6365,\n",
       "                       0.4345, 0.5012, 0.5199, 0.5912, 0.3542, 0.4915, 0.4560, 0.4433, 0.4341,\n",
       "                       0.5342, 0.6913, 0.3216, 0.2692, 0.4634, 0.3794, 0.2775, 0.3841, 0.3383,\n",
       "                       0.3946, 0.3787, 0.5670, 0.3752, 0.3830, 0.4447, 0.3559, 0.3919, 0.5633,\n",
       "                       0.6095, 0.4800, 0.4877, 0.3734, 0.5255, 0.3889, 0.4207, 0.4488, 0.4235,\n",
       "                       0.5316, 0.3443, 0.3567, 0.4744, 0.4779, 0.3607, 0.4728, 0.4289, 0.4654,\n",
       "                       0.3786, 0.6149, 0.4134, 0.4702, 0.3814, 0.3477, 0.3995, 0.2719, 0.4042,\n",
       "                       0.4443, 0.4240, 0.3344, 0.4906, 0.4995, 0.4288, 0.2801, 0.3656, 0.4841,\n",
       "                       0.4666, 0.5595, 0.4619, 0.3086, 0.3481, 0.6444, 0.4280, 0.3330, 0.3908,\n",
       "                       0.4841, 0.3377, 0.3796, 0.4663, 0.3868, 0.3115, 0.4606, 0.4765, 0.4824,\n",
       "                       0.4185, 0.4041, 0.4611, 0.4921, 0.6138, 0.4248, 0.3187, 0.3468, 0.4589,\n",
       "                       0.4584, 0.4250, 0.4415, 0.3629, 0.4865, 0.4580, 0.4901, 0.4213, 0.3571,\n",
       "                       0.3483, 0.3807, 0.3697, 0.5572, 0.4222, 0.4895, 0.3149, 0.4858, 0.4452,\n",
       "                       0.4288, 0.5868, 0.3647, 0.4162, 0.3889, 0.3750, 0.4265, 0.5046, 0.5047,\n",
       "                       0.5050, 0.4575, 0.4675, 0.3848, 0.4216, 0.5408, 0.4768, 0.4293, 0.3149,\n",
       "                       0.3983, 0.4122, 0.4614, 0.4840, 0.5337, 0.6604, 0.3710, 0.3634, 0.3956,\n",
       "                       0.4721, 0.4028, 0.4851, 0.5277, 0.3820, 0.4247, 0.3966, 0.4626, 0.3505,\n",
       "                       0.4767, 0.3791, 0.4066, 0.4331, 0.3256, 0.3614, 0.3078, 0.4944, 0.5824,\n",
       "                       0.3512, 0.5420, 0.3938, 0.3414, 0.4871, 0.3713, 0.4634, 0.4656, 0.3996,\n",
       "                       0.3418, 0.4620, 0.6305, 0.4722, 0.5448, 0.5709, 0.3387, 0.4540, 0.4889,\n",
       "                       0.5121, 0.4426, 0.3625, 0.4724, 0.4308, 0.3154, 0.4184, 0.4431, 0.3936,\n",
       "                       0.3306, 0.4089, 0.4340, 0.4615, 0.3533, 0.5060, 0.4272, 0.3741, 0.4345,\n",
       "                       0.4009, 0.4940, 0.3345, 0.4547, 0.2867, 0.5005, 0.4313, 0.4547, 0.3884,\n",
       "                       0.5259, 0.6547, 0.5730, 0.4861, 0.4310, 0.3988, 0.4765, 0.3751, 0.4749,\n",
       "                       0.4448, 0.3903, 0.5806, 0.3365, 0.5112, 0.3069, 0.4243, 0.4018, 0.4264,\n",
       "                       0.4272, 0.3590, 0.4291, 0.4356, 0.2620, 0.3878, 0.4580, 0.4049, 0.3835,\n",
       "                       0.4841, 0.4186, 0.4494])),\n",
       "              ('layer_norm_2.bias',\n",
       "               tensor([ 4.2733e-05,  1.0407e-04,  1.5111e-05, -3.7767e-05,  1.5303e-05,\n",
       "                        1.0986e-05, -7.0500e-06, -6.5288e-06,  7.2976e-06,  1.9782e-05,\n",
       "                        6.1088e-07,  2.1608e-05,  1.9094e-05, -2.5336e-05, -5.2358e-06,\n",
       "                        1.9048e-05, -3.3289e-06,  3.7560e-06,  1.8079e-05, -5.7263e-05,\n",
       "                        7.0013e-06,  1.4302e-08, -3.6964e-05, -9.4925e-06, -1.3845e-05,\n",
       "                        1.4102e-05, -4.1137e-06, -1.7722e-05,  2.0940e-05,  2.4220e-05,\n",
       "                       -3.1655e-06,  3.3821e-05,  1.1374e-05,  1.0759e-05,  3.9164e-05,\n",
       "                        1.8351e-05,  5.4380e-05,  5.7466e-05,  3.9748e-05,  1.7834e-05,\n",
       "                       -1.4354e-04,  2.4139e-05, -2.2304e-05, -3.8075e-05,  6.4402e-06,\n",
       "                        2.2254e-05,  2.0268e-05, -3.1592e-07,  1.3389e-05, -2.2587e-05,\n",
       "                        3.8059e-05,  1.0463e-05, -5.3802e-05,  7.8492e-06,  2.0951e-04,\n",
       "                        4.2177e-05,  2.9556e-06, -2.3510e-05,  3.9937e-05, -4.2377e-05,\n",
       "                        5.6227e-06,  5.8361e-06,  2.3550e-05,  1.2267e-04,  4.3106e-05,\n",
       "                        1.2869e-05,  3.8028e-05, -4.9173e-06,  1.6473e-05,  2.6466e-06,\n",
       "                       -2.1481e-04,  5.0184e-05,  1.8681e-05, -1.2568e-04,  3.2262e-05,\n",
       "                       -2.0320e-04,  7.9416e-05,  5.3851e-05,  1.5393e-04,  1.9902e-05,\n",
       "                        3.2163e-05,  2.5807e-05,  9.3382e-05, -6.4463e-05,  3.8914e-05,\n",
       "                        9.4164e-05, -2.3934e-04,  1.1262e-05, -6.5170e-06,  1.8411e-05,\n",
       "                        1.8947e-05,  1.5925e-04, -1.8101e-04,  1.6168e-05,  6.5843e-06,\n",
       "                        4.0094e-06,  1.1650e-05,  4.5095e-06, -2.5864e-05,  2.8125e-05,\n",
       "                       -2.5880e-05, -5.0083e-06,  2.1097e-06, -6.1887e-06,  3.3611e-05,\n",
       "                       -2.6297e-06,  1.4737e-05,  2.5298e-05,  1.0097e-05,  9.2781e-06,\n",
       "                        6.0835e-05,  2.4609e-05, -1.5213e-05,  7.8077e-06, -3.2046e-05,\n",
       "                        2.0486e-05,  1.0763e-04,  1.2631e-05,  1.1734e-04,  7.9378e-06,\n",
       "                        9.9914e-06, -1.5957e-05,  9.4934e-06,  8.4328e-05, -1.5024e-05,\n",
       "                       -4.5525e-05,  1.8309e-05,  7.8048e-06,  2.5034e-05,  2.9825e-05,\n",
       "                       -6.3932e-04,  1.3044e-05,  8.7602e-06,  1.2086e-05,  9.7643e-06,\n",
       "                       -1.8660e-05, -7.9337e-06, -3.9606e-06,  8.9826e-06,  2.0184e-06,\n",
       "                        1.8919e-04, -5.2940e-05, -3.0303e-05, -1.3543e-05,  5.7765e-05,\n",
       "                        9.4248e-06,  3.1929e-05,  4.7605e-05, -1.4059e-05,  6.1833e-05,\n",
       "                       -6.2875e-04,  1.1034e-05, -5.0135e-05,  6.1488e-05,  3.4019e-05,\n",
       "                        2.0344e-06, -2.8424e-06, -2.8404e-04,  1.8330e-05,  4.8749e-06,\n",
       "                        1.1284e-05, -4.1883e-06, -5.7643e-06,  1.7205e-05,  4.4472e-04,\n",
       "                        2.7997e-05,  1.6984e-04, -2.0613e-05,  9.2876e-06, -3.0746e-06,\n",
       "                        2.6379e-06,  1.1489e-05, -3.6047e-06,  9.9880e-05, -3.6384e-05,\n",
       "                        1.0500e-05, -2.7689e-06,  7.5443e-06,  6.1723e-06, -5.0969e-05,\n",
       "                        1.2329e-04, -1.1045e-04,  1.2310e-05,  3.3303e-04,  2.0483e-05,\n",
       "                       -1.1306e-05, -1.6126e-05,  1.0716e-05, -3.1884e-05, -5.8155e-06,\n",
       "                        2.3982e-06,  1.1551e-05,  7.5364e-06, -1.0726e-05, -5.9592e-05,\n",
       "                        2.1155e-05,  3.0777e-05,  9.9716e-06,  1.6061e-05,  1.7281e-05,\n",
       "                        1.8441e-06, -2.8172e-06,  3.4537e-05,  8.9207e-05,  6.6083e-06,\n",
       "                        2.4551e-05,  5.4193e-05, -2.1721e-06, -1.4830e-05,  7.7857e-05,\n",
       "                       -4.5178e-05, -1.0186e-04,  7.9740e-06, -9.1896e-06,  1.1051e-05,\n",
       "                        1.7503e-05,  7.0509e-05, -2.9758e-06,  4.4321e-05,  3.7675e-06,\n",
       "                        2.0602e-04, -1.9843e-04,  2.9258e-06, -3.7173e-06, -7.3667e-05,\n",
       "                        1.8196e-05, -1.4823e-04, -1.5577e-06, -3.1359e-05,  9.1656e-06,\n",
       "                        1.9957e-06, -1.4490e-04,  2.9336e-05,  3.1208e-05,  2.7979e-05,\n",
       "                        9.9341e-06,  5.3418e-05,  1.1796e-05,  1.0450e-05, -8.0154e-06,\n",
       "                        2.3239e-05,  9.9676e-06,  7.2596e-05, -5.9278e-06,  1.1896e-04,\n",
       "                        5.9963e-06,  1.8226e-05,  7.7135e-05,  1.0587e-05,  6.9299e-06,\n",
       "                        2.2271e-05,  2.3997e-06,  1.8382e-05,  1.7879e-05,  2.2149e-06,\n",
       "                       -2.2255e-05,  8.9449e-07,  4.8102e-05,  3.6884e-05,  3.6511e-05,\n",
       "                        9.6622e-06,  1.0792e-04,  1.8938e-04,  2.6688e-06,  2.6564e-05,\n",
       "                        3.1111e-05,  2.2379e-05, -1.9236e-06,  3.8420e-05,  1.1256e-05,\n",
       "                        8.9500e-06,  1.6719e-04, -1.0210e-05,  8.2943e-06,  2.6549e-05,\n",
       "                        2.5686e-06,  9.9135e-06,  1.4422e-05,  4.9665e-05,  2.4496e-03,\n",
       "                        1.7271e-05,  2.1400e-04, -2.2349e-05,  1.7142e-06,  1.0727e-05,\n",
       "                        2.6350e-05, -4.5103e-06,  1.0488e-05, -1.0647e-05,  2.0406e-04,\n",
       "                        5.4607e-06,  2.8888e-05,  9.9604e-06,  2.5470e-05, -7.7146e-06,\n",
       "                        4.8564e-04,  7.0500e-06,  1.5741e-05, -1.0322e-04,  1.5835e-06])),\n",
       "              ('lstm_3.weight_ih_l0',\n",
       "               tensor([[-1.9626e-06,  2.9992e-06, -4.6502e-07,  ..., -2.9102e-07,\n",
       "                        -1.3531e-06, -2.4409e-07],\n",
       "                       [-9.1391e-08,  6.3848e-08, -3.9726e-09,  ...,  1.4020e-08,\n",
       "                        -7.5274e-09,  7.3116e-09],\n",
       "                       [ 1.2600e-07, -1.6112e-07,  1.3202e-07,  ..., -4.6791e-08,\n",
       "                         4.4187e-08, -2.8220e-09],\n",
       "                       ...,\n",
       "                       [ 7.8927e-07,  3.4871e-07, -1.5669e-06,  ..., -5.4354e-07,\n",
       "                        -3.7812e-07, -7.7789e-08],\n",
       "                       [ 1.2419e-05,  2.2981e-05,  8.0825e-06,  ...,  8.1853e-08,\n",
       "                         9.2218e-09,  1.5952e-08],\n",
       "                       [-1.2461e-06,  3.2991e-07, -6.2939e-07,  ...,  8.8034e-09,\n",
       "                        -1.2664e-08,  5.6067e-08]])),\n",
       "              ('lstm_3.weight_hh_l0',\n",
       "               tensor([[-1.2169e-06,  1.0328e-07,  4.9490e-08,  ..., -5.0142e-07,\n",
       "                        -3.9214e-07, -1.2780e-09],\n",
       "                       [ 4.1837e-09,  1.3426e-07,  4.6026e-10,  ...,  1.2933e-09,\n",
       "                        -1.4390e-09,  4.5949e-09],\n",
       "                       [-1.7767e-08,  3.7688e-09,  4.9533e-08,  ..., -1.6556e-03,\n",
       "                        -4.2064e-08,  2.4034e-08],\n",
       "                       ...,\n",
       "                       [ 1.5801e-07,  1.3585e-08,  1.1155e-08,  ...,  8.9783e-09,\n",
       "                         2.8413e-07,  5.5155e-08],\n",
       "                       [-3.5086e-06, -1.6455e-07,  7.8569e-05,  ..., -1.5938e-07,\n",
       "                         1.2451e-06, -5.6761e-10],\n",
       "                       [ 1.0808e-07,  9.2397e-09,  1.1286e-08,  ...,  8.0504e-04,\n",
       "                        -1.1743e-07, -8.1449e-05]])),\n",
       "              ('lstm_3.bias_ih_l0',\n",
       "               tensor([ 5.3378e-05,  9.5106e-07,  9.6747e-06,  ...,  8.3687e-05,\n",
       "                       -2.4664e-04,  2.3575e-05])),\n",
       "              ('lstm_3.bias_hh_l0',\n",
       "               tensor([-9.7979e-06,  9.1400e-07,  1.0741e-05,  ...,  8.0728e-05,\n",
       "                       -2.9832e-04,  2.2911e-05])),\n",
       "              ('layer_norm_3.weight',\n",
       "               tensor([1.7775, 1.5518, 1.4368, 1.8365, 1.5670, 1.6996, 1.8303, 1.6793, 1.6255,\n",
       "                       1.8498, 1.4962, 1.4093, 1.0991, 1.7592, 1.5054, 1.1712, 1.6994, 1.7382,\n",
       "                       1.3041, 1.3662, 1.5301, 1.6065, 1.7188, 1.2348, 1.7748, 1.4380, 1.8885,\n",
       "                       1.9348, 1.4229, 1.9288, 1.7255, 1.8907, 1.2126, 1.7115, 1.4554, 1.8092,\n",
       "                       1.5783, 1.6764, 1.3864, 1.6343, 1.6233, 1.7998, 1.8484, 1.6548, 1.4589,\n",
       "                       1.8583, 1.3503, 1.8204, 1.8589, 1.2290, 1.4715, 1.8553, 1.5052, 1.7764,\n",
       "                       1.7517, 1.5196, 1.5336, 1.7375, 1.0702, 1.3448, 1.6833, 1.8723, 1.8787,\n",
       "                       1.6532, 1.7410, 1.3889, 1.7439, 1.8395, 1.3901, 1.7790, 1.8573, 1.4811,\n",
       "                       1.1237, 1.6555, 1.9302, 1.4309, 1.7586, 1.4016, 1.7206, 1.7062, 1.6509,\n",
       "                       1.8347, 1.7681, 1.7217, 1.3311, 1.2369, 1.8420, 1.9403, 1.9410, 1.8125,\n",
       "                       0.9878, 1.8181, 1.8147, 1.6132, 1.9648, 1.5951, 1.4628, 1.6301, 1.7936,\n",
       "                       1.8336, 1.8521, 1.5182, 1.4588, 1.9729, 2.1196, 0.8540, 1.9354, 1.6386,\n",
       "                       1.5400, 1.4348, 1.4382, 1.1582, 1.5462, 1.7275, 1.8999, 1.6908, 1.2174,\n",
       "                       1.1382, 1.7517, 1.4223, 1.7585, 1.7366, 1.6094, 1.8055, 1.7408, 1.6787,\n",
       "                       1.6675, 1.4633, 1.9830, 1.2572, 1.8004, 1.6821, 1.2217, 1.8277, 1.6590,\n",
       "                       1.6018, 1.8304, 1.5264, 1.2350, 1.6191, 1.7899, 0.8575, 1.7170, 1.6773,\n",
       "                       1.3975, 1.8930, 1.3399, 1.2045, 1.8680, 1.5545, 1.6483, 1.6333, 1.4871,\n",
       "                       1.1873, 1.8984, 1.6823, 1.7312, 1.6789, 1.4419, 1.6647, 1.1889, 1.7437,\n",
       "                       1.5142, 1.6226, 1.7076, 1.8149, 1.7941, 1.8139, 1.2981, 1.5563, 1.8368,\n",
       "                       1.7028, 1.6447, 1.6244, 1.2597, 1.6672, 1.9226, 1.9355, 1.7838, 1.9443,\n",
       "                       1.5345, 1.5876, 1.3201, 1.4813, 1.9452, 2.0040, 1.3869, 1.9545, 1.7098,\n",
       "                       1.4503, 1.8571, 1.8601, 1.3731, 1.7244, 1.7186, 1.7555, 1.6647, 1.6374,\n",
       "                       1.7546, 1.8428, 1.5970, 1.1489, 1.4137, 1.5481, 0.9695, 1.7106, 2.0530,\n",
       "                       1.5260, 1.7622, 1.7748, 1.6389, 1.9302, 1.7020, 1.6085, 1.4915, 1.6481,\n",
       "                       1.5098, 1.3635, 0.7965, 1.0786, 1.9829, 1.8447, 1.6997, 1.3267, 1.6423,\n",
       "                       1.8392, 1.9665, 1.6699, 1.7106, 1.3402, 1.6408, 1.8738, 1.5778, 1.9966,\n",
       "                       1.7088, 1.2620, 1.6209, 1.4519, 1.5626, 1.7034, 1.8861, 1.3503, 1.6280,\n",
       "                       1.0432, 1.9474, 1.9816, 1.3821, 1.0940, 1.5090, 1.6836, 1.4789, 1.3791,\n",
       "                       1.2973, 1.5805, 1.6112, 1.7362, 1.7469, 1.6229, 1.8249, 1.9250, 1.8351,\n",
       "                       1.8065, 1.7301, 1.7310, 1.4210, 2.0205, 1.4385, 1.5541, 1.8630, 1.4397,\n",
       "                       1.4426, 1.9741, 1.8237, 1.7604, 1.7432, 1.4058, 1.3807, 1.6514, 1.8324,\n",
       "                       1.7599, 1.7257, 1.7764, 1.7395, 1.3597, 1.7588, 1.5901, 2.0202, 1.5774,\n",
       "                       1.8495, 1.6512, 1.6542, 1.3545, 1.4369, 1.7169, 1.8199, 1.9314, 1.2788,\n",
       "                       1.7792, 1.7185, 1.6745])),\n",
       "              ('layer_norm_3.bias',\n",
       "               tensor([ 3.8572e-04, -1.9398e-02,  7.3072e-06,  1.4613e-01, -7.9653e-05,\n",
       "                       -9.6750e-03,  2.2598e-04,  2.1335e-04, -1.6627e-03, -2.1354e-03,\n",
       "                       -9.8626e-02,  9.9745e-06,  2.9819e-02, -1.9580e-03,  4.3012e-02,\n",
       "                       -4.5426e-04,  2.6116e-02,  1.5172e-04,  6.0764e-02,  2.9133e-02,\n",
       "                       -1.5076e-04, -2.7322e-03,  1.2844e-01,  1.6562e-04,  9.6976e-03,\n",
       "                        5.1823e-03,  5.7509e-02, -7.6094e-02, -2.9950e-03, -3.2777e-02,\n",
       "                       -4.8029e-03, -9.7117e-02, -1.3864e-03, -6.2976e-02,  1.2449e-03,\n",
       "                        3.6362e-04,  1.6833e-03,  6.5798e-03,  3.4912e-06,  1.7481e-04,\n",
       "                        5.8253e-02,  5.8930e-04, -1.3885e-01, -2.9580e-02,  2.2689e-04,\n",
       "                       -1.3251e-01, -4.9570e-05, -1.8523e-01,  2.8651e-03,  5.1175e-03,\n",
       "                       -1.1141e-01, -3.1882e-04, -7.3313e-05, -1.2804e-01,  1.1988e-01,\n",
       "                       -3.4257e-02,  4.3563e-02, -1.3421e-01, -1.9464e-03, -1.6037e-02,\n",
       "                       -2.9624e-03, -2.3870e-02,  6.5331e-02, -1.1115e-01,  9.4592e-02,\n",
       "                       -3.4028e-04, -2.6626e-05, -1.7567e-02, -1.0510e-04, -7.7699e-04,\n",
       "                       -7.2275e-05, -8.7609e-03, -4.8892e-04,  6.0267e-05,  1.2805e-01,\n",
       "                       -3.1920e-04,  1.0188e-02, -1.3037e-02,  6.1027e-05,  3.1665e-02,\n",
       "                        2.0002e-02,  2.0450e-05,  3.4326e-04,  2.3578e-01, -2.7176e-02,\n",
       "                        2.5129e-04, -1.2244e-01,  1.0160e-01,  3.2852e-02,  7.2247e-05,\n",
       "                       -2.1576e-04, -5.0129e-02, -1.8475e-04,  7.3779e-02,  6.0680e-02,\n",
       "                       -1.7355e-04,  8.4929e-02,  2.7718e-03,  3.9025e-04,  7.0729e-04,\n",
       "                       -2.5687e-02, -6.3998e-05, -1.4342e-02,  9.4509e-02, -1.1769e-03,\n",
       "                       -3.3398e-02, -2.0880e-04,  9.4271e-02,  2.7885e-02, -1.0097e-04,\n",
       "                        1.9348e-02,  2.9750e-03, -1.9620e-04,  1.2021e-03, -2.3292e-02,\n",
       "                        8.3477e-04, -1.0594e-01,  1.5756e-02,  4.3465e-04,  3.7891e-04,\n",
       "                       -4.0849e-04, -1.6873e-04,  1.4590e-01,  1.0589e-02, -1.1252e-04,\n",
       "                       -4.3480e-04, -5.4066e-05,  5.6255e-04, -4.7930e-02,  1.5580e-02,\n",
       "                       -7.6952e-02, -2.7464e-02,  6.9921e-02,  1.9211e-03, -1.1780e-04,\n",
       "                        1.2034e-01,  2.3482e-02, -4.0061e-04, -2.1133e-02,  8.8436e-05,\n",
       "                       -4.5364e-03, -8.5699e-03, -6.6477e-02,  7.5409e-05,  7.4138e-02,\n",
       "                       -3.1595e-02, -4.2746e-02, -2.0867e-05,  2.7426e-02, -3.5768e-02,\n",
       "                       -6.0456e-02, -4.1230e-02, -2.3596e-02, -7.2183e-03,  5.1540e-02,\n",
       "                       -1.2914e-01, -7.4879e-02,  1.3948e-04, -3.2312e-02,  8.3892e-03,\n",
       "                       -1.8913e-05,  1.7398e-02, -2.9395e-02,  8.6194e-05,  6.2162e-05,\n",
       "                       -8.9570e-03,  3.3870e-02, -2.7628e-03,  2.3853e-01, -4.4378e-02,\n",
       "                       -1.7836e-02, -7.7717e-05, -4.1073e-02,  2.4701e-02, -2.3202e-02,\n",
       "                        1.2288e-02,  1.0370e-01,  1.2670e-01, -3.7440e-02,  1.0865e-01,\n",
       "                       -3.3391e-05,  1.0167e-05,  4.5683e-04,  7.0063e-02,  4.5094e-03,\n",
       "                        3.3399e-02,  3.7772e-05, -7.0085e-02, -2.6587e-02,  1.4713e-01,\n",
       "                       -5.9240e-02,  7.1547e-02, -1.4301e-02,  9.3864e-05, -2.4375e-03,\n",
       "                       -1.8360e-01, -8.0041e-05, -7.1257e-05, -1.2010e-01, -1.1297e-01,\n",
       "                        5.2116e-03, -1.4391e-02, -6.9890e-05,  2.4193e-04, -2.5382e-05,\n",
       "                       -1.2263e-04, -2.1262e-01, -8.8175e-06,  4.7847e-02,  2.5255e-02,\n",
       "                       -2.5836e-03,  3.9114e-02, -7.5644e-02,  1.2717e-01, -2.5584e-04,\n",
       "                        2.9915e-02, -1.4583e-04, -3.0404e-05,  2.4630e-02,  1.1187e-01,\n",
       "                        2.0019e-01, -3.1735e-03,  2.1273e-02, -8.1700e-07, -9.1671e-02,\n",
       "                        1.6634e-04,  7.4770e-02,  7.1589e-02,  2.4339e-04, -3.6364e-02,\n",
       "                       -5.3599e-02,  3.3561e-04, -6.6427e-02,  2.3196e-01, -5.6936e-04,\n",
       "                       -7.9950e-02,  4.1641e-02, -1.0873e-04,  5.1491e-03,  2.2386e-02,\n",
       "                        6.4398e-02, -1.3458e-01, -7.6723e-02,  8.2839e-02,  7.3985e-02,\n",
       "                       -7.7398e-02, -9.6580e-02,  1.1857e-01, -4.7858e-03,  3.3102e-02,\n",
       "                       -6.8352e-03, -9.6451e-05,  3.5009e-05,  5.7347e-04,  6.8808e-05,\n",
       "                        8.7442e-02, -1.9351e-01,  8.7743e-03,  8.6631e-02, -2.0512e-03,\n",
       "                        2.6330e-02, -8.0247e-04,  8.6436e-04, -1.0580e-05,  3.0682e-04,\n",
       "                       -3.5345e-02,  2.6257e-05,  2.0268e-03,  6.9743e-02, -7.1575e-02,\n",
       "                        1.9555e-02,  6.3706e-02,  9.0540e-03,  6.1535e-03, -2.0321e-01,\n",
       "                       -2.3859e-02,  1.3948e-02, -1.7826e-02, -2.0928e-05,  8.5515e-02,\n",
       "                       -1.3409e-02,  1.3859e-04, -6.3459e-05,  6.6827e-02,  2.1350e-04,\n",
       "                        1.2310e-04,  6.9698e-03, -5.1164e-03,  9.8318e-02, -8.6837e-02,\n",
       "                        2.2429e-02,  2.0711e-03, -4.4090e-02, -4.2477e-02,  1.0382e-04,\n",
       "                        4.2922e-02, -1.5971e-03, -1.8862e-05, -2.8269e-02,  1.0012e-01])),\n",
       "              ('to_notes.weight',\n",
       "               tensor([[ 4.7129e-03, -8.7459e-04, -1.5003e-04,  ..., -9.7352e-05,\n",
       "                        -4.3137e-05, -2.0712e-05],\n",
       "                       [-1.5035e-03,  2.1788e-04, -5.9550e-05,  ...,  4.4285e-05,\n",
       "                        -4.1280e-07, -1.1767e-04],\n",
       "                       [-1.7397e-03,  2.0546e-04, -6.7565e-05,  ..., -3.5732e-04,\n",
       "                        -5.0403e-05,  1.1877e-05],\n",
       "                       ...,\n",
       "                       [-1.7743e-03,  1.9945e-04, -6.5300e-05,  ...,  2.5018e-05,\n",
       "                        -9.3898e-06,  9.2209e-06],\n",
       "                       [-1.2391e-03,  2.2880e-04, -6.3791e-05,  ...,  4.3492e-05,\n",
       "                        -4.3503e-05,  8.4811e-06],\n",
       "                       [-1.7282e-03,  1.9966e-04, -6.7373e-05,  ..., -1.4230e-04,\n",
       "                        -1.2851e-05,  6.8656e-06]])),\n",
       "              ('to_notes.bias',\n",
       "               tensor([ 5.8762e-03, -2.3680e-03, -2.3653e-03, -2.3703e-03, -2.3678e-03,\n",
       "                       -2.3564e-03, -2.3690e-03, -2.3620e-03, -2.3679e-03, -2.3607e-03,\n",
       "                       -2.3646e-03, -2.3600e-03, -2.3650e-03, -2.3640e-03, -2.3659e-03,\n",
       "                       -2.3651e-03, -2.3656e-03, -2.3514e-03, -2.3911e-03, -2.2568e-03,\n",
       "                       -2.3032e-03, -2.2803e-03, -2.1286e-03, -2.0245e-03, -1.9111e-03,\n",
       "                       -1.8682e-03, -9.3720e-04, -1.7860e-03, -1.6598e-03, -1.6750e-03,\n",
       "                       -6.2970e-04, -1.4161e-03,  2.8135e-04, -5.1808e-04, -8.7707e-04,\n",
       "                       -3.6926e-04, -5.2576e-04, -3.7564e-04, -4.0214e-04, -5.6867e-04,\n",
       "                       -5.2955e-04, -2.9656e-04, -3.7163e-04, -3.5764e-04, -6.9632e-06,\n",
       "                       -1.0668e-04, -6.3543e-05, -2.4835e-04,  1.3921e-05, -2.2032e-04,\n",
       "                        2.5641e-05, -4.5342e-04,  7.9763e-05, -6.7335e-05, -1.9146e-05,\n",
       "                        1.6815e-04, -8.4221e-05, -6.3475e-05, -1.2616e-04,  2.3537e-04,\n",
       "                        2.4086e-04, -2.0207e-04, -8.5834e-05,  6.4337e-05,  5.5997e-05,\n",
       "                        1.7491e-04, -1.8042e-04, -6.5552e-05,  6.5768e-05,  4.2292e-05,\n",
       "                       -1.5936e-05, -1.1865e-04, -1.4310e-05, -2.1645e-04,  3.7811e-06,\n",
       "                        1.7433e-04,  5.8382e-07,  1.7800e-05, -1.3849e-04, -1.8074e-04,\n",
       "                       -5.7696e-06, -9.3613e-05,  2.7506e-04,  6.3328e-06, -1.1324e-04,\n",
       "                        2.2141e-04,  4.9206e-04, -1.8961e-04,  2.4284e-04, -3.5103e-04,\n",
       "                       -4.5087e-04, -3.0587e-04, -6.6102e-04, -1.0006e-03, -6.2949e-04,\n",
       "                       -1.7256e-03, -1.2055e-03, -1.4029e-03, -9.2786e-04, -7.0286e-04,\n",
       "                       -9.2118e-04, -2.2781e-03, -2.1291e-03, -2.3245e-03, -2.3531e-03,\n",
       "                       -2.3311e-03, -2.3540e-03, -2.3471e-03, -2.3579e-03, -2.3560e-03,\n",
       "                       -2.3630e-03, -2.3597e-03, -2.3664e-03, -2.3645e-03, -2.3544e-03,\n",
       "                       -2.3606e-03, -2.3517e-03, -2.3730e-03, -2.3708e-03, -2.3674e-03,\n",
       "                       -2.3578e-03, -2.3576e-03, -2.3681e-03, -2.3658e-03, -2.3679e-03,\n",
       "                       -2.3581e-03, -2.3703e-03, -2.3595e-03, -2.3648e-03]))]),\n",
       " 'opt_dict': {'state': {1747370804064: {'step': 1114,\n",
       "    'exp_avg': tensor([0.0722, 0.0854, 0.0849, 0.0848, 0.0851, 0.0854, 0.0854, 0.0854, 0.0854,\n",
       "            0.0855, 0.0851, 0.0855, 0.0852, 0.0853, 0.0850, 0.0848, 0.0853, 0.0850,\n",
       "            0.0852, 0.1719, 0.1675, 0.2133, 0.2214, 0.2217, 0.2291, 0.2412, 0.2107,\n",
       "            0.2042, 0.2050, 0.1865, 0.2038, 0.1708, 0.1812, 0.1724, 0.1742, 0.1592,\n",
       "            0.1710, 0.1625, 0.1487, 0.1435, 0.1493, 0.1369, 0.1599, 0.1327, 0.1408,\n",
       "            0.1433, 0.1384, 0.1433, 0.1389, 0.1301, 0.1323, 0.1438, 0.1512, 0.1396,\n",
       "            0.1502, 0.1479, 0.1448, 0.1580, 0.1411, 0.1269, 0.1376, 0.1437, 0.1460,\n",
       "            0.1302, 0.1380, 0.1385, 0.1319, 0.1279, 0.1256, 0.1315, 0.1345, 0.1248,\n",
       "            0.1238, 0.1284, 0.1192, 0.1417, 0.1260, 0.1244, 0.1266, 0.1348, 0.1401,\n",
       "            0.1374, 0.1330, 0.1530, 0.1468, 0.1518, 0.1498, 0.1636, 0.1597, 0.1684,\n",
       "            0.1602, 0.1721, 0.1658, 0.1746, 0.1821, 0.1895, 0.1903, 0.2198, 0.2069,\n",
       "            0.2044, 0.2152, 0.1829, 0.1493, 0.1464, 0.1174, 0.1069, 0.0932, 0.0920,\n",
       "            0.0942, 0.0854, 0.0855, 0.0852, 0.0855, 0.0849, 0.0853, 0.0855, 0.0851,\n",
       "            0.0852, 0.0861, 0.0854, 0.0850, 0.0854, 0.0854, 0.0853, 0.0853, 0.0858,\n",
       "            0.0851, 0.0854, 0.0852], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([0.0369, 0.0356, 0.0354, 0.0354, 0.0355, 0.0356, 0.0356, 0.0356, 0.0356,\n",
       "            0.0356, 0.0355, 0.0356, 0.0355, 0.0356, 0.0354, 0.0354, 0.0356, 0.0354,\n",
       "            0.0355, 0.0765, 0.0742, 0.0996, 0.1044, 0.1046, 0.1090, 0.1164, 0.0981,\n",
       "            0.0943, 0.0946, 0.0842, 0.0941, 0.0760, 0.0815, 0.0768, 0.0777, 0.0698,\n",
       "            0.0762, 0.0716, 0.0648, 0.0620, 0.0651, 0.0593, 0.0704, 0.0571, 0.0608,\n",
       "            0.0623, 0.0595, 0.0620, 0.0598, 0.0559, 0.0570, 0.0622, 0.0662, 0.0602,\n",
       "            0.0659, 0.0645, 0.0631, 0.0698, 0.0611, 0.0546, 0.0599, 0.0623, 0.0639,\n",
       "            0.0563, 0.0600, 0.0602, 0.0570, 0.0553, 0.0541, 0.0570, 0.0584, 0.0538,\n",
       "            0.0529, 0.0551, 0.0510, 0.0615, 0.0543, 0.0536, 0.0547, 0.0586, 0.0610,\n",
       "            0.0599, 0.0578, 0.0669, 0.0643, 0.0665, 0.0655, 0.0725, 0.0704, 0.0750,\n",
       "            0.0706, 0.0769, 0.0733, 0.0779, 0.0820, 0.0859, 0.0864, 0.1034, 0.0958,\n",
       "            0.0944, 0.1007, 0.0824, 0.0649, 0.0634, 0.0496, 0.0448, 0.0389, 0.0384,\n",
       "            0.0393, 0.0356, 0.0356, 0.0355, 0.0357, 0.0354, 0.0355, 0.0356, 0.0355,\n",
       "            0.0355, 0.0359, 0.0356, 0.0354, 0.0356, 0.0356, 0.0356, 0.0356, 0.0358,\n",
       "            0.0355, 0.0356, 0.0355], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([0.0479, 0.0467, 0.0465, 0.0464, 0.0465, 0.0467, 0.0467, 0.0467, 0.0467,\n",
       "            0.0467, 0.0466, 0.0467, 0.0466, 0.0466, 0.0465, 0.0464, 0.0466, 0.0465,\n",
       "            0.0466, 0.0899, 0.0875, 0.1133, 0.1181, 0.1183, 0.1227, 0.1301, 0.1117,\n",
       "            0.1079, 0.1083, 0.0978, 0.1077, 0.0894, 0.0950, 0.0902, 0.0911, 0.0830,\n",
       "            0.0895, 0.0849, 0.0778, 0.0750, 0.0782, 0.0721, 0.0837, 0.0698, 0.0737,\n",
       "            0.0752, 0.0723, 0.0750, 0.0726, 0.0686, 0.0698, 0.0752, 0.0793, 0.0731,\n",
       "            0.0791, 0.0777, 0.0762, 0.0831, 0.0742, 0.0673, 0.0728, 0.0753, 0.0770,\n",
       "            0.0691, 0.0730, 0.0732, 0.0698, 0.0681, 0.0668, 0.0699, 0.0713, 0.0665,\n",
       "            0.0655, 0.0678, 0.0635, 0.0746, 0.0670, 0.0662, 0.0674, 0.0715, 0.0740,\n",
       "            0.0728, 0.0707, 0.0801, 0.0774, 0.0797, 0.0787, 0.0858, 0.0837, 0.0884,\n",
       "            0.0839, 0.0903, 0.0867, 0.0914, 0.0955, 0.0995, 0.1000, 0.1171, 0.1095,\n",
       "            0.1081, 0.1144, 0.0959, 0.0779, 0.0764, 0.0618, 0.0568, 0.0503, 0.0497,\n",
       "            0.0508, 0.0467, 0.0467, 0.0466, 0.0467, 0.0465, 0.0466, 0.0467, 0.0465,\n",
       "            0.0466, 0.0470, 0.0467, 0.0465, 0.0467, 0.0467, 0.0466, 0.0466, 0.0469,\n",
       "            0.0465, 0.0467, 0.0466], device='cuda:0')},\n",
       "   1747370807160: {'step': 1114,\n",
       "    'exp_avg': tensor([-1.2514e-03,  5.8137e-05,  6.6743e-05,  5.4771e-05,  9.2457e-05,\n",
       "             6.0856e-05,  4.2649e-05,  5.6549e-05,  5.1507e-05,  6.7845e-05,\n",
       "             5.8464e-05,  2.5581e-05,  5.5026e-05,  7.6952e-05,  5.2024e-05,\n",
       "             8.0042e-05,  3.2667e-05,  5.1957e-05,  1.1402e-04,  1.0499e-04,\n",
       "             3.2929e-04,  3.5271e-04,  4.5320e-04,  5.8116e-05,  1.0681e-04,\n",
       "            -4.9746e-05,  1.3469e-04,  4.5189e-04,  5.3233e-05,  5.7054e-05,\n",
       "            -2.4276e-04, -2.1964e-04,  1.7137e-04,  5.5487e-05, -8.3620e-05,\n",
       "            -1.9966e-04,  2.6275e-04, -1.4280e-04, -1.7292e-04,  3.1365e-04,\n",
       "            -1.8765e-05,  9.7937e-05, -2.2041e-04,  3.2632e-04,  8.9615e-04,\n",
       "             3.7213e-04, -2.3706e-04,  3.9122e-05,  8.5007e-04,  2.5582e-04,\n",
       "             5.3127e-04,  6.2137e-04,  2.4366e-04, -3.3610e-04, -2.5692e-04,\n",
       "             9.7592e-04,  3.0650e-04,  1.6020e-04, -6.6239e-04, -4.4252e-04,\n",
       "             2.2092e-05, -1.0538e-03, -4.8259e-05, -4.2606e-04, -1.2511e-03,\n",
       "            -1.0770e-03, -9.1190e-04,  1.1981e-05, -1.3237e-03, -2.3427e-04,\n",
       "            -5.8701e-04, -5.2702e-04,  4.4486e-04, -8.2092e-04,  1.2428e-04,\n",
       "            -2.0096e-04, -3.2894e-04,  6.9107e-04, -2.6595e-04,  4.1163e-04,\n",
       "            -1.1957e-04,  1.9373e-04, -2.0688e-05, -2.7806e-04,  1.8936e-04,\n",
       "            -1.7923e-05,  1.0211e-03,  1.9466e-05, -1.0270e-05,  7.8135e-04,\n",
       "            -1.2385e-04, -2.0818e-04,  1.5733e-04,  4.4460e-04,  4.1652e-04,\n",
       "             2.8459e-04,  3.0814e-04,  9.8877e-05,  3.0421e-04,  4.0246e-06,\n",
       "             3.0393e-04,  3.1579e-04,  1.8084e-04,  2.9792e-04,  5.1570e-06,\n",
       "             1.6177e-04,  5.8120e-05,  5.6437e-05,  1.0604e-04,  5.9140e-05,\n",
       "             5.2495e-05,  5.9934e-05,  4.3783e-05,  7.9996e-05,  5.2832e-05,\n",
       "             3.1841e-05,  4.3822e-05,  4.7983e-05,  4.7370e-05,  5.5623e-05,\n",
       "             4.2027e-05,  6.0906e-05,  4.2051e-05,  6.8130e-05,  4.7499e-05,\n",
       "             6.2613e-05,  4.2630e-05,  5.2674e-05,  4.1864e-05], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([0.0014, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([0.0029, 0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "            0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0001, 0.0000,\n",
       "            0.0001, 0.0000, 0.0000, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "            0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "            0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000], device='cuda:0')},\n",
       "   1747370638072: {'step': 1114,\n",
       "    'exp_avg': tensor([[ 2.6166e-08, -2.6094e-10, -3.5917e-11,  ...,  3.6593e-10,\n",
       "             -3.3287e-10,  2.6823e-09],\n",
       "            [-6.5016e-08,  8.5523e-10,  1.5805e-09,  ...,  2.3329e-10,\n",
       "             -9.3955e-11,  1.3990e-09],\n",
       "            [-1.5440e-07,  2.2076e-09,  7.2780e-10,  ...,  3.3318e-09,\n",
       "              3.3989e-09,  2.5635e-09],\n",
       "            ...,\n",
       "            [-2.2922e-07,  3.7439e-09,  5.5232e-09,  ...,  4.1347e-09,\n",
       "              1.3456e-09,  5.9085e-10],\n",
       "            [ 4.3396e-07, -7.1584e-09, -9.2655e-09,  ..., -6.1900e-09,\n",
       "             -6.1079e-09, -5.9891e-09],\n",
       "            [ 1.1335e-07,  4.1541e-09, -1.3909e-09,  ...,  9.1491e-09,\n",
       "             -5.5778e-10,  2.9640e-09]], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([[1.4662e-06, 1.0304e-05, 1.6231e-05,  ..., 2.8893e-05, 4.6439e-07,\n",
       "             8.0120e-05],\n",
       "            [4.0314e-08, 3.2038e-06, 8.8573e-06,  ..., 1.8632e-06, 1.3738e-06,\n",
       "             6.3219e-06],\n",
       "            [1.9633e-06, 2.1502e-05, 5.0667e-08,  ..., 1.7149e-06, 1.1424e-06,\n",
       "             9.1892e-06],\n",
       "            ...,\n",
       "            [7.4477e-06, 2.8187e-06, 2.6186e-05,  ..., 3.9455e-06, 1.5073e-06,\n",
       "             1.2151e-06],\n",
       "            [2.1645e-05, 6.5163e-07, 4.5186e-07,  ..., 1.4339e-08, 1.5376e-06,\n",
       "             1.9167e-06],\n",
       "            [4.0360e-05, 7.4350e-07, 1.6739e-05,  ..., 2.8907e-08, 7.1789e-06,\n",
       "             2.1509e-06]], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([[3.9805e-06, 2.8731e-05, 4.4582e-05,  ..., 7.7656e-05, 1.3547e-06,\n",
       "             2.0552e-04],\n",
       "            [7.6677e-08, 9.2122e-06, 2.4810e-05,  ..., 5.4116e-06, 4.0062e-06,\n",
       "             1.7879e-05],\n",
       "            [5.6778e-06, 5.8459e-05, 1.4678e-07,  ..., 4.9870e-06, 3.3371e-06,\n",
       "             2.5711e-05],\n",
       "            ...,\n",
       "            [2.0874e-05, 8.1269e-06, 7.0661e-05,  ..., 1.1292e-05, 4.3906e-06,\n",
       "             3.5478e-06],\n",
       "            [5.8795e-05, 1.9063e-06, 1.3176e-06,  ..., 4.1496e-08, 4.4780e-06,\n",
       "             5.5645e-06],\n",
       "            [1.0699e-04, 2.1756e-06, 4.5925e-05,  ..., 8.3833e-08, 2.0232e-05,\n",
       "             6.2324e-06]], device='cuda:0')},\n",
       "   1747370804136: {'step': 1114,\n",
       "    'exp_avg': tensor([[-4.1244e-12, -3.6954e-06, -2.5124e-11,  ..., -5.9418e-12,\n",
       "              3.6152e-11, -7.2853e-12],\n",
       "            [ 2.6281e-12, -3.0061e-11, -6.9239e-12,  ..., -1.5821e-12,\n",
       "             -2.5375e-07,  1.4510e-11],\n",
       "            [-4.5809e-12, -1.7335e-11, -1.0359e-11,  ..., -1.3196e-11,\n",
       "              5.6923e-12, -3.8164e-11],\n",
       "            ...,\n",
       "            [-5.7396e-10, -1.1410e-12, -1.3152e-11,  ..., -1.0739e-10,\n",
       "              3.7154e-11,  5.1763e-11],\n",
       "            [ 1.7781e-10, -3.9966e-11, -1.8350e-11,  ...,  2.2828e-10,\n",
       "              1.4645e-09,  1.1960e-10],\n",
       "            [-1.1232e-12, -4.6875e-11, -3.0033e-11,  ...,  6.4772e-08,\n",
       "             -1.6312e-11, -2.0276e-10]], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([[4.6606e-05, 5.7038e-04, 1.2775e-04,  ..., 3.4116e-05, 6.7334e-07,\n",
       "             1.3845e-07],\n",
       "            [4.7900e-05, 1.3717e-04, 1.1643e-04,  ..., 3.5289e-05, 3.7413e-04,\n",
       "             1.9754e-07],\n",
       "            [3.2442e-05, 5.5987e-05, 4.4095e-05,  ..., 2.2256e-05, 1.0585e-05,\n",
       "             1.1091e-04],\n",
       "            ...,\n",
       "            [1.8861e-04, 1.8820e-10, 1.8061e-06,  ..., 4.7531e-06, 2.2060e-07,\n",
       "             3.8877e-08],\n",
       "            [5.8927e-05, 1.2630e-04, 2.5405e-05,  ..., 2.3460e-05, 1.4847e-06,\n",
       "             1.8465e-06],\n",
       "            [4.3613e-05, 8.2018e-08, 1.3793e-05,  ..., 3.1289e-04, 1.0256e-05,\n",
       "             7.4462e-05]], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([[1.2273e-04, 1.2841e-03, 3.1940e-04,  ..., 9.1071e-05, 1.9698e-06,\n",
       "             4.0270e-07],\n",
       "            [1.2598e-04, 3.4154e-04, 2.9266e-04,  ..., 9.4069e-05, 8.7100e-04,\n",
       "             5.7370e-07],\n",
       "            [8.6782e-05, 1.4618e-04, 1.1641e-04,  ..., 6.0430e-05, 2.9487e-05,\n",
       "             2.7955e-04],\n",
       "            ...,\n",
       "            [4.6051e-04, 4.9459e-10, 5.2482e-06,  ..., 1.3543e-05, 6.4026e-07,\n",
       "             1.1264e-07],\n",
       "            [1.5349e-04, 3.1599e-04, 6.8632e-05,  ..., 6.3574e-05, 4.3254e-06,\n",
       "             5.3641e-06],\n",
       "            [1.1519e-04, 2.3852e-07, 3.8098e-05,  ..., 7.3813e-04, 2.8599e-05,\n",
       "             1.9173e-04]], device='cuda:0')},\n",
       "   1747370804640: {'step': 1114,\n",
       "    'exp_avg': tensor([-4.9959e-09, -1.6083e-07, -2.1875e-09,  ...,  1.7825e-08,\n",
       "             8.6631e-07, -5.6730e-07], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([2.3025e-05, 2.8577e-05, 4.8443e-09,  ..., 3.9979e-07, 5.1327e-06,\n",
       "            3.5760e-08], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([6.2434e-05, 7.6858e-05, 7.8100e-09,  ..., 1.1566e-06, 1.4589e-05,\n",
       "            9.7988e-08], device='cuda:0')},\n",
       "   1747370806296: {'step': 1114,\n",
       "    'exp_avg': tensor([ 3.3608e-08, -1.1744e-07, -1.2738e-07,  ..., -2.5639e-07,\n",
       "             8.1232e-07,  1.3853e-07], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([7.7897e-06, 1.0322e-05, 1.0614e-07,  ..., 8.3814e-06, 4.4986e-06,\n",
       "            1.5167e-05], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([2.1893e-05, 2.8775e-05, 3.0517e-07,  ..., 2.3521e-05, 1.2828e-05,\n",
       "            4.1737e-05], device='cuda:0')},\n",
       "   1747370804352: {'step': 1114,\n",
       "    'exp_avg': tensor([0.0754, 0.0648, 0.0726, 0.0993, 0.0615, 0.0710, 0.0811, 0.0727, 0.0892,\n",
       "            0.0561, 0.0635, 0.0706, 0.0817, 0.0921, 0.0880, 0.0816, 0.0756, 0.0757,\n",
       "            0.0633, 0.0712, 0.0851, 0.0727, 0.0773, 0.0681, 0.0806, 0.0767, 0.0697,\n",
       "            0.0770, 0.0663, 0.0695, 0.0794, 0.0708, 0.0680, 0.1003, 0.0678, 0.0887,\n",
       "            0.0961, 0.0718, 0.0762, 0.1069, 0.0808, 0.0835, 0.0641, 0.0867, 0.0777,\n",
       "            0.0835, 0.0614, 0.0666, 0.0656, 0.0778, 0.0678, 0.0814, 0.0942, 0.0704,\n",
       "            0.0763, 0.0715, 0.0672, 0.0820, 0.0703, 0.0681, 0.0813, 0.0688, 0.0868,\n",
       "            0.0813, 0.0662, 0.0946, 0.0958, 0.0659, 0.1065, 0.0737, 0.0792, 0.0651,\n",
       "            0.1009, 0.0684, 0.0841, 0.0867, 0.0785, 0.1029, 0.0684, 0.0762, 0.0811,\n",
       "            0.0908, 0.0766, 0.0855, 0.0831, 0.0632, 0.0684, 0.0774, 0.0832, 0.0708,\n",
       "            0.0701, 0.0655, 0.0815, 0.0810, 0.0984, 0.0895, 0.0871, 0.0936, 0.1000,\n",
       "            0.0854, 0.0782, 0.0683, 0.0727, 0.0770, 0.0826, 0.0855, 0.0773, 0.0911,\n",
       "            0.0945, 0.0990, 0.0661, 0.0782, 0.0696, 0.0654, 0.0876, 0.0883, 0.0680,\n",
       "            0.0627, 0.0824, 0.1253, 0.0877, 0.0886, 0.0694, 0.0877, 0.0851, 0.1025,\n",
       "            0.0992, 0.0853, 0.0752, 0.0841, 0.0818, 0.0766, 0.0738, 0.0776, 0.0938,\n",
       "            0.0667, 0.0753, 0.0844, 0.0843, 0.0783, 0.0661, 0.0797, 0.0734, 0.0709,\n",
       "            0.0658, 0.0780, 0.0774, 0.0973, 0.0525, 0.1049, 0.0849, 0.0881, 0.0832,\n",
       "            0.0875, 0.0913, 0.0833, 0.0908, 0.0679, 0.0743, 0.0697, 0.0957, 0.0800,\n",
       "            0.0768, 0.0681, 0.0780, 0.0926, 0.0781, 0.0533, 0.0831, 0.0842, 0.0820,\n",
       "            0.0647, 0.0775, 0.0499, 0.0842, 0.0660, 0.1094, 0.0956, 0.0621, 0.1136,\n",
       "            0.0753, 0.0759, 0.0611, 0.0781, 0.0997, 0.0859, 0.0794, 0.0931, 0.0753,\n",
       "            0.0845, 0.0898, 0.1062, 0.0768, 0.0829, 0.0847, 0.0632, 0.0763, 0.0919,\n",
       "            0.0921, 0.1029, 0.0843, 0.0768, 0.0747, 0.0865, 0.0883, 0.0749, 0.0728,\n",
       "            0.0613, 0.0863, 0.0936, 0.0966, 0.0748, 0.0906, 0.0775, 0.0797, 0.0625,\n",
       "            0.0811, 0.0723, 0.0736, 0.1049, 0.0642, 0.0797, 0.0809, 0.0692, 0.0833,\n",
       "            0.0754, 0.0654, 0.0637, 0.0939, 0.0952, 0.1029, 0.0771, 0.0634, 0.0820,\n",
       "            0.0974, 0.0728, 0.0788, 0.0999, 0.0837, 0.0745, 0.0661, 0.0686, 0.0967,\n",
       "            0.0575, 0.0765, 0.0741, 0.0830, 0.0535, 0.0863, 0.0861, 0.0766, 0.0611,\n",
       "            0.0710, 0.0914, 0.0907, 0.0803, 0.0705, 0.0837, 0.0883, 0.0639, 0.1082,\n",
       "            0.0553, 0.0771, 0.0810, 0.0723, 0.0676, 0.0841, 0.0789, 0.0697, 0.0832,\n",
       "            0.0846, 0.0825, 0.0724, 0.0822, 0.0803, 0.0633, 0.0899, 0.0811, 0.0954,\n",
       "            0.0718, 0.0850, 0.0747, 0.0902, 0.0889, 0.0683, 0.0921, 0.0749, 0.0866,\n",
       "            0.0936, 0.0874, 0.0783, 0.0887, 0.0611, 0.0728, 0.0787, 0.0883, 0.0828,\n",
       "            0.0765, 0.0606, 0.0602], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([0.0315, 0.0273, 0.0304, 0.0415, 0.0260, 0.0297, 0.0348, 0.0304, 0.0372,\n",
       "            0.0258, 0.0268, 0.0296, 0.0341, 0.0384, 0.0367, 0.0340, 0.0316, 0.0327,\n",
       "            0.0267, 0.0298, 0.0355, 0.0304, 0.0322, 0.0285, 0.0336, 0.0320, 0.0292,\n",
       "            0.0321, 0.0279, 0.0290, 0.0331, 0.0297, 0.0285, 0.0421, 0.0285, 0.0367,\n",
       "            0.0405, 0.0301, 0.0318, 0.0451, 0.0337, 0.0345, 0.0270, 0.0361, 0.0325,\n",
       "            0.0348, 0.0260, 0.0279, 0.0276, 0.0325, 0.0285, 0.0339, 0.0396, 0.0295,\n",
       "            0.0318, 0.0299, 0.0282, 0.0342, 0.0295, 0.0286, 0.0339, 0.0289, 0.0362,\n",
       "            0.0339, 0.0278, 0.0395, 0.0400, 0.0277, 0.0454, 0.0308, 0.0330, 0.0274,\n",
       "            0.0425, 0.0287, 0.0350, 0.0361, 0.0327, 0.0432, 0.0287, 0.0319, 0.0338,\n",
       "            0.0381, 0.0320, 0.0357, 0.0347, 0.0266, 0.0287, 0.0323, 0.0347, 0.0297,\n",
       "            0.0326, 0.0276, 0.0340, 0.0338, 0.0411, 0.0373, 0.0363, 0.0391, 0.0419,\n",
       "            0.0370, 0.0327, 0.0287, 0.0304, 0.0322, 0.0345, 0.0356, 0.0323, 0.0384,\n",
       "            0.0395, 0.0414, 0.0278, 0.0326, 0.0292, 0.0275, 0.0365, 0.0368, 0.0286,\n",
       "            0.0264, 0.0344, 0.0557, 0.0366, 0.0368, 0.0291, 0.0366, 0.0355, 0.0433,\n",
       "            0.0415, 0.0356, 0.0314, 0.0351, 0.0341, 0.0320, 0.0309, 0.0324, 0.0392,\n",
       "            0.0280, 0.0314, 0.0352, 0.0352, 0.0327, 0.0278, 0.0332, 0.0307, 0.0297,\n",
       "            0.0277, 0.0325, 0.0323, 0.0417, 0.0226, 0.0443, 0.0354, 0.0376, 0.0347,\n",
       "            0.0365, 0.0387, 0.0347, 0.0379, 0.0285, 0.0311, 0.0292, 0.0400, 0.0334,\n",
       "            0.0321, 0.0286, 0.0326, 0.0386, 0.0326, 0.0228, 0.0346, 0.0351, 0.0342,\n",
       "            0.0272, 0.0324, 0.0216, 0.0351, 0.0278, 0.0466, 0.0403, 0.0263, 0.0486,\n",
       "            0.0315, 0.0317, 0.0309, 0.0326, 0.0418, 0.0358, 0.0331, 0.0388, 0.0315,\n",
       "            0.0352, 0.0375, 0.0453, 0.0321, 0.0349, 0.0356, 0.0266, 0.0319, 0.0384,\n",
       "            0.0384, 0.0433, 0.0351, 0.0321, 0.0312, 0.0361, 0.0373, 0.0313, 0.0305,\n",
       "            0.0259, 0.0360, 0.0391, 0.0404, 0.0313, 0.0378, 0.0324, 0.0333, 0.0264,\n",
       "            0.0338, 0.0303, 0.0308, 0.0442, 0.0271, 0.0332, 0.0337, 0.0290, 0.0347,\n",
       "            0.0315, 0.0275, 0.0268, 0.0392, 0.0397, 0.0435, 0.0322, 0.0268, 0.0342,\n",
       "            0.0411, 0.0305, 0.0329, 0.0430, 0.0349, 0.0312, 0.0278, 0.0288, 0.0442,\n",
       "            0.0245, 0.0319, 0.0310, 0.0346, 0.0229, 0.0360, 0.0359, 0.0320, 0.0258,\n",
       "            0.0297, 0.0381, 0.0378, 0.0335, 0.0295, 0.0349, 0.0368, 0.0269, 0.0465,\n",
       "            0.0236, 0.0322, 0.0338, 0.0302, 0.0284, 0.0351, 0.0329, 0.0292, 0.0347,\n",
       "            0.0352, 0.0344, 0.0303, 0.0343, 0.0334, 0.0267, 0.0387, 0.0339, 0.0402,\n",
       "            0.0300, 0.0355, 0.0312, 0.0376, 0.0371, 0.0287, 0.0386, 0.0313, 0.0361,\n",
       "            0.0391, 0.0364, 0.0326, 0.0370, 0.0259, 0.0305, 0.0329, 0.0383, 0.0345,\n",
       "            0.0320, 0.0257, 0.0255], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([0.0421, 0.0373, 0.0408, 0.0532, 0.0358, 0.0401, 0.0459, 0.0408, 0.0484,\n",
       "            0.0358, 0.0367, 0.0399, 0.0450, 0.0498, 0.0479, 0.0449, 0.0422, 0.0436,\n",
       "            0.0366, 0.0402, 0.0466, 0.0409, 0.0429, 0.0387, 0.0445, 0.0427, 0.0395,\n",
       "            0.0428, 0.0380, 0.0393, 0.0439, 0.0400, 0.0387, 0.0538, 0.0386, 0.0479,\n",
       "            0.0521, 0.0405, 0.0424, 0.0570, 0.0446, 0.0454, 0.0370, 0.0473, 0.0432,\n",
       "            0.0458, 0.0358, 0.0380, 0.0377, 0.0432, 0.0386, 0.0448, 0.0511, 0.0399,\n",
       "            0.0424, 0.0403, 0.0384, 0.0451, 0.0398, 0.0388, 0.0448, 0.0391, 0.0473,\n",
       "            0.0448, 0.0379, 0.0510, 0.0515, 0.0378, 0.0575, 0.0414, 0.0438, 0.0375,\n",
       "            0.0542, 0.0390, 0.0461, 0.0473, 0.0435, 0.0551, 0.0389, 0.0425, 0.0447,\n",
       "            0.0494, 0.0426, 0.0468, 0.0456, 0.0366, 0.0389, 0.0431, 0.0457, 0.0400,\n",
       "            0.0427, 0.0376, 0.0449, 0.0447, 0.0528, 0.0486, 0.0475, 0.0505, 0.0537,\n",
       "            0.0484, 0.0434, 0.0389, 0.0409, 0.0429, 0.0455, 0.0467, 0.0430, 0.0498,\n",
       "            0.0509, 0.0531, 0.0379, 0.0434, 0.0395, 0.0376, 0.0477, 0.0480, 0.0388,\n",
       "            0.0363, 0.0453, 0.0685, 0.0477, 0.0480, 0.0394, 0.0478, 0.0466, 0.0551,\n",
       "            0.0532, 0.0466, 0.0420, 0.0461, 0.0450, 0.0426, 0.0414, 0.0431, 0.0506,\n",
       "            0.0381, 0.0420, 0.0463, 0.0462, 0.0434, 0.0379, 0.0441, 0.0412, 0.0401,\n",
       "            0.0378, 0.0433, 0.0430, 0.0534, 0.0318, 0.0562, 0.0464, 0.0490, 0.0457,\n",
       "            0.0477, 0.0502, 0.0457, 0.0492, 0.0387, 0.0416, 0.0395, 0.0515, 0.0442,\n",
       "            0.0428, 0.0388, 0.0433, 0.0500, 0.0433, 0.0321, 0.0456, 0.0461, 0.0451,\n",
       "            0.0372, 0.0431, 0.0306, 0.0462, 0.0380, 0.0587, 0.0519, 0.0361, 0.0609,\n",
       "            0.0421, 0.0423, 0.0415, 0.0433, 0.0535, 0.0469, 0.0439, 0.0502, 0.0421,\n",
       "            0.0463, 0.0487, 0.0573, 0.0427, 0.0460, 0.0467, 0.0366, 0.0425, 0.0497,\n",
       "            0.0498, 0.0551, 0.0462, 0.0427, 0.0418, 0.0472, 0.0486, 0.0418, 0.0409,\n",
       "            0.0357, 0.0471, 0.0505, 0.0519, 0.0418, 0.0491, 0.0431, 0.0441, 0.0363,\n",
       "            0.0447, 0.0407, 0.0413, 0.0561, 0.0371, 0.0440, 0.0446, 0.0393, 0.0457,\n",
       "            0.0421, 0.0376, 0.0368, 0.0506, 0.0512, 0.0554, 0.0429, 0.0367, 0.0451,\n",
       "            0.0528, 0.0410, 0.0436, 0.0549, 0.0459, 0.0419, 0.0379, 0.0390, 0.0562,\n",
       "            0.0341, 0.0426, 0.0415, 0.0456, 0.0322, 0.0471, 0.0470, 0.0426, 0.0356,\n",
       "            0.0401, 0.0494, 0.0491, 0.0443, 0.0399, 0.0459, 0.0480, 0.0369, 0.0587,\n",
       "            0.0330, 0.0429, 0.0447, 0.0407, 0.0385, 0.0461, 0.0437, 0.0395, 0.0457,\n",
       "            0.0463, 0.0453, 0.0407, 0.0452, 0.0443, 0.0366, 0.0502, 0.0448, 0.0518,\n",
       "            0.0405, 0.0465, 0.0418, 0.0489, 0.0483, 0.0389, 0.0500, 0.0419, 0.0473,\n",
       "            0.0505, 0.0476, 0.0434, 0.0482, 0.0357, 0.0409, 0.0436, 0.0498, 0.0455,\n",
       "            0.0426, 0.0354, 0.0352], device='cuda:0')},\n",
       "   1747370803992: {'step': 1114,\n",
       "    'exp_avg': tensor([-6.9170e-06, -2.7128e-05, -3.8233e-06, -1.6818e-05, -6.5107e-05,\n",
       "             2.4817e-05,  1.1161e-04,  8.9920e-07, -2.4679e-05,  1.5401e-03,\n",
       "             5.1719e-05,  2.7165e-05,  5.8302e-05, -4.7596e-06, -6.0358e-05,\n",
       "            -2.0822e-05, -1.1607e-04, -1.3097e-03,  1.8170e-04, -1.6309e-05,\n",
       "            -1.2377e-04, -2.8127e-05, -3.7049e-04, -1.3240e-04,  1.5408e-04,\n",
       "            -3.3750e-05,  6.4534e-06, -6.6280e-05,  3.2829e-05, -2.6866e-04,\n",
       "             3.0853e-05,  3.9107e-06,  1.6017e-04, -2.0570e-04, -1.2637e-05,\n",
       "            -8.8355e-04,  7.8453e-05, -1.4863e-04,  2.6503e-05, -3.3740e-04,\n",
       "             3.9176e-05, -7.7799e-04, -2.8664e-05, -7.8477e-05, -5.0225e-05,\n",
       "            -5.6283e-06, -3.4249e-05,  5.9406e-04, -8.6967e-06, -1.3380e-05,\n",
       "            -3.7517e-05,  4.6314e-05,  8.6251e-06,  1.9593e-05,  2.5291e-04,\n",
       "             1.9759e-05,  2.0045e-04,  2.3473e-05,  3.9972e-06,  8.3301e-05,\n",
       "            -3.7005e-04, -6.9234e-06,  2.4918e-05, -1.6959e-05, -3.7691e-05,\n",
       "             2.6286e-05,  1.7213e-04,  3.0825e-05, -8.9582e-05, -5.5718e-05,\n",
       "             9.8454e-05, -1.9914e-05, -5.8197e-06, -1.3139e-05,  4.7348e-06,\n",
       "            -7.2988e-06, -1.5171e-06,  2.1128e-04,  2.7874e-04, -2.1504e-06,\n",
       "            -7.9075e-06,  2.7785e-04, -5.0528e-05, -4.9347e-05, -1.8702e-06,\n",
       "            -2.3088e-04, -1.0920e-04, -1.2715e-04, -1.6647e-04, -9.8147e-05,\n",
       "            -3.1619e-04,  1.2384e-05,  3.6534e-06, -1.7581e-04, -2.5327e-05,\n",
       "            -7.8083e-05,  1.5335e-05, -1.3484e-05,  9.6881e-05,  6.3483e-04,\n",
       "             2.4096e-04, -1.3106e-05, -7.1740e-06,  1.8137e-06,  2.4075e-04,\n",
       "            -2.6442e-05, -6.2048e-05,  3.9951e-04, -2.0048e-05,  1.4746e-04,\n",
       "            -6.4651e-06, -1.9068e-04, -1.5016e-04, -3.9754e-05, -1.3592e-05,\n",
       "             9.7984e-05, -1.5235e-05, -4.1520e-04, -4.0160e-06,  8.1051e-04,\n",
       "             5.1882e-05, -4.8208e-04, -4.2596e-06,  9.8857e-06, -3.6967e-05,\n",
       "             1.3541e-04, -1.6960e-04, -4.0109e-05, -2.3723e-05,  4.2325e-05,\n",
       "             2.2213e-04, -1.1720e-05, -7.0286e-06,  8.9115e-05, -3.2569e-05,\n",
       "            -1.1004e-04, -2.3238e-05, -7.4524e-05,  9.1385e-05,  3.8259e-05,\n",
       "            -1.7160e-05, -6.9917e-05,  4.5105e-05, -2.5257e-04,  7.1432e-06,\n",
       "            -3.1346e-05,  1.1121e-04,  6.6761e-05, -7.1617e-06, -8.2706e-05,\n",
       "             6.6201e-06,  8.5415e-04, -1.7908e-05, -3.4022e-05, -2.2441e-04,\n",
       "            -1.2552e-05,  2.9788e-05, -1.1850e-04, -1.7230e-05,  6.7842e-05,\n",
       "             1.0288e-04,  8.1169e-05, -7.6105e-05,  6.0872e-06,  2.8321e-05,\n",
       "            -2.1639e-04, -3.2219e-05,  1.1176e-05, -8.1836e-05,  2.2944e-06,\n",
       "            -9.6931e-06,  1.2196e-04, -5.1886e-06,  5.5055e-05, -6.0872e-05,\n",
       "            -1.1507e-04,  1.0259e-04,  8.6851e-05,  4.3575e-05, -4.7321e-04,\n",
       "            -4.2033e-06,  7.6035e-05,  4.7889e-03,  1.4412e-04, -4.5401e-05,\n",
       "            -3.4783e-06, -1.9425e-05, -2.3133e-04, -7.4801e-07, -3.4423e-05,\n",
       "            -4.5603e-05, -1.4201e-04, -3.3711e-05,  1.1183e-04, -5.3600e-04,\n",
       "            -1.2518e-05, -1.9190e-04,  1.6456e-05, -6.5198e-05, -1.3430e-05,\n",
       "             1.2388e-04,  1.2930e-05,  1.8187e-05, -8.5283e-06, -3.1361e-04,\n",
       "             3.7079e-05, -3.6843e-07,  4.2293e-05, -2.4592e-05, -9.9234e-05,\n",
       "            -1.8620e-04, -4.6575e-06,  1.3730e-05, -2.3237e-05, -1.4017e-04,\n",
       "             2.8860e-05,  1.2749e-05, -1.7142e-05, -1.5525e-06, -2.1893e-04,\n",
       "             3.9230e-05,  1.9935e-04,  1.2605e-04, -2.6803e-05, -4.4438e-05,\n",
       "             1.5608e-05,  3.6093e-05, -1.9336e-04, -2.4152e-05, -6.6084e-05,\n",
       "            -5.2696e-05,  9.1293e-05, -1.4155e-05, -1.4460e-04, -7.3767e-05,\n",
       "             3.1966e-05, -5.7045e-05, -1.5616e-03, -3.7547e-06, -7.9265e-05,\n",
       "             8.0075e-06,  3.0700e-05, -4.6958e-04, -2.7217e-05, -1.5273e-04,\n",
       "            -1.6600e-05,  5.3714e-05,  1.4485e-05, -1.2457e-04, -2.1265e-05,\n",
       "             1.4434e-04,  1.6195e-04,  2.4483e-04,  1.5169e-04, -5.2927e-05,\n",
       "            -2.8656e-04, -8.0117e-07, -8.3841e-05, -3.3922e-04,  2.3788e-06,\n",
       "             4.5969e-04, -1.4763e-06,  3.4344e-05, -1.1238e-04, -5.9237e-05,\n",
       "             4.0870e-06, -8.1852e-05,  3.9450e-05, -4.2339e-06, -2.7651e-05,\n",
       "            -1.8626e-05, -9.0287e-05, -5.3024e-05,  3.0789e-04, -2.3074e-04,\n",
       "             1.4606e-05, -1.3435e-04, -1.5155e-04, -8.6107e-05,  2.3704e-04,\n",
       "             3.5624e-05,  3.5951e-05, -4.4630e-05, -3.5412e-05,  3.1501e-05,\n",
       "             2.2766e-04, -4.5400e-05, -4.2182e-05, -5.9462e-05, -1.1619e-04,\n",
       "             7.6011e-07, -7.4470e-05,  3.4781e-06,  5.9268e-06,  1.9300e-05,\n",
       "             8.6325e-04, -2.9288e-05, -2.7994e-06,  3.0326e-04,  4.8076e-05],\n",
       "           device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([7.1030e-06, 3.8223e-07, 7.3538e-06, 4.6843e-06, 8.4790e-07, 5.0489e-07,\n",
       "            1.1744e-05, 6.2970e-07, 8.3715e-07, 5.7039e-05, 3.4018e-07, 5.9870e-07,\n",
       "            1.1008e-06, 5.6696e-06, 2.9278e-06, 9.7672e-06, 1.6447e-06, 2.6600e-05,\n",
       "            6.2910e-07, 9.5153e-07, 2.9404e-06, 7.8801e-07, 6.3460e-07, 6.2735e-07,\n",
       "            1.9421e-06, 5.6817e-07, 8.6668e-07, 5.3639e-07, 2.8371e-06, 5.6998e-07,\n",
       "            4.5214e-06, 2.4019e-05, 5.2173e-07, 2.0247e-06, 1.3079e-06, 8.2793e-07,\n",
       "            3.0269e-06, 9.0933e-06, 1.5364e-05, 2.3821e-06, 1.3099e-06, 8.9776e-07,\n",
       "            4.9035e-07, 3.5266e-06, 1.1843e-06, 6.1051e-07, 9.8135e-07, 6.1738e-07,\n",
       "            1.2438e-05, 5.6168e-07, 4.7065e-07, 5.4137e-07, 2.4372e-06, 7.6665e-07,\n",
       "            1.5833e-05, 6.6125e-06, 4.2744e-07, 3.1857e-06, 4.6769e-07, 1.2837e-05,\n",
       "            1.2268e-06, 8.0515e-07, 9.2585e-06, 1.2466e-06, 3.6656e-07, 8.2189e-07,\n",
       "            7.3039e-07, 4.3021e-07, 9.5380e-06, 5.4446e-06, 4.5441e-07, 5.4767e-07,\n",
       "            1.2944e-05, 5.3180e-07, 6.0445e-07, 1.5022e-06, 6.8901e-07, 2.0703e-06,\n",
       "            4.3833e-07, 1.2149e-06, 4.5721e-06, 2.8819e-06, 1.0206e-06, 1.4019e-06,\n",
       "            8.0306e-06, 3.0453e-06, 1.6573e-06, 7.5669e-07, 4.7908e-06, 5.9148e-07,\n",
       "            3.0799e-04, 3.6299e-06, 6.1474e-07, 6.9723e-07, 1.1900e-06, 1.0714e-06,\n",
       "            6.6665e-06, 1.0510e-06, 1.6436e-05, 1.1266e-05, 7.0910e-07, 1.3385e-06,\n",
       "            5.5225e-06, 5.3496e-07, 6.1310e-06, 7.6529e-07, 6.7942e-07, 4.6404e-06,\n",
       "            7.6880e-06, 1.2484e-06, 4.9750e-07, 7.0788e-07, 6.1254e-07, 1.8443e-06,\n",
       "            1.0109e-06, 5.4420e-07, 6.2951e-06, 7.7011e-07, 2.2779e-06, 1.3200e-04,\n",
       "            1.5687e-06, 9.1840e-07, 5.0803e-07, 1.4455e-06, 7.2524e-07, 3.3163e-06,\n",
       "            1.5378e-06, 6.1137e-06, 4.7087e-07, 5.2446e-07, 6.1980e-07, 3.7523e-06,\n",
       "            5.5609e-07, 5.7331e-07, 7.3460e-07, 4.3464e-07, 1.0219e-06, 7.4293e-07,\n",
       "            7.6569e-07, 5.8898e-07, 4.0273e-07, 2.8343e-06, 4.4331e-06, 1.1030e-06,\n",
       "            5.0407e-07, 9.1284e-07, 1.8371e-06, 1.1772e-05, 2.4160e-06, 9.9152e-05,\n",
       "            6.2326e-07, 1.0319e-05, 5.4476e-06, 1.0387e-06, 5.4536e-06, 9.7469e-06,\n",
       "            3.2783e-06, 3.8631e-07, 5.5844e-05, 1.4169e-06, 2.3782e-06, 7.9074e-07,\n",
       "            2.7387e-05, 1.3180e-06, 7.4897e-06, 6.5290e-07, 8.2398e-07, 1.0045e-06,\n",
       "            1.0495e-06, 8.8274e-07, 6.3781e-07, 3.9411e-07, 5.1060e-07, 4.7430e-07,\n",
       "            6.2261e-07, 1.0990e-05, 5.9787e-06, 3.1372e-06, 4.4998e-07, 9.4087e-06,\n",
       "            6.2760e-06, 5.8843e-07, 2.7406e-04, 7.5535e-07, 1.8819e-06, 1.5921e-06,\n",
       "            1.1220e-06, 1.5714e-06, 4.9413e-07, 2.5803e-06, 9.3477e-07, 7.0514e-06,\n",
       "            6.9228e-07, 8.3005e-06, 4.5620e-06, 6.0578e-07, 7.8288e-07, 4.8536e-06,\n",
       "            8.7258e-07, 4.7056e-05, 6.4358e-07, 4.3510e-06, 1.4467e-06, 1.3563e-06,\n",
       "            5.3564e-06, 2.1301e-05, 4.2282e-07, 9.5913e-07, 7.9758e-07, 1.1650e-06,\n",
       "            9.9979e-07, 6.1838e-06, 6.8259e-07, 6.1970e-06, 5.4650e-07, 1.1315e-06,\n",
       "            2.1955e-06, 3.6971e-06, 8.1568e-07, 3.0499e-06, 5.6435e-07, 4.0840e-06,\n",
       "            4.4800e-07, 9.8915e-07, 6.4235e-07, 5.3325e-07, 6.5356e-07, 5.9904e-07,\n",
       "            1.0561e-06, 4.1820e-06, 2.8929e-06, 5.1263e-07, 2.9440e-06, 1.7280e-06,\n",
       "            2.9287e-06, 5.5723e-07, 9.9394e-07, 4.8678e-05, 8.1421e-06, 4.3135e-06,\n",
       "            7.6658e-07, 2.7117e-06, 1.7131e-04, 1.9098e-06, 6.3565e-07, 3.7218e-06,\n",
       "            8.6533e-07, 8.4445e-07, 1.3477e-06, 1.8865e-05, 2.1292e-06, 1.2206e-06,\n",
       "            5.3086e-07, 4.4226e-06, 1.8961e-06, 1.8564e-06, 1.3691e-05, 7.4940e-07,\n",
       "            1.0954e-06, 9.1926e-07, 2.7910e-05, 7.7924e-07, 5.2278e-06, 1.8258e-06,\n",
       "            1.2520e-06, 3.3584e-06, 1.3360e-05, 5.8163e-06, 6.4282e-06, 3.9164e-06,\n",
       "            1.0088e-06, 6.0753e-07, 5.3540e-07, 6.2868e-07, 2.3447e-06, 4.5631e-07,\n",
       "            5.1454e-05, 2.8956e-06, 3.4436e-06, 4.4245e-07, 7.0191e-07, 7.3811e-06,\n",
       "            6.6568e-07, 1.1020e-06, 1.1184e-06, 2.8379e-06, 1.2629e-06, 7.3419e-07,\n",
       "            5.3889e-06, 2.1122e-06, 9.1143e-06, 2.1294e-06, 4.3657e-07, 1.6794e-05,\n",
       "            1.3417e-06, 2.0822e-05, 2.2457e-06, 2.1533e-06, 4.0280e-07, 5.7247e-07],\n",
       "           device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([1.9092e-05, 5.1080e-07, 1.9817e-05, 1.1612e-05, 8.4790e-07, 5.4239e-07,\n",
       "            1.1755e-05, 7.9183e-07, 8.6394e-07, 5.7039e-05, 4.2694e-07, 5.9870e-07,\n",
       "            1.2638e-06, 1.4210e-05, 5.6238e-06, 2.5775e-05, 1.7762e-06, 2.6600e-05,\n",
       "            7.4232e-07, 1.2025e-06, 2.9434e-06, 1.0244e-06, 6.3460e-07, 6.2735e-07,\n",
       "            4.4353e-06, 6.7827e-07, 1.1546e-06, 5.6805e-07, 7.2309e-06, 5.6998e-07,\n",
       "            1.1532e-05, 6.3765e-05, 5.4602e-07, 2.0280e-06, 2.6688e-06, 8.2793e-07,\n",
       "            3.0269e-06, 2.3853e-05, 4.1212e-05, 2.3821e-06, 2.1983e-06, 8.9776e-07,\n",
       "            5.5238e-07, 8.4349e-06, 1.9697e-06, 7.0036e-07, 1.1652e-06, 6.1738e-07,\n",
       "            3.3351e-05, 6.5940e-07, 5.2992e-07, 5.9833e-07, 2.4372e-06, 1.0812e-06,\n",
       "            4.2193e-05, 1.7675e-05, 4.5728e-07, 7.3971e-06, 5.6940e-07, 3.4640e-05,\n",
       "            1.9147e-06, 1.2136e-06, 2.4452e-05, 1.8022e-06, 4.6701e-07, 8.4961e-07,\n",
       "            7.3111e-07, 5.8017e-07, 9.5380e-06, 1.4312e-05, 5.1823e-07, 5.9754e-07,\n",
       "            3.0067e-05, 6.3637e-07, 6.8532e-07, 1.7651e-06, 7.9863e-07, 2.0703e-06,\n",
       "            4.3833e-07, 2.3008e-06, 1.1539e-05, 2.8964e-06, 1.7261e-06, 2.4049e-06,\n",
       "            2.1148e-05, 7.9931e-06, 3.4350e-06, 7.6965e-07, 8.7489e-06, 6.6925e-07,\n",
       "            3.0799e-04, 9.4779e-06, 6.7141e-07, 7.1536e-07, 1.2134e-06, 1.4464e-06,\n",
       "            1.4501e-05, 1.1015e-06, 4.1795e-05, 1.1266e-05, 7.1381e-07, 2.5207e-06,\n",
       "            1.4670e-05, 5.8789e-07, 1.1549e-05, 9.2817e-07, 8.2867e-07, 4.6404e-06,\n",
       "            1.9342e-05, 1.2521e-06, 6.0152e-07, 7.0949e-07, 6.1452e-07, 3.8918e-06,\n",
       "            1.3089e-06, 5.8896e-07, 1.6494e-05, 7.7011e-07, 5.1184e-06, 1.3234e-04,\n",
       "            2.7526e-06, 9.1840e-07, 6.1579e-07, 2.7653e-06, 8.3081e-07, 4.0056e-06,\n",
       "            1.5378e-06, 1.5739e-05, 5.3257e-07, 5.6209e-07, 6.4569e-07, 9.7873e-06,\n",
       "            5.8883e-07, 6.4936e-07, 7.6518e-07, 5.0884e-07, 1.3737e-06, 8.6238e-07,\n",
       "            7.9442e-07, 6.9280e-07, 4.9747e-07, 6.7346e-06, 1.1786e-05, 1.9652e-06,\n",
       "            5.8158e-07, 1.0867e-06, 1.9396e-06, 1.1772e-05, 5.8230e-06, 2.4541e-04,\n",
       "            6.9301e-07, 1.0321e-05, 1.3982e-05, 1.1266e-06, 5.4536e-06, 2.6198e-05,\n",
       "            7.9206e-06, 4.6342e-07, 1.4444e-04, 3.3132e-06, 2.8626e-06, 1.2205e-06,\n",
       "            7.2584e-05, 2.9737e-06, 1.9927e-05, 6.7393e-07, 1.3677e-06, 1.8052e-06,\n",
       "            1.5178e-06, 9.8784e-07, 7.5933e-07, 4.8553e-07, 6.0377e-07, 4.8373e-07,\n",
       "            6.6591e-07, 1.1078e-05, 5.9787e-06, 3.1372e-06, 5.0142e-07, 9.4139e-06,\n",
       "            1.6034e-05, 6.0191e-07, 2.7406e-04, 8.1795e-07, 1.8869e-06, 3.1843e-06,\n",
       "            2.1759e-06, 2.7981e-06, 5.6776e-07, 6.1447e-06, 1.0361e-06, 7.0514e-06,\n",
       "            8.5461e-07, 1.1757e-05, 4.5627e-06, 8.4549e-07, 1.0427e-06, 1.1511e-05,\n",
       "            9.1785e-07, 1.2024e-04, 6.4900e-07, 1.1442e-05, 2.6114e-06, 2.4821e-06,\n",
       "            5.3564e-06, 5.6977e-05, 5.2214e-07, 1.6990e-06, 8.7232e-07, 1.1843e-06,\n",
       "            1.0062e-06, 1.6433e-05, 7.1488e-07, 1.5869e-05, 6.3431e-07, 1.9373e-06,\n",
       "            5.0642e-06, 9.6601e-06, 1.2711e-06, 3.9411e-06, 6.5930e-07, 1.0065e-05,\n",
       "            5.2287e-07, 1.8439e-06, 7.8334e-07, 6.1051e-07, 1.0262e-06, 6.0087e-07,\n",
       "            1.1326e-06, 9.7083e-06, 2.8929e-06, 5.9612e-07, 7.7363e-06, 3.3335e-06,\n",
       "            2.9320e-06, 6.3844e-07, 1.0991e-06, 4.8688e-05, 2.1062e-05, 4.3311e-06,\n",
       "            1.1928e-06, 6.7124e-06, 1.7135e-04, 4.5940e-06, 6.5500e-07, 9.2667e-06,\n",
       "            9.0449e-07, 1.6825e-06, 1.8525e-06, 5.0178e-05, 4.9887e-06, 2.4424e-06,\n",
       "            5.4276e-07, 1.0793e-05, 3.4891e-06, 3.6400e-06, 3.6161e-05, 8.1668e-07,\n",
       "            1.4083e-06, 1.4753e-06, 2.9339e-05, 1.1895e-06, 1.3456e-05, 2.9937e-06,\n",
       "            2.0220e-06, 8.5192e-06, 3.5406e-05, 1.5587e-05, 1.7162e-05, 9.7046e-06,\n",
       "            1.3296e-06, 7.1545e-07, 6.4415e-07, 6.7930e-07, 5.2652e-06, 5.1054e-07,\n",
       "            5.1502e-05, 2.9470e-06, 3.4436e-06, 4.8889e-07, 7.3110e-07, 1.9606e-05,\n",
       "            6.7115e-07, 1.1693e-06, 2.1932e-06, 2.8484e-06, 2.1047e-06, 7.7138e-07,\n",
       "            1.2074e-05, 4.5406e-06, 2.2200e-05, 3.9314e-06, 5.4735e-07, 4.5224e-05,\n",
       "            2.3957e-06, 2.0822e-05, 4.6327e-06, 4.8105e-06, 4.0286e-07, 6.0276e-07],\n",
       "           device='cuda:0')},\n",
       "   1747370804280: {'step': 1114,\n",
       "    'exp_avg': tensor([[ 2.2142e-09, -2.7743e-09,  1.1001e-09,  ...,  1.1779e-08,\n",
       "              4.5783e-09,  2.9929e-09],\n",
       "            [-9.2274e-09, -9.5990e-10, -3.8037e-09,  ..., -4.8497e-09,\n",
       "              5.1907e-09,  2.2603e-08],\n",
       "            [-4.7014e-09, -6.8331e-09, -1.5224e-09,  ...,  3.5194e-08,\n",
       "              3.8930e-08,  3.5335e-08],\n",
       "            ...,\n",
       "            [ 7.6201e-09, -3.2778e-10, -1.1258e-08,  ...,  7.5856e-09,\n",
       "              2.2014e-08,  1.7395e-08],\n",
       "            [-2.2636e-09,  9.7314e-09,  1.9187e-08,  ...,  6.2490e-09,\n",
       "              3.3715e-08,  3.2396e-08],\n",
       "            [ 7.8342e-09, -1.5867e-09, -2.9377e-09,  ..., -2.1200e-09,\n",
       "              1.4690e-08, -3.3894e-09]], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([[2.0480e-06, 3.6087e-06, 5.1419e-06,  ..., 4.2271e-08, 5.5713e-07,\n",
       "             9.9490e-06],\n",
       "            [4.9916e-09, 4.3847e-07, 2.6421e-05,  ..., 4.5577e-08, 3.6563e-05,\n",
       "             3.2293e-06],\n",
       "            [9.1932e-05, 5.1099e-07, 1.7367e-05,  ..., 2.6898e-06, 4.6384e-07,\n",
       "             5.1904e-06],\n",
       "            ...,\n",
       "            [2.0992e-07, 2.2529e-08, 7.5347e-06,  ..., 3.4956e-06, 2.3331e-08,\n",
       "             1.4581e-09],\n",
       "            [1.8678e-05, 1.9016e-08, 3.6699e-08,  ..., 5.3369e-08, 1.8337e-06,\n",
       "             1.5673e-06],\n",
       "            [1.4655e-05, 3.1814e-06, 1.2651e-05,  ..., 7.4763e-08, 2.8728e-10,\n",
       "             3.4292e-09]], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([[5.9373e-06, 1.0347e-05, 1.4619e-05,  ..., 1.2242e-07, 1.6284e-06,\n",
       "             2.7770e-05],\n",
       "            [1.1990e-08, 1.2770e-06, 7.1258e-05,  ..., 1.3196e-07, 9.7318e-05,\n",
       "             9.2841e-06],\n",
       "            [2.3406e-04, 1.4922e-06, 4.7584e-05,  ..., 7.7625e-06, 1.3528e-06,\n",
       "             1.4755e-05],\n",
       "            ...,\n",
       "            [6.0705e-07, 6.4170e-08, 2.1201e-05,  ..., 1.0032e-05, 6.7646e-08,\n",
       "             4.0960e-09],\n",
       "            [5.1059e-05, 5.4094e-08, 1.0550e-07,  ..., 1.5467e-07, 5.3272e-06,\n",
       "             4.5630e-06],\n",
       "            [4.0402e-05, 9.1482e-06, 3.5045e-05,  ..., 2.1737e-07, 6.7730e-10,\n",
       "             9.8540e-09]], device='cuda:0')},\n",
       "   1747370804496: {'step': 1114,\n",
       "    'exp_avg': tensor([[ 5.3266e-10,  2.0490e-09, -2.1363e-11,  ..., -9.1916e-11,\n",
       "              1.2102e-10,  1.0861e-10],\n",
       "            [-2.4333e-10, -8.6486e-10, -3.1852e-10,  ...,  3.5898e-10,\n",
       "              1.3670e-09,  7.1982e-10],\n",
       "            [ 1.9802e-10, -1.7310e-09,  2.3571e-09,  ...,  8.6925e-11,\n",
       "              1.9616e-09, -3.0219e-10],\n",
       "            ...,\n",
       "            [ 8.0358e-10, -3.5490e-10,  5.0168e-10,  ..., -1.7474e-09,\n",
       "              1.1953e-09,  1.7418e-10],\n",
       "            [-6.0733e-10, -8.5283e-10, -1.9192e-10,  ..., -5.9714e-10,\n",
       "              3.8860e-10, -4.1270e-10],\n",
       "            [-3.1061e-10,  6.2233e-12,  1.0716e-10,  ..., -6.4986e-10,\n",
       "             -2.6915e-10,  7.2270e-10]], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([[2.8340e-05, 2.1924e-04, 1.1578e-07,  ..., 1.7595e-04, 3.0015e-06,\n",
       "             1.3925e-04],\n",
       "            [2.0199e-06, 4.1051e-05, 2.2097e-05,  ..., 1.0847e-06, 2.8736e-05,\n",
       "             1.2556e-05],\n",
       "            [7.4392e-06, 1.6408e-06, 2.1274e-04,  ..., 8.2988e-06, 2.1373e-04,\n",
       "             1.8129e-04],\n",
       "            ...,\n",
       "            [3.8021e-05, 3.7635e-05, 1.3633e-04,  ..., 4.2543e-06, 7.5271e-05,\n",
       "             4.9236e-05],\n",
       "            [1.3526e-05, 1.5133e-04, 1.4288e-07,  ..., 1.1620e-05, 9.7343e-05,\n",
       "             2.1789e-05],\n",
       "            [3.2026e-06, 1.6229e-04, 2.3833e-05,  ..., 2.9591e-05, 3.0629e-06,\n",
       "             1.0227e-05]], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([[7.6229e-05, 5.3008e-04, 3.3684e-07,  ..., 4.3149e-04, 8.6425e-06,\n",
       "             3.4642e-04],\n",
       "            [5.8584e-06, 1.0871e-04, 6.0013e-05,  ..., 3.1701e-06, 7.7253e-05,\n",
       "             3.4789e-05],\n",
       "            [2.0946e-05, 4.7744e-06, 5.1535e-04,  ..., 2.3291e-05, 5.1765e-04,\n",
       "             4.4376e-04],\n",
       "            ...,\n",
       "            [1.0103e-04, 1.0005e-04, 3.3956e-04,  ..., 1.2154e-05, 1.9369e-04,\n",
       "             1.2933e-04],\n",
       "            [3.7386e-05, 3.7458e-04, 4.1509e-07,  ..., 3.2274e-05, 2.4712e-04,\n",
       "             5.9210e-05],\n",
       "            [9.2104e-06, 4.0000e-04, 6.4547e-05,  ..., 7.9456e-05, 8.8152e-06,\n",
       "             2.8521e-05]], device='cuda:0')},\n",
       "   1747370803776: {'step': 1114,\n",
       "    'exp_avg': tensor([-4.3899e-07, -1.5855e-06, -2.9392e-06,  ..., -1.0784e-06,\n",
       "            -2.8348e-06,  6.4552e-07], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([6.0318e-07, 5.3839e-07, 5.9793e-07,  ..., 1.7438e-06, 1.0191e-05,\n",
       "            3.1673e-07], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([1.7527e-06, 1.5615e-06, 1.7281e-06,  ..., 5.0566e-06, 2.8424e-05,\n",
       "            9.0899e-07], device='cuda:0')},\n",
       "   1747370803416: {'step': 1114,\n",
       "    'exp_avg': tensor([-1.7795e-07, -1.6212e-07, -2.9464e-06,  ...,  1.4375e-06,\n",
       "            -2.4937e-06,  9.9057e-07], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([5.0551e-06, 2.4353e-05, 5.7724e-07,  ..., 1.4141e-08, 1.6585e-06,\n",
       "            3.7555e-06], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([1.4372e-05, 6.5904e-05, 1.6625e-06,  ..., 2.1206e-08, 4.8122e-06,\n",
       "            1.0753e-05], device='cuda:0')},\n",
       "   1747370803848: {'step': 1114,\n",
       "    'exp_avg': tensor([0.2370, 0.2199, 0.2111, 0.2149, 0.2134, 0.2408, 0.2005, 0.2138, 0.1916,\n",
       "            0.2959, 0.2158, 0.1382, 0.2203, 0.2572, 0.2704, 0.2524, 0.2123, 0.2283,\n",
       "            0.2810, 0.1793, 0.2299, 0.1814, 0.2258, 0.2315, 0.2227, 0.1974, 0.2229,\n",
       "            0.2604, 0.2261, 0.2672, 0.2039, 0.2687, 0.2075, 0.2299, 0.2168, 0.1896,\n",
       "            0.1464, 0.2675, 0.1857, 0.2031, 0.2647, 0.2020, 0.1756, 0.1996, 0.2028,\n",
       "            0.2128, 0.2296, 0.2600, 0.2345, 0.3050, 0.2645, 0.2352, 0.2445, 0.1838,\n",
       "            0.2652, 0.1863, 0.2089, 0.2862, 0.2790, 0.2593, 0.1819, 0.2107, 0.2222,\n",
       "            0.1841, 0.2626, 0.2598, 0.2020, 0.1497, 0.1782, 0.1888, 0.2837, 0.3209,\n",
       "            0.2199, 0.2528, 0.2627, 0.2978, 0.1795, 0.2485, 0.2307, 0.2243, 0.2197,\n",
       "            0.2699, 0.3439, 0.1631, 0.1367, 0.2337, 0.1922, 0.1409, 0.1946, 0.1715,\n",
       "            0.1998, 0.1919, 0.2854, 0.1901, 0.1940, 0.2251, 0.1804, 0.1985, 0.2844,\n",
       "            0.3077, 0.2428, 0.2466, 0.1892, 0.2656, 0.1970, 0.2130, 0.2271, 0.2144,\n",
       "            0.2687, 0.1745, 0.1808, 0.2392, 0.2417, 0.1828, 0.2392, 0.2171, 0.2345,\n",
       "            0.1918, 0.3090, 0.2093, 0.2378, 0.1932, 0.1763, 0.2023, 0.1381, 0.2047,\n",
       "            0.2246, 0.2146, 0.1695, 0.2480, 0.2494, 0.2171, 0.1422, 0.1853, 0.2448,\n",
       "            0.2360, 0.2821, 0.2337, 0.1566, 0.1764, 0.3250, 0.2166, 0.1689, 0.1979,\n",
       "            0.2448, 0.1712, 0.1923, 0.2359, 0.1960, 0.1581, 0.2330, 0.2410, 0.2438,\n",
       "            0.2119, 0.2046, 0.2332, 0.2489, 0.3083, 0.2150, 0.1617, 0.1758, 0.2322,\n",
       "            0.2319, 0.2152, 0.2234, 0.1839, 0.2450, 0.2318, 0.2478, 0.2133, 0.1810,\n",
       "            0.1766, 0.1928, 0.1873, 0.2807, 0.2137, 0.2475, 0.1597, 0.2457, 0.2253,\n",
       "            0.2170, 0.2943, 0.1848, 0.2107, 0.1970, 0.1900, 0.2159, 0.2522, 0.2552,\n",
       "            0.2554, 0.2315, 0.2365, 0.1949, 0.2135, 0.2731, 0.2412, 0.2173, 0.1597,\n",
       "            0.2017, 0.2087, 0.2334, 0.2448, 0.2697, 0.3308, 0.1880, 0.1842, 0.2004,\n",
       "            0.2388, 0.2040, 0.2454, 0.2667, 0.1935, 0.2150, 0.2009, 0.2340, 0.1777,\n",
       "            0.2411, 0.1920, 0.2059, 0.2192, 0.1651, 0.1831, 0.1562, 0.2500, 0.2932,\n",
       "            0.1780, 0.2727, 0.1995, 0.1731, 0.2463, 0.1881, 0.2344, 0.2355, 0.2024,\n",
       "            0.1733, 0.2337, 0.3169, 0.2384, 0.2748, 0.2881, 0.1717, 0.2298, 0.2472,\n",
       "            0.2589, 0.2240, 0.1837, 0.2389, 0.2181, 0.1600, 0.2118, 0.2242, 0.1994,\n",
       "            0.1677, 0.2070, 0.2197, 0.2335, 0.1791, 0.2558, 0.2162, 0.1896, 0.2200,\n",
       "            0.2030, 0.2498, 0.1696, 0.2301, 0.1456, 0.2531, 0.2183, 0.2300, 0.1967,\n",
       "            0.2645, 0.3221, 0.2894, 0.2458, 0.2181, 0.2019, 0.2410, 0.1901, 0.2402,\n",
       "            0.2187, 0.1977, 0.2910, 0.1706, 0.2584, 0.1557, 0.2148, 0.2035, 0.2158,\n",
       "            0.2163, 0.1819, 0.2172, 0.2205, 0.1331, 0.1964, 0.2317, 0.2051, 0.1943,\n",
       "            0.2448, 0.2119, 0.2274], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([0.1138, 0.1035, 0.0983, 0.1005, 0.0997, 0.1162, 0.0922, 0.0999, 0.0872,\n",
       "            0.1528, 0.1011, 0.0594, 0.1038, 0.1270, 0.1356, 0.1235, 0.0990, 0.1085,\n",
       "            0.1429, 0.0804, 0.1095, 0.0815, 0.1071, 0.1104, 0.1052, 0.0904, 0.1053,\n",
       "            0.1287, 0.1072, 0.1332, 0.0941, 0.1348, 0.0962, 0.1095, 0.1017, 0.0861,\n",
       "            0.0634, 0.1333, 0.0839, 0.0937, 0.1318, 0.0931, 0.0785, 0.0917, 0.0935,\n",
       "            0.0993, 0.1094, 0.1284, 0.1123, 0.1592, 0.1314, 0.1127, 0.1185, 0.0829,\n",
       "            0.1318, 0.0843, 0.0970, 0.1460, 0.1415, 0.1280, 0.0818, 0.0981, 0.1049,\n",
       "            0.0830, 0.1301, 0.1283, 0.0931, 0.0651, 0.0799, 0.0856, 0.1447, 0.1707,\n",
       "            0.1035, 0.1240, 0.1303, 0.1542, 0.0806, 0.1211, 0.1099, 0.1061, 0.1034,\n",
       "            0.1350, 0.1902, 0.0719, 0.0587, 0.1120, 0.0875, 0.0607, 0.0889, 0.0763,\n",
       "            0.0918, 0.0873, 0.1458, 0.0863, 0.0885, 0.1066, 0.0810, 0.0911, 0.1448,\n",
       "            0.1609, 0.1174, 0.1199, 0.0859, 0.1321, 0.0902, 0.0994, 0.1078, 0.1002,\n",
       "            0.1341, 0.0779, 0.0812, 0.1154, 0.1168, 0.0823, 0.1152, 0.1018, 0.1126,\n",
       "            0.0873, 0.1625, 0.0973, 0.1144, 0.0881, 0.0788, 0.0932, 0.0593, 0.0946,\n",
       "            0.1063, 0.1003, 0.0753, 0.1208, 0.1225, 0.1018, 0.0613, 0.0837, 0.1187,\n",
       "            0.1132, 0.1434, 0.1118, 0.0685, 0.0789, 0.1736, 0.1015, 0.0749, 0.0907,\n",
       "            0.1187, 0.0761, 0.0876, 0.1132, 0.0896, 0.0693, 0.1114, 0.1164, 0.1181,\n",
       "            0.0988, 0.0945, 0.1115, 0.1213, 0.1621, 0.1006, 0.0711, 0.0786, 0.1109,\n",
       "            0.1107, 0.1007, 0.1056, 0.0830, 0.1191, 0.1106, 0.1206, 0.0996, 0.0814,\n",
       "            0.0790, 0.0879, 0.0848, 0.1425, 0.0999, 0.1204, 0.0701, 0.1193, 0.1067,\n",
       "            0.1018, 0.1524, 0.0835, 0.0981, 0.0902, 0.0863, 0.1011, 0.1241, 0.1253,\n",
       "            0.1254, 0.1105, 0.1135, 0.0890, 0.0997, 0.1371, 0.1164, 0.1019, 0.0701,\n",
       "            0.0929, 0.0969, 0.1116, 0.1187, 0.1348, 0.1791, 0.0852, 0.0831, 0.0921,\n",
       "            0.1150, 0.0942, 0.1191, 0.1328, 0.0883, 0.1005, 0.0924, 0.1120, 0.0796,\n",
       "            0.1164, 0.0874, 0.0953, 0.1031, 0.0729, 0.0825, 0.0682, 0.1220, 0.1511,\n",
       "            0.0798, 0.1372, 0.0916, 0.0771, 0.1197, 0.0852, 0.1123, 0.1129, 0.0932,\n",
       "            0.0772, 0.1118, 0.1682, 0.1149, 0.1383, 0.1473, 0.0764, 0.1094, 0.1203,\n",
       "            0.1277, 0.1059, 0.0828, 0.1151, 0.1024, 0.0703, 0.0987, 0.1061, 0.0916,\n",
       "            0.0743, 0.0959, 0.1034, 0.1117, 0.0803, 0.1257, 0.1013, 0.0860, 0.1035,\n",
       "            0.0937, 0.1218, 0.0753, 0.1096, 0.0630, 0.1240, 0.1025, 0.1096, 0.0901,\n",
       "            0.1319, 0.1746, 0.1481, 0.1193, 0.1024, 0.0930, 0.1163, 0.0863, 0.1159,\n",
       "            0.1035, 0.0906, 0.1502, 0.0758, 0.1274, 0.0681, 0.1005, 0.0939, 0.1011,\n",
       "            0.1013, 0.0819, 0.1019, 0.1038, 0.0569, 0.0899, 0.1106, 0.0948, 0.0887,\n",
       "            0.1187, 0.0988, 0.1080], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([0.1275, 0.1172, 0.1120, 0.1142, 0.1133, 0.1299, 0.1058, 0.1136, 0.1008,\n",
       "            0.1660, 0.1148, 0.0722, 0.1174, 0.1407, 0.1491, 0.1372, 0.1127, 0.1222,\n",
       "            0.1564, 0.0939, 0.1231, 0.0950, 0.1208, 0.1241, 0.1188, 0.1041, 0.1190,\n",
       "            0.1422, 0.1209, 0.1467, 0.1078, 0.1484, 0.1099, 0.1232, 0.1153, 0.0996,\n",
       "            0.0764, 0.1468, 0.0975, 0.1073, 0.1453, 0.1067, 0.0919, 0.1053, 0.1072,\n",
       "            0.1129, 0.1230, 0.1420, 0.1259, 0.1724, 0.1449, 0.1264, 0.1322, 0.0964,\n",
       "            0.1454, 0.0978, 0.1107, 0.1593, 0.1550, 0.1415, 0.0953, 0.1118, 0.1186,\n",
       "            0.0965, 0.1436, 0.1418, 0.1067, 0.0781, 0.0933, 0.0992, 0.1581, 0.1836,\n",
       "            0.1172, 0.1376, 0.1438, 0.1675, 0.0940, 0.1347, 0.1236, 0.1198, 0.1171,\n",
       "            0.1484, 0.2031, 0.0852, 0.0715, 0.1258, 0.1011, 0.0736, 0.1025, 0.0897,\n",
       "            0.1055, 0.1009, 0.1592, 0.0999, 0.1021, 0.1203, 0.0945, 0.1047, 0.1581,\n",
       "            0.1740, 0.1311, 0.1335, 0.0994, 0.1456, 0.1038, 0.1131, 0.1215, 0.1139,\n",
       "            0.1476, 0.0913, 0.0947, 0.1292, 0.1304, 0.0959, 0.1289, 0.1155, 0.1263,\n",
       "            0.1009, 0.1757, 0.1109, 0.1280, 0.1017, 0.0922, 0.1069, 0.0721, 0.1082,\n",
       "            0.1200, 0.1140, 0.0886, 0.1344, 0.1362, 0.1155, 0.0743, 0.0972, 0.1324,\n",
       "            0.1269, 0.1568, 0.1255, 0.0817, 0.0923, 0.1864, 0.1152, 0.0882, 0.1043,\n",
       "            0.1324, 0.0895, 0.1012, 0.1269, 0.1032, 0.0825, 0.1251, 0.1300, 0.1317,\n",
       "            0.1124, 0.1082, 0.1252, 0.1349, 0.1753, 0.1143, 0.0844, 0.0920, 0.1246,\n",
       "            0.1244, 0.1144, 0.1193, 0.0965, 0.1328, 0.1243, 0.1342, 0.1133, 0.0949,\n",
       "            0.0924, 0.1015, 0.0984, 0.1559, 0.1135, 0.1341, 0.0834, 0.1329, 0.1204,\n",
       "            0.1155, 0.1658, 0.0970, 0.1118, 0.1038, 0.0999, 0.1148, 0.1381, 0.1389,\n",
       "            0.1390, 0.1241, 0.1272, 0.1027, 0.1134, 0.1505, 0.1301, 0.1156, 0.0834,\n",
       "            0.1065, 0.1106, 0.1253, 0.1323, 0.1483, 0.1920, 0.0987, 0.0966, 0.1058,\n",
       "            0.1286, 0.1078, 0.1327, 0.1463, 0.1019, 0.1142, 0.1061, 0.1257, 0.0930,\n",
       "            0.1301, 0.1010, 0.1089, 0.1168, 0.0862, 0.0961, 0.0814, 0.1356, 0.1645,\n",
       "            0.0932, 0.1508, 0.1053, 0.0905, 0.1333, 0.0988, 0.1259, 0.1266, 0.1069,\n",
       "            0.0906, 0.1255, 0.1812, 0.1286, 0.1518, 0.1607, 0.0898, 0.1231, 0.1339,\n",
       "            0.1413, 0.1196, 0.0964, 0.1287, 0.1161, 0.0835, 0.1124, 0.1198, 0.1052,\n",
       "            0.0876, 0.1096, 0.1171, 0.1253, 0.0938, 0.1393, 0.1150, 0.0996, 0.1172,\n",
       "            0.1073, 0.1355, 0.0886, 0.1233, 0.0760, 0.1376, 0.1162, 0.1233, 0.1037,\n",
       "            0.1455, 0.1879, 0.1614, 0.1330, 0.1161, 0.1066, 0.1300, 0.0999, 0.1295,\n",
       "            0.1174, 0.1043, 0.1636, 0.0892, 0.1410, 0.0812, 0.1142, 0.1076, 0.1148,\n",
       "            0.1150, 0.0954, 0.1156, 0.1175, 0.0696, 0.1035, 0.1243, 0.1085, 0.1023,\n",
       "            0.1324, 0.1125, 0.1217], device='cuda:0')},\n",
       "   1747370803560: {'step': 1114,\n",
       "    'exp_avg': tensor([ 1.6719e-05, -5.8850e-05,  2.1407e-05, -1.3304e-05, -5.4010e-06,\n",
       "            -2.0815e-05,  7.6418e-06, -9.6657e-06,  3.0512e-06,  2.0606e-04,\n",
       "             2.9561e-09, -2.0122e-05,  6.8657e-06,  1.8969e-04,  3.0771e-05,\n",
       "            -5.8024e-05,  2.5869e-05, -7.2715e-06,  1.9124e-04,  1.7829e-05,\n",
       "             9.4419e-07,  1.3858e-06,  5.5652e-05,  1.6038e-05,  2.1009e-05,\n",
       "             2.1133e-05,  1.7704e-05,  6.4498e-05,  5.9006e-06,  6.4861e-06,\n",
       "             2.3860e-05,  2.2910e-04, -2.9428e-06,  7.7101e-06,  1.2837e-05,\n",
       "             2.4514e-06, -1.6181e-05, -8.5662e-05,  9.1633e-06, -1.0768e-05,\n",
       "            -1.7238e-04, -2.0302e-05,  1.1567e-05,  4.4331e-06, -4.5975e-06,\n",
       "             2.4018e-05,  5.9483e-06, -2.2429e-05, -1.8241e-05, -9.1996e-06,\n",
       "             9.2690e-05,  5.6491e-06,  2.4629e-05, -1.9083e-05, -1.2163e-04,\n",
       "            -1.0960e-05,  1.2537e-05, -1.9891e-05,  1.6647e-05,  1.3072e-05,\n",
       "             3.2134e-06, -1.1191e-05,  1.1868e-05,  1.7198e-05,  7.8171e-05,\n",
       "             2.8548e-05,  1.1171e-08,  1.5976e-05, -4.7578e-06, -2.5578e-06,\n",
       "             1.3865e-05, -3.9556e-05,  3.0977e-05, -4.6488e-05, -2.3071e-05,\n",
       "             6.5582e-05, -2.5397e-05,  1.9832e-05, -1.1044e-05,  2.9369e-05,\n",
       "            -3.1739e-05, -4.5995e-06,  7.9607e-04,  1.4587e-05, -1.9517e-06,\n",
       "            -1.1995e-04, -2.2385e-05, -2.8407e-06,  4.9525e-05, -1.0748e-06,\n",
       "             3.1851e-05,  2.4814e-06,  1.5752e-04, -1.6992e-06, -5.4200e-06,\n",
       "            -7.3436e-05, -2.9628e-05, -1.1485e-05, -3.4778e-05, -1.6507e-04,\n",
       "             2.0676e-05, -1.3556e-05,  1.2820e-05, -3.2345e-05,  8.5678e-06,\n",
       "             2.7029e-05, -3.3881e-05, -5.0547e-06, -5.5633e-05, -5.4784e-06,\n",
       "            -1.0443e-06,  9.3760e-05,  5.3716e-05,  1.5973e-05, -5.1147e-05,\n",
       "            -5.3824e-06, -9.9988e-05,  2.2924e-05,  2.5011e-05, -2.5963e-05,\n",
       "            -1.4793e-05, -4.2736e-06, -4.0683e-06, -2.6726e-06,  2.8217e-05,\n",
       "            -2.0973e-05,  3.5875e-05,  2.1197e-05, -2.5419e-05, -9.1428e-05,\n",
       "            -2.2268e-04,  1.1972e-05,  2.3680e-06, -5.6328e-07,  1.7677e-05,\n",
       "             5.7063e-06,  1.0676e-04, -2.0406e-05,  1.8490e-07,  8.4699e-06,\n",
       "            -7.3673e-05, -1.7873e-05, -6.6925e-06, -3.0799e-06, -2.0246e-05,\n",
       "            -3.0814e-07, -7.9469e-06,  1.0893e-05, -4.6349e-05, -2.4071e-05,\n",
       "             2.3168e-05,  3.3424e-05,  5.5750e-05, -2.3019e-05, -9.2592e-06,\n",
       "            -1.9901e-05,  1.0826e-04, -3.6327e-04,  6.8495e-06,  5.1562e-05,\n",
       "             3.3222e-06,  2.9334e-05,  7.5403e-05, -4.9969e-05,  4.1125e-06,\n",
       "            -1.3098e-05, -1.1446e-04, -1.2251e-05, -2.2857e-05, -3.6210e-06,\n",
       "             7.1178e-06, -7.0689e-06, -7.1598e-07, -9.7463e-06, -3.4610e-05,\n",
       "             1.6215e-05,  1.2413e-05,  7.3179e-06,  2.6870e-06,  2.2860e-06,\n",
       "            -4.4613e-05, -1.6921e-04, -1.6610e-05,  2.8929e-05, -1.3929e-05,\n",
       "            -3.3589e-06, -1.5116e-05, -4.5938e-04,  2.8169e-05,  3.3203e-05,\n",
       "            -3.0697e-05, -5.8442e-05,  1.6658e-05,  6.8123e-05, -1.9997e-04,\n",
       "            -4.1675e-05,  5.3405e-05, -5.1851e-06,  3.5532e-05,  2.7091e-05,\n",
       "            -2.1338e-05,  3.2267e-05, -6.5972e-05, -2.8509e-05,  1.6164e-06,\n",
       "             4.0587e-07, -5.0957e-06, -5.9102e-06,  4.4618e-06, -5.6364e-05,\n",
       "            -5.9807e-05,  4.6167e-05,  4.5342e-06, -6.2583e-06, -5.5187e-06,\n",
       "            -4.0925e-06, -2.2747e-05,  1.1565e-05,  1.7965e-05,  3.1084e-06,\n",
       "            -3.7060e-05,  3.0509e-05,  9.2344e-06,  5.0960e-05, -4.5796e-06,\n",
       "            -1.8761e-05, -3.9260e-05, -2.2518e-05,  1.0308e-05, -1.3186e-05,\n",
       "             3.3069e-06,  2.5126e-05, -2.4947e-05,  9.8497e-06,  2.8037e-05,\n",
       "            -3.1347e-05, -2.6313e-04, -4.7050e-06, -1.0868e-04,  8.2996e-05,\n",
       "             9.9881e-06,  1.1118e-05, -2.8345e-06, -4.7361e-05,  5.0148e-06,\n",
       "             2.5330e-05,  3.3047e-05,  5.6201e-06,  2.7712e-05, -8.6817e-06,\n",
       "            -2.0200e-05,  1.1646e-05, -5.9217e-06, -9.7313e-06,  1.8477e-05,\n",
       "             9.0526e-06,  2.7446e-06,  2.5805e-05,  5.1658e-06, -6.1420e-05,\n",
       "             7.0019e-05,  4.1948e-05, -6.5095e-05, -3.9514e-05, -3.1759e-05,\n",
       "            -2.1988e-05,  3.0944e-05,  9.1720e-06,  1.8732e-06, -8.4875e-06,\n",
       "             1.7880e-04, -4.4198e-04,  2.2664e-05, -2.9725e-05,  3.6355e-05,\n",
       "             8.2832e-06, -9.7614e-06,  2.6402e-07, -1.6712e-05,  2.0551e-04,\n",
       "             3.7528e-05,  1.6264e-04, -8.0693e-06, -4.0010e-05,  1.3717e-05,\n",
       "            -1.2226e-05,  3.9442e-06, -7.1751e-06, -4.1109e-05, -3.1050e-05,\n",
       "             4.7018e-06,  1.1653e-05, -1.6337e-06, -1.6092e-05, -7.1466e-06,\n",
       "            -5.9811e-05, -7.9093e-06,  8.1069e-06,  2.1322e-05,  4.0614e-05],\n",
       "           device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([5.3944e-07, 3.9078e-07, 2.7310e-06, 1.5817e-06, 1.1905e-04, 1.4606e-05,\n",
       "            2.6264e-07, 3.3711e-06, 7.4555e-07, 1.0523e-04, 2.7302e-07, 5.0859e-06,\n",
       "            5.9322e-06, 7.1874e-05, 3.3988e-05, 2.4868e-06, 1.5160e-05, 9.2029e-06,\n",
       "            9.0273e-06, 1.3304e-07, 5.8804e-05, 8.7841e-07, 3.0519e-06, 1.0600e-04,\n",
       "            9.4135e-07, 1.4476e-05, 1.5129e-04, 1.9678e-04, 9.0734e-06, 8.9136e-05,\n",
       "            9.6023e-06, 1.1485e-05, 1.1613e-05, 1.1228e-04, 1.0137e-06, 1.3916e-05,\n",
       "            2.4928e-07, 3.9543e-07, 6.6081e-06, 8.1144e-06, 5.2999e-07, 3.3455e-05,\n",
       "            3.7668e-06, 1.7742e-06, 7.4843e-07, 3.4606e-04, 4.4573e-06, 2.1612e-06,\n",
       "            9.6416e-05, 4.1693e-05, 1.1024e-05, 6.0332e-05, 5.2998e-06, 1.4433e-06,\n",
       "            3.6071e-07, 2.9699e-07, 1.1938e-04, 2.2271e-05, 1.9889e-05, 7.3155e-06,\n",
       "            5.9562e-06, 3.5893e-05, 2.7479e-05, 1.1188e-07, 1.7946e-04, 2.6132e-06,\n",
       "            4.7793e-07, 1.2136e-06, 1.2072e-06, 3.1071e-07, 6.5754e-07, 1.1293e-04,\n",
       "            2.7413e-05, 1.3073e-04, 1.0760e-06, 1.1660e-05, 1.2855e-07, 2.0019e-06,\n",
       "            3.4774e-07, 1.8926e-05, 8.5040e-06, 1.5716e-05, 9.7961e-06, 2.5587e-07,\n",
       "            2.0157e-07, 9.7488e-06, 1.3426e-07, 3.0606e-06, 6.5440e-06, 3.5518e-06,\n",
       "            1.5236e-05, 1.4106e-07, 1.1510e-06, 3.9187e-07, 5.6874e-07, 1.4814e-04,\n",
       "            1.3593e-05, 2.0096e-05, 3.9049e-05, 8.1205e-05, 3.6080e-05, 9.3705e-06,\n",
       "            3.2597e-07, 1.4200e-04, 1.3110e-05, 9.2705e-06, 1.9851e-04, 6.7539e-05,\n",
       "            1.0741e-04, 1.2448e-05, 2.6095e-07, 2.8352e-04, 1.2865e-05, 3.4979e-05,\n",
       "            5.1209e-05, 1.0100e-04, 2.6434e-05, 4.1889e-06, 2.1897e-06, 9.2215e-06,\n",
       "            3.5201e-05, 4.4738e-06, 1.9822e-05, 2.2341e-05, 1.9455e-06, 2.4397e-05,\n",
       "            1.7171e-05, 3.3243e-06, 9.9375e-08, 3.1547e-05, 4.2241e-04, 1.8769e-05,\n",
       "            1.2308e-06, 9.3298e-07, 1.6691e-05, 3.8535e-05, 2.8022e-05, 6.7055e-06,\n",
       "            4.4809e-06, 3.5675e-05, 3.4129e-06, 1.5654e-07, 1.9667e-07, 5.5129e-07,\n",
       "            1.1140e-06, 6.8746e-07, 2.1370e-07, 1.3813e-07, 1.1293e-05, 1.8594e-07,\n",
       "            1.8648e-07, 2.7432e-05, 2.1948e-05, 4.3364e-07, 3.4788e-05, 8.3631e-06,\n",
       "            3.9545e-05, 4.5097e-06, 1.1512e-06, 6.8519e-06, 4.8613e-06, 3.5481e-06,\n",
       "            5.5986e-06, 4.8726e-05, 1.0796e-07, 3.6827e-07, 3.5642e-04, 1.2051e-05,\n",
       "            9.9933e-05, 5.5608e-07, 4.7753e-06, 8.6237e-06, 8.3528e-05, 1.4122e-07,\n",
       "            6.1582e-04, 1.3176e-05, 2.9685e-04, 2.9510e-07, 2.1165e-05, 4.2203e-04,\n",
       "            8.9533e-08, 1.1493e-05, 7.6800e-06, 1.4869e-07, 2.0199e-05, 9.6053e-08,\n",
       "            1.1054e-04, 8.5432e-06, 3.2398e-06, 2.7082e-05, 6.0398e-06, 2.8269e-05,\n",
       "            2.8161e-05, 1.3094e-06, 1.3085e-04, 6.4757e-06, 7.8517e-05, 1.4267e-06,\n",
       "            1.8878e-05, 3.4460e-06, 7.4330e-05, 1.6573e-07, 1.7549e-05, 1.9824e-05,\n",
       "            2.1214e-05, 3.5810e-07, 2.7024e-06, 7.2108e-07, 1.9435e-06, 1.8249e-06,\n",
       "            5.0136e-05, 1.5614e-07, 6.4984e-05, 8.8302e-06, 1.0032e-05, 9.8669e-07,\n",
       "            1.3005e-04, 1.0619e-06, 6.5114e-05, 7.3735e-06, 7.9441e-08, 9.6174e-08,\n",
       "            1.3541e-06, 2.5968e-04, 8.7693e-06, 3.3748e-06, 1.2718e-06, 1.1770e-05,\n",
       "            2.1696e-07, 3.4301e-05, 9.9651e-06, 1.5580e-07, 5.9717e-05, 6.5437e-07,\n",
       "            1.6608e-06, 1.5553e-04, 3.5183e-06, 3.2833e-05, 1.3575e-05, 3.1234e-05,\n",
       "            2.6364e-05, 4.2804e-07, 6.3250e-07, 2.1234e-05, 2.5773e-05, 5.1631e-07,\n",
       "            1.9066e-05, 1.9657e-04, 1.5903e-06, 1.7491e-04, 1.1454e-06, 2.3096e-05,\n",
       "            2.0580e-07, 2.0915e-04, 3.8045e-05, 3.0939e-06, 7.0928e-07, 3.9284e-05,\n",
       "            4.7974e-07, 8.5957e-06, 8.2724e-06, 1.1283e-07, 2.5112e-07, 9.8048e-05,\n",
       "            3.2118e-05, 4.2409e-06, 3.4527e-06, 4.9118e-05, 1.0671e-05, 6.1721e-06,\n",
       "            5.7410e-05, 2.7133e-05, 5.3615e-06, 3.5490e-06, 2.1767e-05, 4.0509e-07,\n",
       "            2.3635e-05, 5.2925e-06, 1.1345e-06, 9.1463e-05, 2.7758e-06, 2.3631e-04,\n",
       "            1.0222e-07, 5.0228e-04, 2.3035e-05, 8.4442e-07, 1.2764e-05, 4.2933e-05,\n",
       "            4.5405e-05, 9.1114e-08, 2.2468e-05, 9.8967e-06, 3.7951e-06, 7.4080e-06,\n",
       "            8.9507e-06, 1.3364e-07, 7.7676e-05, 8.6604e-06, 2.3845e-07, 4.0523e-06],\n",
       "           device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([1.2717e-06, 7.6450e-07, 7.5180e-06, 4.2125e-06, 2.9899e-04, 3.9985e-05,\n",
       "            4.6387e-07, 9.4003e-06, 1.7148e-06, 2.6517e-04, 5.7219e-07, 1.4293e-05,\n",
       "            1.6483e-05, 1.8309e-04, 8.9958e-05, 6.4231e-06, 4.1481e-05, 2.5412e-05,\n",
       "            1.8050e-05, 1.4440e-07, 1.5324e-04, 2.3718e-06, 8.4241e-06, 2.6756e-04,\n",
       "            2.4346e-06, 3.9755e-05, 3.7455e-04, 4.7887e-04, 2.5325e-05, 2.2726e-04,\n",
       "            2.6549e-05, 2.3989e-05, 3.2095e-05, 2.8295e-04, 2.6067e-06, 3.8320e-05,\n",
       "            6.0237e-07, 4.6357e-07, 1.8514e-05, 2.2489e-05, 5.2999e-07, 8.9233e-05,\n",
       "            1.0603e-05, 4.7727e-06, 1.8297e-06, 8.1034e-04, 1.2480e-05, 5.7766e-06,\n",
       "            2.4509e-04, 1.0975e-04, 3.0143e-05, 1.5674e-04, 1.4619e-05, 3.7533e-06,\n",
       "            3.6108e-07, 6.3024e-07, 2.9938e-04, 5.9908e-05, 5.2427e-05, 1.9955e-05,\n",
       "            1.6801e-05, 9.5424e-05, 7.3822e-05, 1.2847e-07, 4.3930e-04, 7.1220e-06,\n",
       "            1.0063e-06, 3.4277e-06, 3.3276e-06, 6.0780e-07, 6.5767e-07, 2.8395e-04,\n",
       "            7.3304e-05, 3.2547e-04, 2.5302e-06, 3.0666e-05, 2.0326e-07, 5.4648e-06,\n",
       "            6.8954e-07, 5.1198e-05, 2.3657e-05, 4.2770e-05, 9.7961e-06, 5.6348e-07,\n",
       "            4.1464e-07, 2.5357e-05, 1.5329e-07, 8.6721e-06, 1.8300e-05, 1.0014e-05,\n",
       "            4.1741e-05, 1.6084e-07, 1.1553e-06, 9.4613e-07, 1.4883e-06, 3.6715e-04,\n",
       "            3.7412e-05, 5.4703e-05, 1.0316e-04, 2.0737e-04, 9.5753e-05, 2.5842e-05,\n",
       "            7.4816e-07, 3.5273e-04, 3.6050e-05, 2.5712e-05, 4.8311e-04, 1.7466e-04,\n",
       "            2.7095e-04, 3.4291e-05, 5.9957e-07, 6.7257e-04, 3.5269e-05, 9.3169e-05,\n",
       "            1.3407e-04, 2.5578e-04, 6.9298e-05, 1.1782e-05, 2.1915e-06, 2.5295e-05,\n",
       "            9.3748e-05, 1.2521e-05, 5.3970e-05, 6.0602e-05, 5.5113e-06, 6.5844e-05,\n",
       "            4.6027e-05, 9.2132e-06, 1.0282e-07, 8.4236e-05, 9.6998e-04, 5.1081e-05,\n",
       "            3.4865e-06, 2.5325e-06, 4.5605e-05, 1.0199e-04, 7.4391e-05, 1.8655e-05,\n",
       "            1.2622e-05, 9.4702e-05, 8.8690e-06, 2.1854e-07, 3.4222e-07, 1.0615e-06,\n",
       "            2.9479e-06, 1.7754e-06, 4.2785e-07, 1.4756e-07, 3.1161e-05, 3.2116e-07,\n",
       "            2.1424e-07, 7.3623e-05, 5.9068e-05, 1.0594e-06, 9.2905e-05, 2.3199e-05,\n",
       "            1.0455e-04, 9.6842e-06, 3.1072e-06, 1.9214e-05, 1.3482e-05, 9.8485e-06,\n",
       "            1.4394e-05, 1.2796e-04, 1.2887e-07, 7.3182e-07, 8.3156e-04, 3.3187e-05,\n",
       "            2.5295e-04, 1.3984e-06, 1.3380e-05, 2.3995e-05, 2.1369e-04, 2.3392e-07,\n",
       "            1.3761e-03, 3.6181e-05, 7.0335e-04, 6.8430e-07, 5.7347e-05, 9.7394e-04,\n",
       "            9.3397e-08, 2.6304e-05, 2.1391e-05, 1.7232e-07, 5.4978e-05, 9.7006e-08,\n",
       "            2.7864e-04, 8.5432e-06, 8.9070e-06, 7.2674e-05, 1.6892e-05, 7.5824e-05,\n",
       "            7.5636e-05, 3.5665e-06, 3.2607e-04, 1.7986e-05, 2.0161e-04, 3.8777e-06,\n",
       "            5.1436e-05, 9.6743e-06, 1.9096e-04, 2.0632e-07, 4.7668e-05, 4.3456e-05,\n",
       "            5.7630e-05, 6.8538e-07, 7.5834e-06, 1.7328e-06, 5.3795e-06, 4.8558e-06,\n",
       "            1.3130e-04, 1.7372e-07, 1.6835e-04, 2.4584e-05, 2.7646e-05, 2.6596e-06,\n",
       "            3.2459e-04, 2.9762e-06, 1.6872e-04, 2.0457e-05, 9.2995e-08, 1.0696e-07,\n",
       "            2.9445e-06, 6.2064e-04, 2.2822e-05, 9.5069e-06, 1.2731e-06, 3.2430e-05,\n",
       "            3.7961e-07, 9.1308e-05, 2.7782e-05, 1.8046e-07, 1.5523e-04, 1.4280e-06,\n",
       "            4.2961e-06, 3.8446e-04, 3.8477e-06, 8.6718e-05, 3.5988e-05, 8.2768e-05,\n",
       "            7.1065e-05, 9.8282e-07, 1.5258e-06, 5.7333e-05, 6.9383e-05, 1.0548e-06,\n",
       "            5.1836e-05, 4.7886e-04, 4.5081e-06, 4.2913e-04, 3.0227e-06, 6.2543e-05,\n",
       "            4.2098e-07, 5.0725e-04, 1.0105e-04, 8.6441e-06, 1.7398e-06, 1.0410e-04,\n",
       "            1.1853e-06, 2.3987e-05, 2.2928e-05, 1.1561e-07, 3.1499e-07, 2.4876e-04,\n",
       "            8.5667e-05, 1.1563e-05, 9.5272e-06, 1.2875e-04, 2.9435e-05, 1.7315e-05,\n",
       "            1.4661e-04, 2.7133e-05, 1.4714e-05, 9.7813e-06, 5.8835e-05, 9.5905e-07,\n",
       "            6.3781e-05, 1.4795e-05, 2.9814e-06, 2.2033e-04, 7.7855e-06, 5.6172e-04,\n",
       "            1.5764e-07, 1.1423e-03, 6.2405e-05, 2.1881e-06, 3.5234e-05, 1.1341e-04,\n",
       "            1.1966e-04, 1.2369e-07, 6.0836e-05, 2.7423e-05, 1.0723e-05, 2.0498e-05,\n",
       "            2.4774e-05, 1.4117e-07, 1.9927e-04, 2.3895e-05, 4.1024e-07, 1.1305e-05],\n",
       "           device='cuda:0')},\n",
       "   1747370803344: {'step': 1114,\n",
       "    'exp_avg': tensor([[-4.0246e-07,  2.4461e-07, -1.0657e-06,  ..., -6.2188e-08,\n",
       "             -3.1570e-07, -1.0624e-07],\n",
       "            [ 1.5167e-09,  4.7265e-09,  8.7471e-09,  ...,  2.0372e-09,\n",
       "             -5.2434e-09, -1.0870e-10],\n",
       "            [ 7.5460e-08, -1.0743e-07,  4.7731e-08,  ..., -3.0301e-09,\n",
       "             -9.4515e-08, -4.2247e-09],\n",
       "            ...,\n",
       "            [-4.7768e-07,  2.3189e-06, -1.1341e-06,  ..., -1.6794e-07,\n",
       "             -1.4981e-07, -6.5720e-08],\n",
       "            [ 1.4897e-06, -1.8190e-06, -4.1486e-07,  ..., -2.3383e-08,\n",
       "              1.3776e-07,  1.7385e-07],\n",
       "            [ 3.6145e-07, -6.8865e-08,  1.6052e-08,  ..., -2.3452e-08,\n",
       "             -1.3729e-07, -6.8325e-09]], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([[2.6913e-05, 2.8631e-05, 2.9163e-05,  ..., 9.9209e-06, 1.4419e-06,\n",
       "             2.6914e-05],\n",
       "            [1.7718e-07, 1.2038e-05, 1.2813e-05,  ..., 2.3183e-08, 5.9922e-05,\n",
       "             1.2758e-05],\n",
       "            [3.3411e-04, 4.1018e-07, 8.1199e-06,  ..., 2.9524e-06, 1.7989e-07,\n",
       "             2.3035e-04],\n",
       "            ...,\n",
       "            [2.5883e-06, 3.2772e-05, 1.0102e-05,  ..., 6.6681e-06, 1.6597e-04,\n",
       "             3.4021e-05],\n",
       "            [9.8717e-08, 1.0720e-08, 2.7632e-05,  ..., 1.9711e-05, 1.7180e-04,\n",
       "             5.0181e-06],\n",
       "            [6.3832e-06, 1.7263e-04, 4.3418e-08,  ..., 6.5790e-05, 1.4240e-04,\n",
       "             2.5623e-05]], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([[7.2519e-05, 7.6988e-05, 7.8369e-05,  ..., 2.7694e-05, 4.2010e-06,\n",
       "             7.2542e-05],\n",
       "            [5.0867e-07, 3.3392e-05, 3.5471e-05,  ..., 6.6275e-08, 1.5594e-04,\n",
       "             3.5321e-05],\n",
       "            [7.8432e-04, 1.1907e-06, 2.2803e-05,  ..., 8.5030e-06, 5.2093e-07,\n",
       "             5.5511e-04],\n",
       "            ...,\n",
       "            [7.4648e-06, 8.7644e-05, 2.8169e-05,  ..., 1.8829e-05, 4.0856e-04,\n",
       "             9.0830e-05],\n",
       "            [2.7445e-07, 1.6997e-08, 7.4389e-05,  ..., 5.3764e-05, 4.2196e-04,\n",
       "             1.4277e-05],\n",
       "            [1.8034e-05, 4.2382e-04, 1.1575e-07,  ..., 1.7045e-04, 3.5370e-04,\n",
       "             6.9198e-05]], device='cuda:0')},\n",
       "   1747370803488: {'step': 1114,\n",
       "    'exp_avg': tensor([[ 1.6959e-07,  1.1384e-09,  6.0068e-08,  ..., -7.5695e-08,\n",
       "              2.7365e-08, -4.4871e-09],\n",
       "            [ 1.3711e-09,  8.1186e-08,  2.1059e-10,  ...,  4.6511e-10,\n",
       "              7.7613e-10,  8.0811e-10],\n",
       "            [-1.5396e-09, -4.4732e-10, -2.6549e-10,  ..., -8.8543e-04,\n",
       "              1.2997e-08,  2.7845e-09],\n",
       "            ...,\n",
       "            [ 3.3186e-08, -3.6504e-09, -1.2581e-08,  ..., -4.9152e-08,\n",
       "              4.0733e-08, -1.4779e-08],\n",
       "            [ 1.3841e-07, -2.0205e-08,  4.3651e-05,  ..., -1.8745e-07,\n",
       "              7.0271e-08, -4.8083e-10],\n",
       "            [ 1.5253e-08,  4.3801e-10, -4.1950e-09,  ...,  4.3426e-04,\n",
       "              8.9709e-09, -4.5231e-05]], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([[5.7330e-10, 1.5774e-08, 3.2401e-04,  ..., 5.4055e-06, 7.2762e-05,\n",
       "             2.9614e-05],\n",
       "            [8.0116e-05, 3.2175e-04, 2.0661e-05,  ..., 9.9021e-05, 9.2808e-06,\n",
       "             6.0824e-05],\n",
       "            [9.0274e-05, 3.1370e-05, 6.7423e-05,  ..., 2.2687e-03, 8.9108e-06,\n",
       "             2.1361e-05],\n",
       "            ...,\n",
       "            [8.4033e-05, 4.3186e-05, 1.2860e-04,  ..., 3.9401e-06, 6.1090e-07,\n",
       "             6.5417e-05],\n",
       "            [4.0494e-10, 2.1484e-04, 9.4577e-04,  ..., 1.6902e-05, 1.7736e-06,\n",
       "             2.8440e-05],\n",
       "            [2.2137e-04, 2.6843e-06, 4.9561e-05,  ..., 1.7789e-03, 1.8051e-05,\n",
       "             9.5393e-04]], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([[1.2723e-09, 4.5680e-08, 7.6242e-04,  ..., 1.5352e-05, 1.8756e-04,\n",
       "             7.9518e-05],\n",
       "            [2.0551e-04, 7.5749e-04, 5.6255e-05,  ..., 2.5115e-04, 2.5959e-05,\n",
       "             1.5819e-04],\n",
       "            [2.3011e-04, 8.4030e-05, 1.7446e-04,  ..., 4.4547e-03, 2.4955e-05,\n",
       "             5.8091e-05],\n",
       "            ...,\n",
       "            [2.1502e-04, 1.1411e-04, 3.2141e-04,  ..., 1.1277e-05, 1.7865e-06,\n",
       "             1.6952e-04],\n",
       "            [9.4710e-10, 5.2015e-04, 2.0357e-03,  ..., 4.6356e-05, 5.1546e-06,\n",
       "             7.6489e-05],\n",
       "            [5.3489e-04, 7.7465e-06, 1.3014e-04,  ..., 3.5902e-03, 4.9394e-05,\n",
       "             2.0516e-03]], device='cuda:0')},\n",
       "   1747370807088: {'step': 1114,\n",
       "    'exp_avg': tensor([-1.6282e-06,  1.6288e-08,  2.7499e-07,  ..., -3.1675e-06,\n",
       "             3.4343e-05,  1.2995e-06], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([1.5742e-06, 4.4570e-06, 7.6932e-07,  ..., 2.8115e-08, 4.8059e-08,\n",
       "            2.0024e-05], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([4.5532e-06, 1.2704e-05, 2.2440e-06,  ..., 3.6359e-08, 5.1979e-08,\n",
       "            5.4542e-05], device='cuda:0')},\n",
       "   1747370805792: {'step': 1114,\n",
       "    'exp_avg': tensor([-2.3418e-06,  1.0163e-08,  1.0636e-06,  ...,  1.2251e-05,\n",
       "             3.4241e-05,  6.9805e-07], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([5.3987e-08, 5.7895e-06, 3.4112e-07,  ..., 9.5465e-06, 4.4236e-08,\n",
       "            5.1754e-05], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([1.1936e-07, 1.6401e-05, 9.8328e-07,  ..., 2.6641e-05, 4.6542e-08,\n",
       "            1.3554e-04], device='cuda:0')},\n",
       "   1747370806800: {'step': 1114,\n",
       "    'exp_avg': tensor([0.8926, 0.7799, 0.7223, 0.9221, 0.7874, 0.8532, 0.9190, 0.8436, 0.8167,\n",
       "            0.9286, 0.7520, 0.7084, 0.5532, 0.8836, 0.7566, 0.5893, 0.8535, 0.8730,\n",
       "            0.6556, 0.6869, 0.7686, 0.8071, 0.8634, 0.6211, 0.8909, 0.7228, 0.9481,\n",
       "            0.9702, 0.7152, 0.9682, 0.8668, 0.9480, 0.6100, 0.8592, 0.7316, 0.9081,\n",
       "            0.7915, 0.8422, 0.6970, 0.8209, 0.8156, 0.9035, 0.9281, 0.8311, 0.7332,\n",
       "            0.9323, 0.6789, 0.9136, 0.9331, 0.6181, 0.7396, 0.9313, 0.7565, 0.8915,\n",
       "            0.8798, 0.7636, 0.7704, 0.8722, 0.5387, 0.6761, 0.8455, 0.9401, 0.9429,\n",
       "            0.8298, 0.8741, 0.6983, 0.8757, 0.9232, 0.6989, 0.8923, 0.9325, 0.7444,\n",
       "            0.5655, 0.8316, 0.9672, 0.7191, 0.8829, 0.7046, 0.8642, 0.8570, 0.8295,\n",
       "            0.9213, 0.8879, 0.8646, 0.6694, 0.6222, 0.9244, 0.9736, 0.9742, 0.9100,\n",
       "            0.4975, 0.9129, 0.9111, 0.8101, 0.9851, 0.8013, 0.7352, 0.8187, 0.9008,\n",
       "            0.9201, 0.9300, 0.7630, 0.7331, 0.9900, 1.0620, 0.4304, 0.9715, 0.8232,\n",
       "            0.7739, 0.7211, 0.7230, 0.5828, 0.7768, 0.8676, 0.9537, 0.8491, 0.6124,\n",
       "            0.5728, 0.8796, 0.7149, 0.8832, 0.8722, 0.8082, 0.9066, 0.8741, 0.8430,\n",
       "            0.8377, 0.7354, 0.9948, 0.6324, 0.9040, 0.8450, 0.6146, 0.9175, 0.8335,\n",
       "            0.8035, 0.9186, 0.7670, 0.6212, 0.8133, 0.8988, 0.4322, 0.8619, 0.8425,\n",
       "            0.7026, 0.9505, 0.6738, 0.6060, 0.9377, 0.7811, 0.8280, 0.8202, 0.7473,\n",
       "            0.5974, 0.9530, 0.8444, 0.8694, 0.8428, 0.7247, 0.8360, 0.5982, 0.8757,\n",
       "            0.7606, 0.8151, 0.8577, 0.9105, 0.9007, 0.9107, 0.6522, 0.7815, 0.9224,\n",
       "            0.8552, 0.8263, 0.8162, 0.6337, 0.8369, 0.9647, 0.9716, 0.8956, 0.9739,\n",
       "            0.7711, 0.7971, 0.6639, 0.7446, 0.9757, 1.0045, 0.6973, 0.9796, 0.8587,\n",
       "            0.7290, 0.9321, 0.9335, 0.6903, 0.8662, 0.8630, 0.8811, 0.8364, 0.8226,\n",
       "            0.8811, 0.9245, 0.8024, 0.5781, 0.7107, 0.7779, 0.4883, 0.8588, 1.0058,\n",
       "            0.7669, 0.8849, 0.8911, 0.8232, 0.9688, 0.8549, 0.8082, 0.7496, 0.8277,\n",
       "            0.7586, 0.6855, 0.4016, 0.5429, 0.9941, 0.9262, 0.8539, 0.6670, 0.8250,\n",
       "            0.9227, 0.9859, 0.8389, 0.8589, 0.6736, 0.8241, 0.9407, 0.7927, 1.0004,\n",
       "            0.8578, 0.6348, 0.8143, 0.7298, 0.7848, 0.8553, 0.9469, 0.6790, 0.8179,\n",
       "            0.5252, 0.9775, 0.9936, 0.6949, 0.5506, 0.7584, 0.8457, 0.7431, 0.6933,\n",
       "            0.6524, 0.7936, 0.8095, 0.8718, 0.8771, 0.8151, 0.9161, 0.9664, 0.9215,\n",
       "            0.9066, 0.8689, 0.8691, 0.7142, 1.0143, 0.7229, 0.7808, 0.9352, 0.7238,\n",
       "            0.7252, 0.9900, 0.9158, 0.8838, 0.8753, 0.7067, 0.6941, 0.8296, 0.9203,\n",
       "            0.8838, 0.8665, 0.8921, 0.8733, 0.6837, 0.8832, 0.7987, 1.0137, 0.7922,\n",
       "            0.9281, 0.8291, 0.8310, 0.6811, 0.7223, 0.8624, 0.9138, 0.9691, 0.6432,\n",
       "            0.8936, 0.8630, 0.8412], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([0.8072, 0.6468, 0.5716, 0.8520, 0.6570, 0.7497, 0.8473, 0.7353, 0.6973,\n",
       "            0.8622, 0.6099, 0.5541, 0.3763, 0.7937, 0.6159, 0.4147, 0.7497, 0.7781,\n",
       "            0.4900, 0.5275, 0.6320, 0.6841, 0.7640, 0.4501, 0.8047, 0.5723, 0.8925,\n",
       "            0.9286, 0.5627, 0.9245, 0.7688, 0.8937, 0.4376, 0.7584, 0.5834, 0.8311,\n",
       "            0.6639, 0.7334, 0.5399, 0.7034, 0.6957, 0.8235, 0.8613, 0.7178, 0.5855,\n",
       "            0.8685, 0.5177, 0.8394, 0.8693, 0.4468, 0.5938, 0.8664, 0.6158, 0.8058,\n",
       "            0.7882, 0.6252, 0.6344, 0.7773, 0.3613, 0.5145, 0.7381, 0.8798, 0.8846,\n",
       "            0.7164, 0.7800, 0.5415, 0.7822, 0.8540, 0.5422, 0.8077, 0.8681, 0.6000,\n",
       "            0.3891, 0.7183, 0.9244, 0.5676, 0.7931, 0.5493, 0.7652, 0.7546, 0.7152,\n",
       "            0.8507, 0.8001, 0.7657, 0.5063, 0.4513, 0.8561, 0.9336, 0.9340, 0.8336,\n",
       "            0.3202, 0.8379, 0.8353, 0.6885, 0.9528, 0.6761, 0.5881, 0.7004, 0.8194,\n",
       "            0.8495, 0.8641, 0.6243, 0.5855, 0.9600, 1.0825, 0.2584, 0.9297, 0.7064,\n",
       "            0.6389, 0.5702, 0.5724, 0.4076, 0.6429, 0.7702, 0.9014, 0.7435, 0.4403,\n",
       "            0.3969, 0.7878, 0.5623, 0.7930, 0.7769, 0.6858, 0.8284, 0.7799, 0.7347,\n",
       "            0.7269, 0.5883, 0.9680, 0.4629, 0.8245, 0.7374, 0.4427, 0.8451, 0.7209,\n",
       "            0.6802, 0.8470, 0.6297, 0.4502, 0.6928, 0.8163, 0.2600, 0.7620, 0.7339,\n",
       "            0.5469, 0.8961, 0.5116, 0.4331, 0.8762, 0.6486, 0.7132, 0.7026, 0.6038,\n",
       "            0.4236, 0.9003, 0.7372, 0.7729, 0.7347, 0.5747, 0.7247, 0.4244, 0.7822,\n",
       "            0.6214, 0.6953, 0.7557, 0.8352, 0.8196, 0.8347, 0.4862, 0.6493, 0.8523,\n",
       "            0.7523, 0.7108, 0.6966, 0.4644, 0.7264, 0.9191, 0.9299, 0.8119, 0.9357,\n",
       "            0.6351, 0.6709, 0.4997, 0.6001, 0.9368, 0.9848, 0.5403, 0.9444, 0.7573,\n",
       "            0.5802, 0.8678, 0.8701, 0.5317, 0.7680, 0.7636, 0.7905, 0.7250, 0.7056,\n",
       "            0.7901, 0.8564, 0.6775, 0.4026, 0.5569, 0.6442, 0.3114, 0.7577, 1.0005,\n",
       "            0.6295, 0.7958, 0.8052, 0.7064, 0.9254, 0.7517, 0.6855, 0.6067, 0.7130,\n",
       "            0.6187, 0.5258, 0.2337, 0.3656, 0.9678, 0.8584, 0.7501, 0.5036, 0.7090,\n",
       "            0.8538, 0.9540, 0.7286, 0.7578, 0.5116, 0.7078, 0.8810, 0.6643, 0.9785,\n",
       "            0.7563, 0.4657, 0.6940, 0.5811, 0.6538, 0.7523, 0.8905, 0.5179, 0.6991,\n",
       "            0.3476, 0.9395, 0.9668, 0.5373, 0.3735, 0.6182, 0.7384, 0.5985, 0.5354,\n",
       "            0.4862, 0.6659, 0.6873, 0.7765, 0.7839, 0.6954, 0.8430, 0.9214, 0.8508,\n",
       "            0.8288, 0.7721, 0.7726, 0.5614, 0.9994, 0.5725, 0.6482, 0.8724, 0.5734,\n",
       "            0.5752, 0.9604, 0.8423, 0.7943, 0.7816, 0.5519, 0.5363, 0.7155, 0.8490,\n",
       "            0.7941, 0.7688, 0.8064, 0.7789, 0.5235, 0.7932, 0.6727, 0.9986, 0.6639,\n",
       "            0.8616, 0.7151, 0.7175, 0.5204, 0.5716, 0.7625, 0.8392, 0.9259, 0.4754,\n",
       "            0.8086, 0.7636, 0.7319], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([0.8074, 0.6485, 0.5744, 0.8521, 0.6585, 0.7503, 0.8474, 0.7360, 0.6984,\n",
       "            0.8623, 0.6120, 0.5573, 0.3835, 0.7940, 0.6180, 0.4209, 0.7503, 0.7785,\n",
       "            0.4945, 0.5312, 0.6339, 0.6853, 0.7645, 0.4554, 0.8049, 0.5751, 0.8925,\n",
       "            0.9286, 0.5657, 0.9245, 0.7692, 0.8937, 0.4432, 0.7589, 0.5860, 0.8312,\n",
       "            0.6655, 0.7341, 0.5433, 0.7044, 0.6968, 0.8236, 0.8613, 0.7186, 0.5881,\n",
       "            0.8685, 0.5216, 0.8395, 0.8694, 0.4522, 0.5962, 0.8664, 0.6179, 0.8061,\n",
       "            0.7885, 0.6272, 0.6363, 0.7777, 0.3689, 0.5184, 0.7388, 0.8799, 0.8846,\n",
       "            0.7173, 0.7803, 0.5449, 0.7826, 0.8540, 0.5456, 0.8079, 0.8681, 0.6023,\n",
       "            0.3960, 0.7192, 0.9244, 0.5706, 0.7933, 0.5526, 0.7656, 0.7552, 0.7161,\n",
       "            0.8507, 0.8004, 0.7661, 0.5104, 0.4566, 0.8561, 0.9336, 0.9340, 0.8337,\n",
       "            0.3290, 0.8380, 0.8354, 0.6897, 0.9528, 0.6774, 0.5906, 0.7014, 0.8196,\n",
       "            0.8496, 0.8642, 0.6263, 0.5881, 0.9600, 1.0825, 0.2690, 0.9297, 0.7074,\n",
       "            0.6406, 0.5731, 0.5752, 0.4140, 0.6446, 0.7706, 0.9014, 0.7441, 0.4459,\n",
       "            0.4035, 0.7881, 0.5653, 0.7933, 0.7773, 0.6870, 0.8285, 0.7803, 0.7354,\n",
       "            0.7277, 0.5909, 0.9680, 0.4680, 0.8247, 0.7381, 0.4482, 0.8452, 0.7217,\n",
       "            0.6815, 0.8471, 0.6316, 0.4555, 0.6939, 0.8165, 0.2705, 0.7624, 0.7346,\n",
       "            0.5502, 0.8961, 0.5156, 0.4389, 0.8763, 0.6502, 0.7141, 0.7036, 0.6061,\n",
       "            0.4296, 0.9003, 0.7379, 0.7733, 0.7354, 0.5774, 0.7255, 0.4304, 0.7825,\n",
       "            0.6235, 0.6963, 0.7563, 0.8353, 0.8198, 0.8348, 0.4908, 0.6510, 0.8524,\n",
       "            0.7528, 0.7117, 0.6976, 0.4694, 0.7272, 0.9191, 0.9299, 0.8121, 0.9357,\n",
       "            0.6369, 0.6723, 0.5039, 0.6025, 0.9368, 0.9848, 0.5437, 0.9444, 0.7578,\n",
       "            0.5828, 0.8679, 0.8701, 0.5353, 0.7684, 0.7641, 0.7908, 0.7258, 0.7066,\n",
       "            0.7904, 0.8565, 0.6788, 0.4091, 0.5600, 0.6459, 0.3204, 0.7582, 1.0005,\n",
       "            0.6314, 0.7961, 0.8054, 0.7074, 0.9254, 0.7522, 0.6867, 0.6090, 0.7139,\n",
       "            0.6208, 0.5295, 0.2449, 0.3731, 0.9678, 0.8584, 0.7506, 0.5077, 0.7099,\n",
       "            0.8538, 0.9540, 0.7294, 0.7583, 0.5156, 0.7088, 0.8810, 0.6658, 0.9785,\n",
       "            0.7568, 0.4707, 0.6951, 0.5838, 0.6554, 0.7528, 0.8905, 0.5217, 0.7001,\n",
       "            0.3556, 0.9395, 0.9668, 0.5408, 0.3808, 0.6203, 0.7391, 0.6009, 0.5389,\n",
       "            0.4907, 0.6674, 0.6885, 0.7768, 0.7842, 0.6965, 0.8431, 0.9214, 0.8509,\n",
       "            0.8290, 0.7725, 0.7730, 0.5644, 0.9994, 0.5753, 0.6499, 0.8724, 0.5762,\n",
       "            0.5780, 0.9604, 0.8424, 0.7946, 0.7819, 0.5551, 0.5398, 0.7164, 0.8491,\n",
       "            0.7944, 0.7693, 0.8067, 0.7793, 0.5273, 0.7935, 0.6740, 0.9986, 0.6653,\n",
       "            0.8616, 0.7160, 0.7183, 0.5242, 0.5744, 0.7630, 0.8393, 0.9259, 0.4802,\n",
       "            0.8088, 0.7641, 0.7326], device='cuda:0')},\n",
       "   1747370804784: {'step': 1114,\n",
       "    'exp_avg': tensor([ 9.5754e-06, -1.0073e-02, -5.0701e-06,  7.4304e-02, -5.5535e-06,\n",
       "            -4.8596e-03, -3.7176e-05, -1.2885e-05, -8.8845e-04, -1.2212e-03,\n",
       "            -5.0410e-02, -1.6472e-05,  1.5446e-02, -1.0577e-03,  2.2169e-02,\n",
       "            -2.3350e-04,  1.3373e-02, -1.5918e-05,  3.1058e-02,  1.5092e-02,\n",
       "             1.6256e-05, -1.4437e-03,  6.5535e-02,  9.6611e-06,  5.2655e-03,\n",
       "             2.7742e-03,  2.9463e-02, -3.8374e-02, -1.6470e-03, -1.6814e-02,\n",
       "            -2.4737e-03, -4.9427e-02, -6.8592e-04, -3.2156e-02,  7.0594e-04,\n",
       "             5.5339e-05, -2.0356e-05,  3.4366e-03, -1.9754e-05, -3.9345e-06,\n",
       "             2.9753e-02, -2.6881e-05, -7.1022e-02, -1.5146e-02,  1.5040e-05,\n",
       "            -6.7455e-02,  1.5366e-06, -9.4467e-02,  1.4224e-03,  2.6452e-03,\n",
       "            -5.6963e-02, -2.9001e-04, -9.0356e-06, -6.4901e-02,  6.1173e-02,\n",
       "            -1.7579e-02,  2.2230e-02, -6.8072e-02, -1.0439e-03, -8.2941e-03,\n",
       "            -1.6228e-03, -1.2359e-02,  3.3120e-02, -5.6538e-02,  4.8134e-02,\n",
       "            -1.8491e-04,  9.2360e-06, -8.7611e-03, -3.6650e-06, -1.8502e-05,\n",
       "             8.0788e-06, -4.5836e-03,  4.8408e-06,  4.3325e-07,  6.4527e-02,\n",
       "            -5.4321e-05,  5.3675e-03, -6.8078e-03,  2.9830e-06,  1.6203e-02,\n",
       "             1.0436e-02, -9.8662e-06,  2.5636e-05,  1.2002e-01, -1.4102e-02,\n",
       "             1.5708e-04, -6.2175e-02,  5.1770e-02,  1.7288e-02,  1.2329e-05,\n",
       "            -2.3828e-06, -2.5709e-02, -3.2849e-05,  3.7981e-02,  3.0805e-02,\n",
       "             1.8942e-05,  4.3523e-02,  1.4395e-03,  1.0188e-04, -4.3502e-05,\n",
       "            -1.3236e-02,  9.0370e-06, -7.3366e-03,  4.8076e-02,  3.8639e-05,\n",
       "            -1.7273e-02,  4.2478e-05,  4.8107e-02,  1.4371e-02, -1.1076e-05,\n",
       "             1.0043e-02,  1.5779e-03, -7.1277e-05,  5.8648e-04, -1.2052e-02,\n",
       "             4.8459e-04, -5.4100e-02,  8.2048e-03,  3.8033e-06, -8.5183e-06,\n",
       "            -2.8948e-04, -8.1205e-06,  7.4109e-02,  5.5857e-03,  4.4646e-06,\n",
       "             1.0821e-06,  1.5999e-05,  1.7260e-06, -2.4722e-02,  8.1219e-03,\n",
       "            -3.9508e-02, -1.4174e-02,  3.5890e-02,  1.1394e-03, -1.5315e-05,\n",
       "             6.2356e-02,  1.1901e-02, -1.4460e-04, -1.0957e-02,  1.1864e-05,\n",
       "            -2.5492e-03, -4.4967e-03, -3.3710e-02,  2.0985e-05,  3.8051e-02,\n",
       "            -1.6147e-02, -2.1991e-02, -3.0467e-06,  1.4325e-02, -1.8333e-02,\n",
       "            -3.0956e-02, -2.1284e-02, -1.2252e-02, -3.7943e-03,  2.6434e-02,\n",
       "            -6.5548e-02, -3.8335e-02, -1.3673e-05, -1.6599e-02,  4.4099e-03,\n",
       "            -1.5647e-06,  9.1144e-03, -1.5429e-02,  8.4692e-06, -1.4211e-05,\n",
       "            -4.3609e-03,  1.7138e-02, -1.4370e-03,  1.2066e-01, -2.2648e-02,\n",
       "            -9.2859e-03, -1.4311e-05, -2.1237e-02,  1.2758e-02, -1.2020e-02,\n",
       "             6.3032e-03,  5.2621e-02,  6.4478e-02, -1.9249e-02,  5.4677e-02,\n",
       "             8.0815e-06, -1.6041e-05,  2.1665e-04,  3.5932e-02,  1.8636e-03,\n",
       "             1.6695e-02, -1.2122e-05, -3.5274e-02, -1.3716e-02,  7.5050e-02,\n",
       "            -3.0305e-02,  3.6685e-02, -7.4397e-03,  1.6180e-05, -1.0600e-03,\n",
       "            -9.3206e-02,  2.4474e-06, -1.0104e-05, -6.1294e-02, -5.7238e-02,\n",
       "             2.6163e-03, -7.4596e-03,  2.9866e-06,  6.5205e-05, -1.3347e-06,\n",
       "            -2.6328e-05, -1.0507e-01,  4.7339e-06,  2.4627e-02,  1.3064e-02,\n",
       "            -1.3795e-03,  1.9971e-02, -3.8640e-02,  6.4860e-02, -5.7373e-05,\n",
       "             1.5267e-02, -2.2013e-05, -3.9121e-06,  1.2780e-02,  5.7408e-02,\n",
       "             1.0135e-01, -1.4892e-03,  1.0985e-02,  1.7041e-05, -4.6765e-02,\n",
       "             3.5895e-05,  3.7847e-02,  3.6686e-02,  2.8151e-05, -1.8698e-02,\n",
       "            -2.7648e-02, -1.3133e-05, -3.4086e-02,  1.1713e-01,  2.0998e-05,\n",
       "            -4.0859e-02,  2.1498e-02, -9.2780e-06,  2.5811e-03,  1.1362e-02,\n",
       "             3.2831e-02, -6.8650e-02, -3.9320e-02,  4.2422e-02,  3.7649e-02,\n",
       "            -3.9203e-02, -4.9398e-02,  6.0578e-02, -2.5010e-03,  1.7070e-02,\n",
       "            -3.6136e-03,  3.8948e-06, -7.9646e-06,  2.1515e-04,  5.8696e-06,\n",
       "             4.4747e-02, -9.8124e-02,  4.4999e-03,  4.4302e-02, -1.1061e-03,\n",
       "             1.3667e-02,  1.6275e-05,  3.3298e-04, -7.6797e-06, -2.9323e-06,\n",
       "            -1.8233e-02, -2.9032e-05,  1.0424e-03,  3.6120e-02, -3.6673e-02,\n",
       "             1.0157e-02,  3.2244e-02,  4.8920e-03,  3.1375e-03, -1.0311e-01,\n",
       "            -1.2319e-02,  7.2945e-03, -9.3266e-03, -1.1179e-05,  4.3663e-02,\n",
       "            -6.8294e-03,  1.2081e-05,  4.6130e-05,  3.4340e-02,  1.2676e-05,\n",
       "             9.4810e-06,  3.4056e-03, -2.8110e-03,  4.9813e-02, -4.4539e-02,\n",
       "             1.1623e-02,  1.1064e-03, -2.2712e-02, -2.1960e-02,  2.7213e-05,\n",
       "             2.1774e-02, -8.5040e-04, -9.8768e-06, -1.4655e-02,  5.1139e-02],\n",
       "           device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([9.7165e-05, 6.9010e-03, 1.9267e-05, 3.1093e-02, 1.4515e-04, 4.6776e-03,\n",
       "            1.8599e-05, 2.7388e-05, 2.2758e-03, 2.5420e-03, 2.1770e-02, 9.5473e-06,\n",
       "            8.9684e-03, 2.4121e-03, 1.1436e-02, 1.4623e-03, 8.2077e-03, 1.6651e-06,\n",
       "            1.4672e-02, 8.8354e-03, 1.8686e-04, 2.7297e-03, 2.7594e-02, 3.6201e-04,\n",
       "            4.8438e-03, 3.5888e-03, 1.4093e-02, 1.7422e-02, 2.8765e-03, 9.4844e-03,\n",
       "            3.4198e-03, 2.1466e-02, 2.0695e-03, 1.5111e-02, 2.0597e-03, 9.9918e-04,\n",
       "            3.3676e-05, 3.9496e-03, 8.7071e-04, 3.7130e-04, 1.4207e-02, 7.1368e-05,\n",
       "            2.9731e-02, 8.8840e-03, 2.9245e-07, 2.8335e-02, 2.3920e-04, 3.9418e-02,\n",
       "            2.7390e-03, 3.5209e-03, 2.4252e-02, 1.6380e-03, 2.5747e-04, 2.7380e-02,\n",
       "            2.5876e-02, 9.7592e-03, 1.1503e-02, 2.8638e-02, 2.4172e-03, 6.1740e-03,\n",
       "            2.8345e-03, 7.8003e-03, 1.5479e-02, 2.4124e-02, 2.0955e-02, 1.3756e-03,\n",
       "            5.0699e-06, 6.4011e-03, 2.4280e-04, 1.0318e-07, 5.5535e-04, 4.5337e-03,\n",
       "            4.6066e-04, 4.3505e-05, 2.7328e-02, 1.0039e-03, 4.8974e-03, 5.5575e-03,\n",
       "            6.7622e-06, 9.2508e-03, 7.0517e-03, 7.3996e-04, 6.1525e-04, 5.0783e-02,\n",
       "            8.4601e-03, 1.2977e-03, 2.6366e-02, 2.2318e-02, 9.6357e-03, 6.7656e-05,\n",
       "            3.1369e-05, 1.2729e-02, 8.1657e-05, 1.7155e-02, 1.4622e-02, 1.4994e-04,\n",
       "            1.9219e-02, 2.7173e-03, 1.1733e-03, 2.3067e-04, 8.1308e-03, 9.7596e-06,\n",
       "            5.7907e-03, 2.0988e-02, 7.0728e-06, 9.6448e-03, 2.8355e-04, 2.0915e-02,\n",
       "            8.5712e-03, 7.8037e-05, 6.8885e-03, 2.8217e-03, 8.6128e-04, 1.9730e-03,\n",
       "            7.6879e-03, 1.7685e-03, 2.3167e-02, 6.1395e-03, 3.6233e-04, 5.7321e-04,\n",
       "            1.6140e-03, 3.9717e-04, 3.1019e-02, 5.0014e-03, 5.9786e-04, 8.6468e-04,\n",
       "            2.8115e-05, 2.3149e-04, 1.2385e-02, 6.1040e-03, 1.7728e-02, 8.4897e-03,\n",
       "            1.6403e-02, 2.5066e-03, 7.0303e-04, 2.6180e-02, 7.6426e-03, 1.2692e-03,\n",
       "            7.2472e-03, 7.2751e-04, 3.4948e-03, 4.4867e-03, 1.5645e-02, 7.8539e-04,\n",
       "            1.7200e-02, 9.2497e-03, 1.1368e-02, 2.5184e-04, 8.5498e-03, 1.0056e-02,\n",
       "            1.4636e-02, 1.1124e-02, 7.7651e-03, 4.1320e-03, 1.3000e-02, 2.7646e-02,\n",
       "            1.7296e-02, 7.2432e-04, 9.4201e-03, 4.4528e-03, 7.1046e-06, 6.5109e-03,\n",
       "            8.9444e-03, 5.9172e-04, 2.5420e-04, 4.4962e-03, 9.6456e-03, 2.6932e-03,\n",
       "            5.1165e-02, 1.1627e-02, 6.5876e-03, 2.3417e-04, 1.1093e-02, 7.9584e-03,\n",
       "            7.6629e-03, 5.3428e-03, 2.2664e-02, 2.7206e-02, 1.0403e-02, 2.3546e-02,\n",
       "            1.1171e-05, 4.3688e-04, 1.4337e-03, 1.6425e-02, 3.0609e-03, 9.5243e-03,\n",
       "            2.2494e-07, 1.6310e-02, 8.3319e-03, 3.1360e-02, 1.4402e-02, 1.6698e-02,\n",
       "            5.8138e-03, 3.1397e-04, 2.4677e-03, 3.8915e-02, 1.5143e-07, 4.2864e-04,\n",
       "            2.5927e-02, 2.4456e-02, 3.4969e-03, 5.8201e-03, 1.0057e-06, 1.0889e-03,\n",
       "            5.2911e-05, 7.8522e-08, 4.4503e-02, 4.9778e-04, 1.2350e-02, 8.0715e-03,\n",
       "            2.6646e-03, 1.0678e-02, 1.7443e-02, 2.7323e-02, 1.0438e-03, 8.9279e-03,\n",
       "            1.2354e-04, 3.1886e-04, 7.9586e-03, 2.4392e-02, 4.2528e-02, 2.8015e-03,\n",
       "            7.2632e-03, 1.7388e-05, 2.0427e-02, 9.6042e-08, 1.7200e-02, 1.6699e-02,\n",
       "            2.0707e-04, 1.0172e-02, 1.3432e-02, 9.3924e-07, 1.5755e-02, 4.9642e-02,\n",
       "            9.2965e-04, 1.8240e-02, 1.1203e-02, 5.7375e-05, 3.4871e-03, 7.4133e-03,\n",
       "            1.5315e-02, 2.8808e-02, 1.7668e-02, 1.8803e-02, 1.7133e-02, 1.7712e-02,\n",
       "            2.1386e-02, 2.5637e-02, 3.4239e-03, 9.5850e-03, 4.0573e-03, 3.8973e-05,\n",
       "            2.6768e-06, 1.4459e-03, 5.4057e-07, 1.9681e-02, 4.1004e-02, 4.5045e-03,\n",
       "            1.9539e-02, 2.4755e-03, 8.2564e-03, 3.3525e-05, 1.6430e-03, 2.2487e-05,\n",
       "            1.6076e-05, 1.0010e-02, 5.7108e-04, 2.4196e-03, 1.6463e-02, 1.6680e-02,\n",
       "            6.9313e-03, 1.5155e-02, 4.6682e-03, 3.7918e-03, 4.3202e-02, 7.7794e-03,\n",
       "            5.7639e-03, 6.5865e-03, 3.5354e-04, 1.9303e-02, 5.5865e-03, 8.6313e-04,\n",
       "            3.3941e-04, 1.5850e-02, 4.3278e-06, 3.4763e-07, 3.9325e-03, 3.5789e-03,\n",
       "            2.1616e-02, 1.9569e-02, 7.5054e-03, 2.4630e-03, 1.1642e-02, 1.1363e-02,\n",
       "            4.9071e-04, 1.1308e-02, 2.2318e-03, 8.9263e-06, 8.6729e-03, 2.2050e-02],\n",
       "           device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([2.4659e-04, 1.1705e-02, 5.2571e-05, 4.1659e-02, 3.6015e-04, 8.3846e-03,\n",
       "            5.0776e-05, 7.3728e-05, 4.4672e-03, 4.9249e-03, 3.0863e-02, 2.6652e-05,\n",
       "            1.4625e-02, 4.7029e-03, 1.7962e-02, 3.0140e-03, 1.3570e-02, 4.7753e-06,\n",
       "            2.2158e-02, 1.4443e-02, 4.5640e-04, 5.2437e-03, 3.7668e-02, 8.4482e-04,\n",
       "            8.6387e-03, 6.6618e-03, 2.1420e-02, 2.5610e-02, 5.4899e-03, 1.5338e-02,\n",
       "            6.3868e-03, 3.0503e-02, 4.1071e-03, 2.2711e-02, 4.0896e-03, 2.1393e-03,\n",
       "            8.9778e-05, 7.2409e-03, 1.8889e-03, 8.6489e-04, 2.1568e-02, 1.8407e-04,\n",
       "            4.0102e-02, 1.4510e-02, 8.0344e-07, 3.8523e-02, 5.7499e-04, 5.0886e-02,\n",
       "            5.2604e-03, 6.5525e-03, 3.3790e-02, 3.3357e-03, 6.1589e-04, 3.7442e-02,\n",
       "            3.5686e-02, 1.5715e-02, 1.8056e-02, 3.8877e-02, 4.7113e-03, 1.0644e-02,\n",
       "            5.4198e-03, 1.2993e-02, 2.3182e-02, 3.3646e-02, 2.9896e-02, 2.8534e-03,\n",
       "            1.4308e-05, 1.0981e-02, 5.8318e-04, 1.1187e-07, 1.2531e-03, 8.1599e-03,\n",
       "            1.0550e-03, 1.1494e-04, 3.7391e-02, 2.1487e-03, 8.7203e-03, 9.7243e-03,\n",
       "            1.8971e-05, 1.5019e-02, 1.1922e-02, 1.6292e-03, 1.3764e-03, 6.3146e-02,\n",
       "            1.3919e-02, 2.7081e-03, 3.6258e-02, 3.1516e-02, 1.5537e-02, 1.7493e-04,\n",
       "            8.4129e-05, 1.9662e-02, 2.0925e-04, 2.5260e-02, 2.2097e-02, 3.7126e-04,\n",
       "            2.7793e-02, 5.2234e-03, 2.4734e-03, 5.5569e-04, 1.3460e-02, 2.7194e-05,\n",
       "            1.0076e-02, 2.9930e-02, 1.9748e-05, 1.5554e-02, 6.7373e-04, 2.9843e-02,\n",
       "            1.4077e-02, 2.0049e-04, 1.1687e-02, 5.3987e-03, 1.8703e-03, 3.9365e-03,\n",
       "            1.2832e-02, 3.5722e-03, 3.2516e-02, 1.0592e-02, 8.4545e-04, 1.2904e-03,\n",
       "            3.2914e-03, 9.2080e-04, 4.1578e-02, 8.8805e-03, 1.3410e-03, 1.8771e-03,\n",
       "            7.5583e-05, 5.5798e-04, 1.9207e-02, 1.0539e-02, 2.5969e-02, 1.3961e-02,\n",
       "            2.4331e-02, 4.8636e-03, 1.5549e-03, 3.6003e-02, 1.2772e-02, 2.6548e-03,\n",
       "            1.2204e-02, 1.6040e-03, 6.5077e-03, 8.0863e-03, 2.3399e-02, 1.7196e-03,\n",
       "            2.5321e-02, 1.5018e-02, 1.7873e-02, 6.0334e-04, 1.4041e-02, 1.6118e-02,\n",
       "            2.2114e-02, 1.7547e-02, 1.2942e-02, 7.5298e-03, 2.0013e-02, 3.7740e-02,\n",
       "            2.5442e-02, 1.5975e-03, 1.5248e-02, 8.0330e-03, 2.0027e-05, 1.1136e-02,\n",
       "            1.4588e-02, 1.3283e-03, 6.0847e-04, 8.1027e-03, 1.5561e-02, 5.1824e-03,\n",
       "            6.3607e-02, 1.8218e-02, 1.1250e-02, 5.6389e-04, 1.7507e-02, 1.3216e-02,\n",
       "            1.2798e-02, 9.4008e-03, 3.1936e-02, 3.7223e-02, 1.6582e-02, 3.2984e-02,\n",
       "            3.1009e-05, 1.0048e-03, 2.9616e-03, 2.4360e-02, 5.7999e-03, 1.5396e-02,\n",
       "            6.1808e-07, 2.4232e-02, 1.3740e-02, 4.1951e-02, 2.1814e-02, 2.4697e-02,\n",
       "            1.0108e-02, 7.4049e-04, 4.7993e-03, 5.0362e-02, 3.8864e-07, 9.8771e-04,\n",
       "            3.5742e-02, 3.4043e-02, 6.5132e-03, 1.0119e-02, 2.9051e-06, 2.3122e-03,\n",
       "            1.3849e-04, 8.0699e-08, 5.6555e-02, 1.1334e-03, 1.9164e-02, 1.3376e-02,\n",
       "            5.1333e-03, 1.6956e-02, 2.5622e-02, 3.7359e-02, 2.2264e-03, 1.4574e-02,\n",
       "            3.0946e-04, 7.5114e-04, 1.3215e-02, 3.3943e-02, 5.4311e-02, 5.3661e-03,\n",
       "            1.2228e-02, 4.7552e-05, 2.9255e-02, 1.0384e-07, 2.5330e-02, 2.4702e-02,\n",
       "            5.0256e-04, 1.6272e-02, 2.0566e-02, 2.6620e-06, 2.3521e-02, 6.1976e-02,\n",
       "            2.0049e-03, 2.6603e-02, 1.7654e-02, 1.4960e-04, 6.4977e-03, 1.2446e-02,\n",
       "            2.2974e-02, 3.9058e-02, 2.5899e-02, 2.7288e-02, 2.5240e-02, 2.5961e-02,\n",
       "            3.0402e-02, 3.5402e-02, 6.3947e-03, 1.5475e-02, 7.4122e-03, 1.0341e-04,\n",
       "            7.6946e-06, 2.9838e-03, 1.5256e-06, 2.8351e-02, 5.2657e-02, 8.1146e-03,\n",
       "            2.8181e-02, 4.8117e-03, 1.3637e-02, 8.9455e-05, 3.3453e-03, 6.0867e-05,\n",
       "            4.4138e-05, 1.6052e-02, 1.2856e-03, 4.7155e-03, 2.4395e-02, 2.4679e-02,\n",
       "            1.1748e-02, 2.2776e-02, 8.3679e-03, 6.9896e-03, 5.5040e-02, 1.2965e-02,\n",
       "            1.0034e-02, 1.1248e-02, 8.2666e-04, 2.7898e-02, 9.7691e-03, 1.8737e-03,\n",
       "            7.9611e-04, 2.3639e-02, 1.2258e-05, 9.0604e-07, 7.2160e-03, 6.6455e-03,\n",
       "            3.0686e-02, 2.8216e-02, 1.2573e-02, 4.7903e-03, 1.8237e-02, 1.7864e-02,\n",
       "            1.1184e-03, 1.7801e-02, 4.3905e-03, 2.4887e-05, 1.4216e-02, 3.1195e-02],\n",
       "           device='cuda:0')},\n",
       "   1747370806656: {'step': 1114,\n",
       "    'exp_avg': tensor([[-1.6819e-04, -1.0575e-04,  7.4565e-06,  ..., -4.8063e-06,\n",
       "             -8.3631e-06, -3.3945e-05],\n",
       "            [-9.0033e-05,  1.3156e-05,  1.2058e-06,  ...,  8.2461e-06,\n",
       "              8.7152e-06,  3.9832e-05],\n",
       "            [-5.1858e-05, -4.3107e-07, -6.4149e-06,  ...,  1.3294e-05,\n",
       "             -3.7102e-05,  1.0109e-05],\n",
       "            ...,\n",
       "            [-6.5485e-05,  7.2873e-07, -3.4176e-06,  ...,  2.9853e-06,\n",
       "              1.1461e-05,  9.1174e-06],\n",
       "            [-8.3108e-05,  1.6485e-06, -1.8946e-06,  ...,  8.0103e-06,\n",
       "              7.7087e-06,  8.8106e-06],\n",
       "            [-9.7495e-05,  4.4884e-06, -5.6626e-06,  ..., -2.6848e-05,\n",
       "              1.1469e-05,  7.7824e-06]], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([[1.2897e-03, 1.2025e-03, 1.9413e-06,  ..., 1.4949e-04, 7.7834e-05,\n",
       "             1.6501e-04],\n",
       "            [8.9668e-08, 2.7239e-05, 4.8045e-06,  ..., 3.3493e-06, 1.4460e-05,\n",
       "             2.8921e-08],\n",
       "            [4.6463e-06, 1.6080e-06, 2.0863e-06,  ..., 5.2604e-08, 5.1852e-08,\n",
       "             5.8666e-05],\n",
       "            ...,\n",
       "            [1.0323e-05, 2.7228e-06, 2.8894e-06,  ..., 3.1871e-05, 2.7658e-06,\n",
       "             1.6937e-05],\n",
       "            [6.1213e-08, 1.9152e-07, 3.3813e-06,  ..., 3.6393e-06, 7.2384e-07,\n",
       "             1.2605e-05],\n",
       "            [1.3672e-07, 5.2405e-06, 2.2508e-06,  ..., 2.2921e-08, 2.2705e-06,\n",
       "             5.5964e-06]], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([[2.6941e-03, 2.5271e-03, 5.5928e-06,  ..., 3.7040e-04, 1.9995e-04,\n",
       "             4.0620e-04],\n",
       "            [1.0859e-07, 7.3406e-05, 1.3658e-05,  ..., 9.6032e-06, 3.9865e-05,\n",
       "             5.3410e-08],\n",
       "            [1.3152e-05, 4.6654e-06, 6.0217e-06,  ..., 1.2338e-07, 1.1238e-07,\n",
       "             1.5283e-04],\n",
       "            ...,\n",
       "            [2.8685e-05, 7.8398e-06, 8.2981e-06,  ..., 8.5315e-05, 7.9633e-06,\n",
       "             4.6436e-05],\n",
       "            [6.1455e-08, 5.3739e-07, 9.6851e-06,  ..., 1.0418e-05, 2.1013e-06,\n",
       "             3.4903e-05],\n",
       "            [2.5924e-07, 1.4880e-05, 6.4865e-06,  ..., 2.5007e-08, 6.5585e-06,\n",
       "             1.5861e-05]], device='cuda:0')},\n",
       "   1747370804712: {'step': 1114,\n",
       "    'exp_avg': tensor([-2.5445e-04,  7.2612e-06,  8.3681e-06,  7.2257e-06,  5.6284e-06,\n",
       "             1.2841e-05,  7.1433e-06,  1.3516e-05,  7.6384e-06,  1.1573e-05,\n",
       "             6.3231e-06,  1.4790e-05,  7.9450e-06,  1.1466e-05,  9.9143e-06,\n",
       "             4.0577e-06,  1.4343e-05,  2.3221e-05, -7.7355e-06, -2.0989e-05,\n",
       "            -3.0366e-06, -3.8329e-05, -3.5513e-05,  3.9950e-05,  8.2026e-05,\n",
       "             6.1067e-05,  7.3673e-05,  6.0881e-05,  6.0121e-05,  4.1677e-05,\n",
       "             1.5559e-04, -3.1517e-05, -2.7866e-05, -3.1911e-06,  3.2429e-05,\n",
       "             7.1235e-05,  2.2631e-05,  1.6664e-04, -2.5055e-06,  5.9862e-05,\n",
       "             3.3444e-05, -2.8629e-05,  7.9312e-05,  8.5773e-06,  2.0404e-05,\n",
       "             4.5130e-05, -3.8196e-05,  1.8436e-05,  3.7964e-05,  5.1422e-05,\n",
       "            -4.5361e-05,  5.8767e-05, -5.4583e-05, -2.5917e-06, -5.7783e-06,\n",
       "             6.4560e-05,  3.8068e-05,  3.3519e-05, -4.2933e-05, -1.2797e-05,\n",
       "            -5.3350e-05,  3.1127e-05, -6.5964e-05,  6.8473e-05, -8.0712e-05,\n",
       "            -1.2291e-04,  2.8786e-05, -8.4723e-05, -6.3989e-06, -4.8354e-05,\n",
       "            -6.3489e-05, -2.9215e-05,  4.9731e-05,  1.8396e-05, -9.9400e-06,\n",
       "             1.3752e-05,  6.0023e-05, -2.4606e-05, -2.0460e-05, -4.1437e-05,\n",
       "             3.9866e-05, -9.0548e-05, -1.8530e-05,  1.7484e-05, -2.7353e-05,\n",
       "             2.5527e-05, -1.1056e-04,  2.7544e-05, -8.6779e-05, -1.2510e-04,\n",
       "             2.4562e-05, -4.0736e-05,  1.4242e-04,  1.0771e-04,  4.9614e-05,\n",
       "             1.6950e-05,  8.8909e-05,  6.7103e-05,  8.6779e-05,  8.7833e-05,\n",
       "             1.0831e-04,  2.7797e-05,  4.0627e-05,  1.3785e-05,  1.3940e-05,\n",
       "            -1.2707e-06,  4.0477e-06,  1.8278e-05,  1.4052e-05,  1.5619e-05,\n",
       "             6.3668e-06,  1.2629e-05,  5.3610e-06,  1.1988e-05,  1.6134e-05,\n",
       "             1.1093e-05,  1.8890e-05,  5.7061e-06,  6.8172e-06,  1.0048e-05,\n",
       "             1.2251e-05,  1.4482e-05,  7.1438e-06,  1.0987e-05,  6.1825e-06,\n",
       "             7.0472e-06,  7.7132e-06,  1.4327e-05,  1.0170e-05], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([5.7108e-05, 4.2104e-06, 9.4596e-06, 3.5474e-06, 1.2426e-06, 1.9213e-05,\n",
       "            6.8341e-06, 2.3917e-05, 7.1774e-06, 1.6939e-05, 6.7595e-06, 2.1866e-05,\n",
       "            9.0811e-07, 1.6052e-05, 9.6272e-06, 2.1695e-06, 2.5770e-05, 3.4792e-05,\n",
       "            1.5162e-06, 1.0807e-05, 2.8368e-05, 4.9904e-05, 1.9752e-04, 1.0271e-04,\n",
       "            3.1536e-04, 1.8092e-04, 2.1739e-04, 1.8887e-04, 2.5711e-04, 3.2545e-04,\n",
       "            3.2502e-04, 6.0753e-04, 1.9782e-04, 1.7838e-04, 2.7631e-04, 5.6704e-04,\n",
       "            6.2253e-04, 1.1677e-04, 2.2681e-04, 3.7411e-04, 2.8953e-04, 7.4057e-04,\n",
       "            2.6298e-04, 4.6762e-04, 2.5340e-04, 4.6666e-04, 7.1380e-04, 4.2032e-04,\n",
       "            4.3356e-04, 8.3298e-04, 4.9147e-04, 8.2216e-04, 4.0429e-04, 4.5485e-04,\n",
       "            3.9978e-04, 6.8752e-04, 7.0279e-04, 4.8325e-04, 3.7636e-04, 3.5650e-04,\n",
       "            6.9986e-04, 1.0481e-04, 8.1050e-04, 4.4367e-04, 8.2077e-04, 3.6335e-04,\n",
       "            2.9290e-04, 8.5192e-04, 2.9854e-04, 6.7894e-04, 2.1687e-04, 6.2572e-04,\n",
       "            3.9696e-04, 6.1521e-04, 4.9794e-04, 2.6363e-04, 2.3533e-04, 5.8789e-04,\n",
       "            7.8130e-04, 4.8423e-04, 3.2159e-04, 2.2696e-04, 1.9934e-04, 5.4201e-04,\n",
       "            7.2325e-04, 7.0844e-04, 2.5174e-04, 8.0764e-04, 1.0077e-03, 1.2660e-03,\n",
       "            3.3835e-04, 7.4392e-04, 4.6388e-04, 1.0265e-03, 1.0854e-04, 1.1323e-04,\n",
       "            4.2744e-04, 1.1278e-04, 1.0998e-04, 1.2856e-04, 1.7964e-04, 1.9733e-05,\n",
       "            1.4059e-04, 2.3097e-06, 3.0732e-05, 9.6967e-07, 1.8917e-05, 4.0027e-05,\n",
       "            1.9900e-05, 2.9761e-05, 1.7143e-06, 1.8140e-05, 4.4432e-06, 1.5517e-05,\n",
       "            3.2512e-05, 1.3984e-05, 4.1612e-05, 3.0112e-06, 4.4416e-06, 8.0582e-06,\n",
       "            2.1350e-05, 2.5499e-05, 4.6147e-06, 1.1956e-05, 5.4270e-06, 2.3893e-06,\n",
       "            6.5325e-06, 2.1377e-05, 1.1698e-05], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([1.4910e-04, 1.2031e-05, 2.6446e-05, 1.0177e-05, 3.6263e-06, 5.2465e-05,\n",
       "            1.9286e-05, 6.4788e-05, 2.0228e-05, 4.6459e-05, 1.9082e-05, 5.9426e-05,\n",
       "            2.6554e-06, 4.4110e-05, 2.6900e-05, 6.2835e-06, 6.9599e-05, 9.2811e-05,\n",
       "            4.4128e-06, 3.0082e-05, 7.6295e-05, 1.3099e-04, 4.8095e-04, 2.6000e-04,\n",
       "            7.4353e-04, 4.4287e-04, 5.2593e-04, 4.6108e-04, 6.1501e-04, 7.6556e-04,\n",
       "            7.6445e-04, 1.3608e-03, 4.8140e-04, 4.3698e-04, 6.5753e-04, 1.2774e-03,\n",
       "            1.3911e-03, 2.9331e-04, 5.4705e-04, 8.7090e-04, 6.8674e-04, 1.6302e-03,\n",
       "            6.2800e-04, 1.0699e-03, 6.0663e-04, 1.0679e-03, 1.5763e-03, 9.6980e-04,\n",
       "            9.9795e-04, 1.8143e-03, 1.1201e-03, 1.7928e-03, 9.3552e-04, 1.0430e-03,\n",
       "            9.2591e-04, 1.5232e-03, 1.5540e-03, 1.1027e-03, 8.7572e-04, 8.3278e-04,\n",
       "            1.5480e-03, 2.6480e-04, 1.7697e-03, 1.0193e-03, 1.7901e-03, 8.4765e-04,\n",
       "            6.9406e-04, 1.8517e-03, 7.0646e-04, 1.5059e-03, 5.2453e-04, 1.3977e-03,\n",
       "            9.1988e-04, 1.3761e-03, 1.1335e-03, 6.2936e-04, 5.6611e-04, 1.3201e-03,\n",
       "            1.7115e-03, 1.1048e-03, 7.5700e-04, 5.4735e-04, 4.8478e-04, 1.2254e-03,\n",
       "            1.5955e-03, 1.5655e-03, 6.0282e-04, 1.7639e-03, 2.1557e-03, 2.6486e-03,\n",
       "            7.9364e-04, 1.6369e-03, 1.0621e-03, 2.1924e-03, 2.7383e-04, 2.8501e-04,\n",
       "            9.8503e-04, 2.8400e-04, 2.7733e-04, 3.2131e-04, 4.3998e-04, 5.3826e-05,\n",
       "            3.4955e-04, 6.6813e-06, 8.2411e-05, 2.8341e-06, 5.1682e-05, 1.0615e-04,\n",
       "            5.4274e-05, 7.9911e-05, 4.9843e-06, 4.9635e-05, 1.2680e-05, 4.2695e-05,\n",
       "            8.6984e-05, 3.8610e-05, 1.1018e-04, 8.6688e-06, 1.2675e-05, 2.2636e-05,\n",
       "            5.8073e-05, 6.8902e-05, 1.3158e-05, 3.3186e-05, 1.5410e-05, 6.9096e-06,\n",
       "            1.8458e-05, 5.8143e-05, 3.2489e-05], device='cuda:0')}},\n",
       "  'param_groups': [{'lr': 0.001,\n",
       "    'betas': (0.9, 0.999),\n",
       "    'eps': 1e-08,\n",
       "    'weight_decay': 0.5,\n",
       "    'amsgrad': True,\n",
       "    'params': [1747370804064,\n",
       "     1747370807160,\n",
       "     1747370638072,\n",
       "     1747370804136,\n",
       "     1747370804640,\n",
       "     1747370806296,\n",
       "     1747370804352,\n",
       "     1747370803992,\n",
       "     1747370804280,\n",
       "     1747370804496,\n",
       "     1747370803776,\n",
       "     1747370803416,\n",
       "     1747370803848,\n",
       "     1747370803560,\n",
       "     1747370803344,\n",
       "     1747370803488,\n",
       "     1747370807088,\n",
       "     1747370805792,\n",
       "     1747370806800,\n",
       "     1747370804784,\n",
       "     1747370806656,\n",
       "     1747370804712]}]}}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_lstm.cpu()\n",
    "state = {'epoch': epoch, 'loss': loss.item(), 'recall': acc, 'acc': overall_acc, 'state_dict': my_lstm.state_dict(), 'opt_dict': optimizer.state_dict()}\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(state, './3temp_checkpoint.pth')\n",
    "# torch.save(state, './temp_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 5626,\n",
       " 'loss': 3.8841054439544678,\n",
       " 'recall': 0.6553428173065186,\n",
       " 'acc': 0.8917142748832703,\n",
       " 'state_dict': OrderedDict([('layer_norm_0.weight',\n",
       "               tensor([0.9375, 0.9203, 0.9186, 0.9182, 0.9192, 0.9201, 0.9201, 0.9200, 0.9200,\n",
       "                       0.9204, 0.9193, 0.9204, 0.9195, 0.9199, 0.9189, 0.9182, 0.9199, 0.9186,\n",
       "                       0.9196, 1.1785, 1.1666, 1.2874, 1.3082, 1.3089, 1.3274, 1.3575, 1.2807,\n",
       "                       1.2638, 1.2654, 1.2173, 1.2633, 1.1765, 1.2044, 1.1808, 1.1849, 1.1441,\n",
       "                       1.1773, 1.1537, 1.1165, 1.1004, 1.1180, 1.0843, 1.1471, 1.0708, 1.0931,\n",
       "                       1.1016, 1.0855, 1.0999, 1.0873, 1.0636, 1.0706, 1.1018, 1.1241, 1.0901,\n",
       "                       1.1227, 1.1155, 1.1073, 1.1444, 1.0963, 1.0567, 1.0885, 1.1025, 1.1120,\n",
       "                       1.0671, 1.0897, 1.0905, 1.0718, 1.0613, 1.0537, 1.0718, 1.0800, 1.0522,\n",
       "                       1.0461, 1.0594, 1.0341, 1.0985, 1.0550, 1.0504, 1.0569, 1.0808, 1.0950,\n",
       "                       1.0884, 1.0762, 1.1283, 1.1133, 1.1263, 1.1209, 1.1586, 1.1475, 1.1714,\n",
       "                       1.1485, 1.1812, 1.1628, 1.1865, 1.2066, 1.2253, 1.2277, 1.3039, 1.2708,\n",
       "                       1.2644, 1.2922, 1.2080, 1.1163, 1.1080, 1.0228, 0.9904, 0.9464, 0.9424,\n",
       "                       0.9497, 0.9203, 0.9205, 0.9196, 0.9207, 0.9186, 0.9197, 0.9205, 0.9191,\n",
       "                       0.9196, 0.9227, 0.9202, 0.9188, 0.9201, 0.9202, 0.9199, 0.9198, 0.9217,\n",
       "                       0.9191, 0.9201, 0.9195], device='cuda:0')),\n",
       "              ('layer_norm_0.bias',\n",
       "               tensor([-0.3404,  0.0797,  0.0815,  0.0818,  0.0809,  0.0799,  0.0799,  0.0800,\n",
       "                        0.0800,  0.0796,  0.0808,  0.0796,  0.0806,  0.0801,  0.0812,  0.0819,\n",
       "                        0.0801,  0.0814,  0.0804,  0.0907,  0.0865,  0.0985,  0.0918,  0.0975,\n",
       "                        0.0938,  0.0990,  0.0896,  0.0830,  0.0789,  0.0718,  0.0709,  0.0772,\n",
       "                        0.0595,  0.0663,  0.0603,  0.0529,  0.0544,  0.0465,  0.0318,  0.0456,\n",
       "                        0.0258,  0.0197,  0.0272,  0.0523,  0.0496,  0.0385,  0.0554,  0.0594,\n",
       "                        0.0321,  0.0397,  0.0200,  0.0432,  0.0334,  0.0387,  0.0309,  0.0524,\n",
       "                        0.0392,  0.0486,  0.0533,  0.0419,  0.0494,  0.0550,  0.0269,  0.0490,\n",
       "                        0.0425,  0.0518,  0.0471,  0.0524,  0.0602,  0.0479,  0.0435,  0.0666,\n",
       "                        0.0332,  0.0549,  0.0475,  0.0363,  0.0520,  0.0585,  0.0460,  0.0672,\n",
       "                        0.0797,  0.0753,  0.0554,  0.0728,  0.0772,  0.0804,  0.0800,  0.0784,\n",
       "                        0.0862,  0.0833,  0.0866,  0.0758,  0.0771,  0.0891,  0.0982,  0.0966,\n",
       "                        0.0975,  0.0978,  0.0998,  0.0844,  0.1039,  0.0985,  0.0889,  0.0918,\n",
       "                        0.0869,  0.0862,  0.0834,  0.0817,  0.0832,  0.0798,  0.0795,  0.0804,\n",
       "                        0.0794,  0.0814,  0.0804,  0.0796,  0.0809,  0.0804,  0.0774,  0.0799,\n",
       "                        0.0812,  0.0799,  0.0799,  0.0802,  0.0802,  0.0784,  0.0810,  0.0800,\n",
       "                        0.0806], device='cuda:0')),\n",
       "              ('lstm_1.weight_ih_l0',\n",
       "               tensor([[-0.0356, -0.0688, -0.0797,  ..., -0.0961, -0.0247, -0.1337],\n",
       "                       [-0.0026,  0.0470,  0.0655,  ...,  0.0393,  0.0355,  0.0587],\n",
       "                       [-0.0400,  0.0874,  0.0110,  ...,  0.0383,  0.0334,  0.0663],\n",
       "                       ...,\n",
       "                       [-0.0625,  0.0451, -0.0931,  ..., -0.0503, -0.0366,  0.0341],\n",
       "                       [ 0.0876, -0.0277,  0.0244,  ..., -0.0066, -0.0369,  0.0397],\n",
       "                       [ 0.1070, -0.0289,  0.0805,  ...,  0.0088,  0.0612,  0.0412]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_1.weight_hh_l0',\n",
       "               tensor([[ 0.1123, -0.2516, -0.1555,  ..., -0.1015,  0.0280, -0.0161],\n",
       "                       [-0.1132, -0.1591,  0.1509,  ..., -0.1026, -0.2198,  0.0183],\n",
       "                       [ 0.0998,  0.1191, -0.1102,  ...,  0.0883, -0.0694,  0.1486],\n",
       "                       ...,\n",
       "                       [-0.1763,  0.0005, -0.0389,  ..., -0.0535, -0.0190, -0.0099],\n",
       "                       [-0.1211, -0.1549,  0.0922,  ...,  0.0899, -0.0365,  0.0392],\n",
       "                       [-0.1099, -0.0133,  0.0756,  ...,  0.2075,  0.0687, -0.1306]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_1.bias_ih_l0',\n",
       "               tensor([ 0.0893, -0.0956,  0.0003,  ...,  0.0232,  0.0548, -0.0093], device='cuda:0')),\n",
       "              ('lstm_1.bias_hh_l0',\n",
       "               tensor([ 0.0628, -0.0687,  0.0146,  ...,  0.0641,  0.0525, -0.0781], device='cuda:0')),\n",
       "              ('layer_norm_1.weight',\n",
       "               tensor([0.8854, 0.8465, 0.8752, 0.9663, 0.8333, 0.8694, 0.9145, 0.8755, 0.9334,\n",
       "                       0.8335, 0.8412, 0.8678, 0.9078, 0.9432, 0.9289, 0.9072, 0.8858, 0.8970,\n",
       "                       0.8407, 0.8705, 0.9199, 0.8755, 0.8921, 0.8582, 0.9038, 0.8900, 0.8645,\n",
       "                       0.8910, 0.8520, 0.8631, 0.8996, 0.8689, 0.8583, 0.9709, 0.8577, 0.9290,\n",
       "                       0.9593, 0.8728, 0.8881, 0.9922, 0.9044, 0.9108, 0.8438, 0.9248, 0.8937,\n",
       "                       0.9136, 0.8332, 0.8520, 0.8495, 0.8937, 0.8577, 0.9066, 0.9519, 0.8675,\n",
       "                       0.8879, 0.8715, 0.8553, 0.9088, 0.8668, 0.8589, 0.9056, 0.8614, 0.9252,\n",
       "                       0.9059, 0.8515, 0.9513, 0.9549, 0.8503, 0.9952, 0.8795, 0.8986, 0.8477,\n",
       "                       0.9735, 0.8599, 0.9157, 0.9243, 0.8965, 0.9792, 0.8595, 0.8885, 0.9055,\n",
       "                       0.9401, 0.8895, 0.9208, 0.9126, 0.8399, 0.8600, 0.8927, 0.9128, 0.8686,\n",
       "                       0.8872, 0.8490, 0.9069, 0.9053, 0.9636, 0.9341, 0.9262, 0.9477, 0.9695,\n",
       "                       0.9329, 0.8954, 0.8595, 0.8759, 0.8912, 0.9115, 0.9204, 0.8922, 0.9430,\n",
       "                       0.9512, 0.9655, 0.8512, 0.8952, 0.8643, 0.8487, 0.9275, 0.9302, 0.8585,\n",
       "                       0.8379, 0.9100, 1.0635, 0.9279, 0.9303, 0.8637, 0.9281, 0.9193, 0.9794,\n",
       "                       0.9665, 0.9200, 0.8847, 0.9157, 0.9080, 0.8897, 0.8798, 0.8933, 0.9488,\n",
       "                       0.8533, 0.8848, 0.9170, 0.9167, 0.8955, 0.8512, 0.9005, 0.8782, 0.8691,\n",
       "                       0.8503, 0.8944, 0.8924, 0.9684, 0.7977, 0.9867, 0.9186, 0.9374, 0.9127,\n",
       "                       0.9274, 0.9458, 0.9131, 0.9386, 0.8579, 0.8814, 0.8644, 0.9548, 0.9016,\n",
       "                       0.8906, 0.8587, 0.8947, 0.9444, 0.8949, 0.8008, 0.9124, 0.9163, 0.9084,\n",
       "                       0.8458, 0.8930, 0.7865, 0.9163, 0.8518, 1.0032, 0.9573, 0.8361, 1.0170,\n",
       "                       0.8850, 0.8873, 0.8810, 0.8948, 0.9688, 0.9220, 0.8996, 0.9458, 0.8850,\n",
       "                       0.9173, 0.9352, 0.9939, 0.8904, 0.9156, 0.9213, 0.8401, 0.8884, 0.9424,\n",
       "                       0.9428, 0.9794, 0.9164, 0.8903, 0.8830, 0.9240, 0.9347, 0.8834, 0.8761,\n",
       "                       0.8329, 0.9234, 0.9481, 0.9580, 0.8833, 0.9380, 0.8930, 0.9008, 0.8377,\n",
       "                       0.9055, 0.8743, 0.8789, 0.9857, 0.8441, 0.9002, 0.9046, 0.8628, 0.9130,\n",
       "                       0.8855, 0.8486, 0.8417, 0.9488, 0.9530, 0.9815, 0.8915, 0.8411, 0.9088,\n",
       "                       0.9640, 0.8762, 0.8973, 0.9787, 0.9147, 0.8835, 0.8513, 0.8603, 0.9875,\n",
       "                       0.8180, 0.8894, 0.8806, 0.9119, 0.8015, 0.9233, 0.9227, 0.8894, 0.8318,\n",
       "                       0.8694, 0.9399, 0.9383, 0.9025, 0.8676, 0.9147, 0.9301, 0.8428, 1.0029,\n",
       "                       0.8089, 0.8916, 0.9052, 0.8741, 0.8566, 0.9159, 0.8980, 0.8647, 0.9127,\n",
       "                       0.9175, 0.9101, 0.8745, 0.9091, 0.9022, 0.8406, 0.9461, 0.9058, 0.9571,\n",
       "                       0.8724, 0.9193, 0.8828, 0.9366, 0.9322, 0.8596, 0.9445, 0.8839, 0.9244,\n",
       "                       0.9476, 0.9270, 0.8953, 0.9314, 0.8321, 0.8759, 0.8974, 0.9431, 0.9116,\n",
       "                       0.8895, 0.8300, 0.8284], device='cuda:0')),\n",
       "              ('layer_norm_1.bias',\n",
       "               tensor([ 0.0594, -0.0102, -0.0603, -0.0499,  0.0143, -0.0030, -0.0007, -0.0037,\n",
       "                        0.0032, -0.0220, -0.0071,  0.0027, -0.0229, -0.0539,  0.0409,  0.0659,\n",
       "                       -0.0232, -0.0282,  0.0196,  0.0234, -0.0246, -0.0226, -0.0064, -0.0080,\n",
       "                       -0.0369,  0.0126, -0.0232,  0.0091, -0.0431, -0.0029,  0.0508,  0.0901,\n",
       "                        0.0102, -0.0286,  0.0303,  0.0037,  0.0186,  0.0644, -0.0780, -0.0263,\n",
       "                        0.0296,  0.0155,  0.0097,  0.0451, -0.0288,  0.0153, -0.0120, -0.0075,\n",
       "                        0.0731, -0.0092, -0.0126, -0.0109, -0.0212, -0.0231, -0.0780,  0.0581,\n",
       "                        0.0040,  0.0439,  0.0055,  0.0741,  0.0280, -0.0239, -0.0660, -0.0264,\n",
       "                        0.0029, -0.0118, -0.0026,  0.0140,  0.0465,  0.0548,  0.0050, -0.0038,\n",
       "                        0.0707, -0.0142, -0.0128, -0.0271,  0.0167,  0.0087, -0.0028,  0.0304,\n",
       "                        0.0500,  0.0184, -0.0274,  0.0305,  0.0622,  0.0455, -0.0330, -0.0153,\n",
       "                       -0.0474,  0.0188, -0.0837,  0.0476,  0.0041, -0.0030, -0.0191,  0.0249,\n",
       "                       -0.0553,  0.0226,  0.0780, -0.0199, -0.0002,  0.0303,  0.0554, -0.0059,\n",
       "                       -0.0510, -0.0206, -0.0179,  0.0185, -0.0596,  0.0207,  0.0135,  0.0046,\n",
       "                        0.0063, -0.0353, -0.0246,  0.0006, -0.0575,  0.0183, -0.0376,  0.0175,\n",
       "                        0.0313,  0.0199, -0.0136,  0.0312, -0.0197, -0.0358,  0.0216,  0.0562,\n",
       "                        0.0036,  0.0060, -0.0128, -0.0479, -0.0011,  0.0126,  0.0005, -0.0005,\n",
       "                        0.0251,  0.0215,  0.0048, -0.0186,  0.0101,  0.0415, -0.0513,  0.0278,\n",
       "                        0.0103, -0.0213, -0.0054,  0.0038,  0.0403,  0.1435, -0.0103, -0.0359,\n",
       "                       -0.0550,  0.0235, -0.0139, -0.0660,  0.0452,  0.0014,  0.1191,  0.0337,\n",
       "                       -0.0318, -0.0240, -0.0940,  0.0329,  0.0605,  0.0084,  0.0250, -0.0275,\n",
       "                       -0.0256,  0.0229,  0.0185, -0.0046, -0.0112,  0.0043,  0.0015,  0.0463,\n",
       "                       -0.0334, -0.0052, -0.0046,  0.0096, -0.0567,  0.0084, -0.0345,  0.0187,\n",
       "                       -0.0253,  0.0333, -0.0286, -0.0314,  0.0013,  0.0399, -0.0194,  0.0026,\n",
       "                       -0.0182, -0.0512, -0.0286, -0.0180, -0.0225, -0.0515, -0.0162, -0.1116,\n",
       "                        0.0056,  0.0513,  0.0310,  0.0315,  0.0042, -0.0874, -0.0128,  0.0265,\n",
       "                       -0.0200, -0.0117,  0.0089, -0.0568, -0.0142, -0.0566, -0.0123,  0.0248,\n",
       "                        0.0396, -0.0479, -0.0243, -0.0360, -0.0187, -0.0492,  0.0067,  0.0281,\n",
       "                       -0.0155,  0.0118,  0.0229, -0.0115,  0.0223, -0.0470, -0.0229,  0.0047,\n",
       "                        0.0447, -0.0339,  0.0035,  0.0055,  0.0113,  0.0777,  0.0623, -0.0033,\n",
       "                        0.0235,  0.0420,  0.0608,  0.0375, -0.0015,  0.0473, -0.0192, -0.0259,\n",
       "                        0.0272, -0.0835,  0.0384, -0.0301, -0.0060,  0.0503,  0.0342, -0.0355,\n",
       "                       -0.0752,  0.0174,  0.0251,  0.0249,  0.0697,  0.0226, -0.0537,  0.0326,\n",
       "                       -0.0278, -0.0451, -0.0746,  0.0558,  0.0585,  0.0484,  0.0243,  0.0103,\n",
       "                       -0.0117,  0.0037,  0.0392,  0.0082, -0.0373, -0.0296, -0.0064, -0.0064,\n",
       "                        0.0029, -0.0607,  0.0012,  0.0237, -0.0284, -0.0159, -0.0285, -0.0080,\n",
       "                       -0.0518, -0.0366, -0.0630, -0.0358, -0.0135,  0.0801, -0.0296,  0.0236,\n",
       "                       -0.0375, -0.0382, -0.0088, -0.0102], device='cuda:0')),\n",
       "              ('lstm_2.weight_ih_l0',\n",
       "               tensor([[ 0.0406, -0.0488, -0.0549,  ..., -0.0103, -0.0262, -0.0680],\n",
       "                       [-0.0001,  0.0242,  0.0934,  ..., -0.0106,  0.1038,  0.0471],\n",
       "                       [-0.1399, -0.0255, -0.0815,  ...,  0.0444,  0.0246, -0.0550],\n",
       "                       ...,\n",
       "                       [ 0.0187,  0.0079,  0.0621,  ...,  0.0484,  0.0081, -0.0022],\n",
       "                       [-0.0834,  0.0074, -0.0097,  ..., -0.0113,  0.0391,  0.0371],\n",
       "                       [ 0.0771, -0.0469,  0.0735,  ..., -0.0128, -0.0002,  0.0035]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_2.weight_hh_l0',\n",
       "               tensor([[ 0.0955,  0.1851, -0.0151,  ..., -0.1724,  0.0460,  0.1599],\n",
       "                       [ 0.0404, -0.1077, -0.0882,  ..., -0.0328, -0.0960, -0.0734],\n",
       "                       [ 0.0619, -0.0377,  0.1833,  ..., -0.0641,  0.1835, -0.1741],\n",
       "                       ...,\n",
       "                       [-0.1051,  0.1047, -0.1588,  ...,  0.0516, -0.1311, -0.1143],\n",
       "                       [-0.0752, -0.1642, -0.0163,  ..., -0.0715, -0.1424, -0.0877],\n",
       "                       [ 0.0470, -0.1680,  0.0903,  ...,  0.0969,  0.0463,  0.0687]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_2.bias_ih_l0',\n",
       "               tensor([ 0.0269, -0.0258,  0.0267,  ..., -0.0385, -0.0687, -0.0215], device='cuda:0')),\n",
       "              ('lstm_2.bias_hh_l0',\n",
       "               tensor([-0.0546, -0.0909, -0.0265,  ..., -0.0041, -0.0379,  0.0495], device='cuda:0')),\n",
       "              ('layer_norm_2.weight',\n",
       "               tensor([1.3472, 1.3043, 1.2815, 1.2917, 1.2877, 1.3567, 1.2544, 1.2886, 1.2310,\n",
       "                       1.4916, 1.2939, 1.0844, 1.3053, 1.3990, 1.4309, 1.3854, 1.2848, 1.3256,\n",
       "                       1.4582, 1.1984, 1.3294, 1.2040, 1.3192, 1.3334, 1.3112, 1.2462, 1.3119,\n",
       "                       1.4051, 1.3198, 1.4217, 1.2631, 1.4286, 1.2725, 1.3296, 1.2964, 1.2258,\n",
       "                       1.1079, 1.4222, 1.2156, 1.2612, 1.4168, 1.2584, 1.1886, 1.2519, 1.2604,\n",
       "                       1.2861, 1.3288, 1.4041, 1.3410, 1.5133, 1.4152, 1.3428, 1.3660, 1.2104,\n",
       "                       1.4170, 1.2173, 1.2762, 1.4679, 1.4524, 1.4023, 1.2053, 1.2809, 1.3101,\n",
       "                       1.2113, 1.4104, 1.4036, 1.2582, 1.1174, 1.1957, 1.2237, 1.4636, 1.5505,\n",
       "                       1.3042, 1.3876, 1.4110, 1.4967, 1.1990, 1.3759, 1.3317, 1.3154, 1.3037,\n",
       "                       1.4283, 1.6130, 1.1546, 1.0802, 1.3404, 1.2328, 1.0923, 1.2389, 1.1776,\n",
       "                       1.2526, 1.2318, 1.4678, 1.2271, 1.2375, 1.3171, 1.2014, 1.2493, 1.4635,\n",
       "                       1.5187, 1.3616, 1.3712, 1.2247, 1.4178, 1.2452, 1.2866, 1.3224, 1.2902,\n",
       "                       1.4252, 1.1857, 1.2025, 1.3542, 1.3591, 1.2077, 1.3526, 1.2971, 1.3428,\n",
       "                       1.2316, 1.5246, 1.2773, 1.3493, 1.2353, 1.1903, 1.2590, 1.0841, 1.2652,\n",
       "                       1.3166, 1.2909, 1.1721, 1.3747, 1.3839, 1.2970, 1.0960, 1.2144, 1.3665,\n",
       "                       1.3448, 1.4588, 1.3389, 1.1365, 1.1907, 1.5600, 1.2958, 1.1705, 1.2476,\n",
       "                       1.3667, 1.1767, 1.2329, 1.3445, 1.2425, 1.1407, 1.3372, 1.3572, 1.3644,\n",
       "                       1.2837, 1.2648, 1.3379, 1.3767, 1.5234, 1.2918, 1.1508, 1.1891, 1.3351,\n",
       "                       1.3347, 1.2921, 1.3132, 1.2109, 1.3691, 1.3340, 1.3742, 1.2873, 1.2030,\n",
       "                       1.1912, 1.2344, 1.2198, 1.4559, 1.2885, 1.3734, 1.1453, 1.3688, 1.3178,\n",
       "                       1.2969, 1.4911, 1.2132, 1.2808, 1.2452, 1.2268, 1.2940, 1.3899, 1.3923,\n",
       "                       1.3927, 1.3334, 1.3458, 1.2399, 1.2878, 1.4363, 1.3576, 1.2976, 1.1452,\n",
       "                       1.2574, 1.2756, 1.3383, 1.3665, 1.4278, 1.5779, 1.2215, 1.2115, 1.2540,\n",
       "                       1.3517, 1.2635, 1.3680, 1.4203, 1.2361, 1.2916, 1.2554, 1.3399, 1.1942,\n",
       "                       1.3574, 1.2323, 1.2683, 1.3024, 1.1601, 1.2087, 1.1352, 1.3794, 1.4862,\n",
       "                       1.1951, 1.4373, 1.2516, 1.1818, 1.3704, 1.2219, 1.3408, 1.3436, 1.2591,\n",
       "                       1.1825, 1.3391, 1.5431, 1.3516, 1.4411, 1.4726, 1.1781, 1.3290, 1.3727,\n",
       "                       1.4014, 1.3145, 1.2101, 1.3521, 1.2995, 1.1460, 1.2836, 1.3151, 1.2514,\n",
       "                       1.1670, 1.2712, 1.3035, 1.3385, 1.1979, 1.3938, 1.2948, 1.2257, 1.3043,\n",
       "                       1.2609, 1.3789, 1.1724, 1.3299, 1.1055, 1.3871, 1.3002, 1.3298, 1.2445,\n",
       "                       1.4175, 1.5683, 1.4752, 1.3691, 1.2996, 1.2581, 1.3572, 1.2273, 1.3553,\n",
       "                       1.3110, 1.2471, 1.4835, 1.1751, 1.4002, 1.1340, 1.2912, 1.2621, 1.2939,\n",
       "                       1.2949, 1.2056, 1.2973, 1.3057, 1.0697, 1.2437, 1.3341, 1.2662, 1.2382,\n",
       "                       1.3667, 1.2837, 1.3232], device='cuda:0')),\n",
       "              ('layer_norm_2.bias',\n",
       "               tensor([-0.0242, -0.0203, -0.0432, -0.0362,  0.1517,  0.0764, -0.0173, -0.0470,\n",
       "                        0.0266,  0.1461, -0.0178, -0.0551,  0.0570, -0.1281,  0.1007, -0.0407,\n",
       "                       -0.0780, -0.0659, -0.0591,  0.0007,  0.1203, -0.0299, -0.0450, -0.1465,\n",
       "                       -0.0295,  0.0763, -0.1634, -0.1788,  0.0654,  0.1375,  0.0664, -0.0648,\n",
       "                       -0.0711, -0.1482,  0.0312,  0.0751, -0.0185, -0.0169,  0.0596, -0.0632,\n",
       "                       -0.0098,  0.1006, -0.0490, -0.0379, -0.0270,  0.2139, -0.0518, -0.0393,\n",
       "                       -0.1412, -0.1074,  0.0699, -0.1220, -0.0540, -0.0343,  0.0148, -0.0188,\n",
       "                       -0.1522,  0.0879,  0.0837,  0.0606, -0.0564, -0.1029,  0.0935,  0.0048,\n",
       "                        0.1733, -0.0434,  0.0216,  0.0342,  0.0335,  0.0183,  0.0059,  0.1482,\n",
       "                        0.0940, -0.1563, -0.0304, -0.0701,  0.0106, -0.0392,  0.0202,  0.0841,\n",
       "                       -0.0636,  0.0776, -0.0128, -0.0182, -0.0159,  0.0662,  0.0096, -0.0463,\n",
       "                        0.0589,  0.0485,  0.0774,  0.0103, -0.0231,  0.0217, -0.0254, -0.1626,\n",
       "                       -0.0751, -0.0840, -0.1049,  0.1340, -0.1035, -0.0671, -0.0196, -0.1605,\n",
       "                        0.0746,  0.0657,  0.1787,  0.1262, -0.1468, -0.0728,  0.0184,  0.2005,\n",
       "                       -0.0743,  0.1013, -0.1154,  0.1439,  0.0936,  0.0507, -0.0010, -0.0651,\n",
       "                        0.1018, -0.0527,  0.0848,  0.0876,  0.0390, -0.0909,  0.0811,  0.0472,\n",
       "                        0.0053,  0.0981, -0.2282,  0.0836, -0.0335, -0.0307, -0.0793, -0.1060,\n",
       "                       -0.0945,  0.0592,  0.0519, -0.1020,  0.0456, -0.0035, -0.0149,  0.0226,\n",
       "                        0.0316, -0.0270, -0.0164, -0.0011,  0.0707,  0.0147,  0.0069,  0.0942,\n",
       "                       -0.0869, -0.0227,  0.1006, -0.0642, -0.1063, -0.0476, -0.0328,  0.0593,\n",
       "                        0.0531, -0.0475,  0.0551,  0.1136,  0.0050,  0.0197,  0.2163, -0.0720,\n",
       "                        0.1436,  0.0249, -0.0529, -0.0644, -0.1352, -0.0135, -0.2577,  0.0739,\n",
       "                       -0.2032,  0.0191, -0.0867, -0.2281,  0.0019, -0.0675, -0.0624, -0.0046,\n",
       "                        0.0852, -0.0054, -0.1481,  0.0229, -0.0463, -0.0939, -0.0577,  0.0951,\n",
       "                        0.0955,  0.0342, -0.1566, -0.0595,  0.1322, -0.0357,  0.0827,  0.0481,\n",
       "                       -0.1304,  0.0086, -0.0810, -0.0791, -0.0862,  0.0196,  0.0441,  0.0271,\n",
       "                       -0.0390, -0.0379, -0.1145,  0.0110,  0.1246, -0.0648,  0.0672,  0.0315,\n",
       "                        0.1562,  0.0318,  0.1250, -0.0607, -0.0037,  0.0076, -0.0321, -0.1952,\n",
       "                       -0.0632,  0.0473,  0.0098,  0.0710,  0.0160,  0.1015, -0.0678, -0.0028,\n",
       "                        0.1215,  0.0226,  0.0352, -0.1649, -0.0345,  0.0996, -0.0744, -0.0976,\n",
       "                        0.0927,  0.0220,  0.0252, -0.0868,  0.0923, -0.0018,  0.0840,  0.1782,\n",
       "                        0.0367, -0.1716,  0.0315, -0.0890,  0.0164,  0.1819, -0.1044, -0.0459,\n",
       "                       -0.0273,  0.1056, -0.0238,  0.0640, -0.0640, -0.0023, -0.0143, -0.1429,\n",
       "                       -0.0994,  0.0498,  0.0469, -0.1142,  0.0701, -0.0572, -0.1187, -0.0430,\n",
       "                        0.0551,  0.0479,  0.0883,  0.0216, -0.0892, -0.0538, -0.0312,  0.1360,\n",
       "                        0.0442,  0.1886, -0.0108, -0.2413, -0.0886, -0.0289, -0.0722,  0.1090,\n",
       "                       -0.1112,  0.0009, -0.0880,  0.0671, -0.0500, -0.0608, -0.0653,  0.0012,\n",
       "                       -0.1329,  0.0642,  0.0157, -0.0499], device='cuda:0')),\n",
       "              ('lstm_3.weight_ih_l0',\n",
       "               tensor([[-0.0940,  0.0959,  0.0964,  ...,  0.0680,  0.0361, -0.0940],\n",
       "                       [ 0.0175, -0.0724,  0.0739,  ..., -0.0080, -0.1217,  0.0738],\n",
       "                       [ 0.2120, -0.0236,  0.0637,  ..., -0.0458, -0.0177, -0.1880],\n",
       "                       ...,\n",
       "                       [ 0.0438,  0.1002, -0.0683,  ..., -0.0597,  0.1692, -0.1014],\n",
       "                       [-0.0140, -0.0040, -0.0948,  ...,  0.0849, -0.1711, -0.0545],\n",
       "                       [ 0.0589, -0.1714, -0.0101,  ...,  0.1255,  0.1611,  0.0925]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_3.weight_hh_l0',\n",
       "               tensor([[ 0.0011,  0.0069,  0.2098,  ...,  0.0558,  0.1296,  0.0969],\n",
       "                       [-0.1337,  0.2094, -0.0862,  ...,  0.1432, -0.0665,  0.1223],\n",
       "                       [-0.1390,  0.0987, -0.1265,  ..., -0.3902, -0.0656, -0.0872],\n",
       "                       ...,\n",
       "                       [-0.1358, -0.1095, -0.1558,  ..., -0.0503,  0.0271,  0.1253],\n",
       "                       [-0.0001, -0.1839,  0.2957,  ...,  0.0808,  0.0387, -0.0956],\n",
       "                       [ 0.1856, -0.0443, -0.1145,  ...,  0.3614, -0.0825, -0.2965]],\n",
       "                      device='cuda:0')),\n",
       "              ('lstm_3.bias_ih_l0',\n",
       "               tensor([ 0.0371,  0.0523,  0.0292,  ..., -0.0032,  0.0054,  0.0852], device='cuda:0')),\n",
       "              ('lstm_3.bias_hh_l0',\n",
       "               tensor([-0.0102,  0.0570,  0.0221,  ...,  0.0671,  0.0003,  0.1160], device='cuda:0')),\n",
       "              ('layer_norm_3.weight',\n",
       "               tensor([2.7883, 2.5522, 2.4310, 2.8497, 2.5681, 2.7070, 2.8433, 2.6859, 2.6295,\n",
       "                       2.8635, 2.4936, 2.4018, 2.0697, 2.7693, 2.5033, 2.1476, 2.7068, 2.7473,\n",
       "                       2.2899, 2.3561, 2.5293, 2.6096, 2.7271, 2.2159, 2.7854, 2.4322, 2.9037,\n",
       "                       2.9515, 2.4162, 2.9456, 2.7342, 2.9059, 2.1921, 2.7194, 2.4505, 2.8213,\n",
       "                       2.5797, 2.6829, 2.3775, 2.6387, 2.6272, 2.8114, 2.8621, 2.6602, 2.4542,\n",
       "                       2.8722, 2.3392, 2.8328, 2.8730, 2.2097, 2.4675, 2.8692, 2.5032, 2.7870,\n",
       "                       2.7614, 2.5183, 2.5330, 2.7465, 2.0383, 2.3333, 2.6900, 2.8869, 2.8935,\n",
       "                       2.6584, 2.7502, 2.3802, 2.7533, 2.8527, 2.3815, 2.7896, 2.8713, 2.4777,\n",
       "                       2.0963, 2.6609, 2.9467, 2.4246, 2.7686, 2.3936, 2.7290, 2.7139, 2.6561,\n",
       "                       2.8478, 2.7785, 2.7300, 2.3188, 2.2182, 2.8554, 2.9575, 2.9581, 2.8247,\n",
       "                       1.9482, 2.8306, 2.8270, 2.6165, 2.9828, 2.5976, 2.4584, 2.6343, 2.8051,\n",
       "                       2.8467, 2.8659, 2.5169, 2.4542, 2.9912, 3.1427, 1.7998, 2.9524, 2.6432,\n",
       "                       2.5398, 2.4288, 2.4324, 2.1336, 2.5462, 2.7362, 2.9156, 2.6978, 2.1972,\n",
       "                       2.1120, 2.7614, 2.4156, 2.7685, 2.7457, 2.6125, 2.8174, 2.7500, 2.6852,\n",
       "                       2.6735, 2.4590, 3.0016, 2.2400, 2.8122, 2.6888, 2.2019, 2.8406, 2.6646,\n",
       "                       2.6043, 2.8433, 2.5255, 2.2161, 2.6228, 2.8011, 1.8037, 2.7250, 2.6837,\n",
       "                       2.3893, 2.9084, 2.3282, 2.1835, 2.8824, 2.5550, 2.6533, 2.6377, 2.4841,\n",
       "                       2.1650, 2.9140, 2.6888, 2.7400, 2.6853, 2.4363, 2.6705, 2.1666, 2.7530,\n",
       "                       2.5125, 2.6265, 2.7154, 2.8271, 2.8055, 2.8262, 2.2834, 2.5567, 2.8500,\n",
       "                       2.7104, 2.6496, 2.6284, 2.2426, 2.6731, 2.9390, 2.9525, 2.7948, 2.9613,\n",
       "                       2.5340, 2.5897, 2.3071, 2.4779, 2.9624, 3.0232, 2.3781, 2.9720, 2.7177,\n",
       "                       2.4452, 2.8711, 2.8742, 2.3634, 2.7330, 2.7268, 2.7653, 2.6706, 2.6420,\n",
       "                       2.7644, 2.8561, 2.5996, 2.1235, 2.4065, 2.5482, 1.9280, 2.7185, 3.0686,\n",
       "                       2.5250, 2.7723, 2.7855, 2.6435, 2.9469, 2.7096, 2.6117, 2.4887, 2.6531,\n",
       "                       2.5079, 2.3532, 1.7350, 2.0474, 3.0015, 2.8583, 2.7071, 2.3141, 2.6471,\n",
       "                       2.8524, 2.9844, 2.6760, 2.7185, 2.3284, 2.6455, 2.8885, 2.5795, 3.0156,\n",
       "                       2.7165, 2.2451, 2.6247, 2.4469, 2.5634, 2.7109, 2.9012, 2.3392, 2.6322,\n",
       "                       2.0088, 2.9649, 3.0002, 2.3729, 2.0641, 2.5071, 2.6903, 2.4753, 2.3698,\n",
       "                       2.2828, 2.5823, 2.6145, 2.7452, 2.7563, 2.6268, 2.8376, 2.9417, 2.8482,\n",
       "                       2.8185, 2.7389, 2.7397, 2.4141, 3.0406, 2.4327, 2.5545, 2.8772, 2.4340,\n",
       "                       2.4370, 2.9923, 2.8364, 2.7705, 2.7524, 2.3981, 2.3715, 2.6566, 2.8455,\n",
       "                       2.7699, 2.7343, 2.7872, 2.7487, 2.3492, 2.7688, 2.5923, 3.0401, 2.5790,\n",
       "                       2.8631, 2.6564, 2.6596, 2.3437, 2.4311, 2.7251, 2.8325, 2.9482, 2.2630,\n",
       "                       2.7902, 2.7267, 2.6808], device='cuda:0')),\n",
       "              ('layer_norm_3.bias',\n",
       "               tensor([ 0.1425, -0.5524,  0.0843,  0.8817, -0.1620, -0.4896,  0.0830, -0.0943,\n",
       "                       -0.3905, -0.4043, -0.7885, -0.0667,  0.5991, -0.3978,  0.6458, -0.3397,\n",
       "                        0.5827, -0.0379,  0.6976,  0.5962, -0.1757, -0.4135,  0.8489, -0.2174,\n",
       "                        0.4945,  0.4505,  0.6889, -0.7360, -0.4205, -0.6097, -0.4440, -0.7850,\n",
       "                       -0.3791, -0.7039,  0.3785,  0.3011,  0.1019,  0.4643, -0.2878, -0.2193,\n",
       "                        0.6906, -0.1288, -0.8689, -0.5974, -0.0206, -0.8563,  0.1902, -0.9503,\n",
       "                        0.4138,  0.4478, -0.8154, -0.3519, -0.1948, -0.8473,  0.8321, -0.6150,\n",
       "                        0.6469, -0.8593, -0.3980, -0.5336, -0.4185, -0.5737,  0.7094, -0.8144,\n",
       "                        0.7793, -0.3330,  0.0545, -0.5398,  0.1911,  0.0050, -0.2493, -0.4847,\n",
       "                       -0.2353,  0.1094,  0.8469, -0.3013,  0.4967, -0.5164, -0.0601,  0.6050,\n",
       "                        0.5559, -0.2731,  0.2577,  1.0303, -0.5883,  0.3268, -0.8371,  0.7946,\n",
       "                        0.6122,  0.1265,  0.0981, -0.6675, -0.1342,  0.7321,  0.6970, -0.1636,\n",
       "                        0.7583,  0.4128,  0.3168, -0.1880, -0.5812,  0.0676, -0.5229,  0.7794,\n",
       "                        0.0604, -0.6127,  0.2009,  0.7788,  0.5906, -0.1321,  0.5521,  0.4178,\n",
       "                       -0.2870,  0.3734, -0.5712,  0.3607, -0.8039,  0.5326, -0.2173, -0.2515,\n",
       "                       -0.3504,  0.2236,  0.8810,  0.4998, -0.2552, -0.2873,  0.0954, -0.1879,\n",
       "                       -0.6619,  0.5317, -0.7396, -0.5891,  0.7221,  0.4027, -0.2689,  0.8346,\n",
       "                        0.5703, -0.3246, -0.5608,  0.2719, -0.4466, -0.4832, -0.7117,  0.2787,\n",
       "                        0.7326, -0.6048, -0.6448, -0.1934,  0.5902, -0.6206, -0.6969, -0.6402,\n",
       "                       -0.5729, -0.4710,  0.6719, -0.8497, -0.7341,  0.2714, -0.6083,  0.4820,\n",
       "                       -0.0606,  0.5424, -0.5985,  0.2545, -0.1940, -0.4835,  0.6129, -0.4119,\n",
       "                        1.0335, -0.6494, -0.5443, -0.1887, -0.6397,  0.5773, -0.5706,  0.5102,\n",
       "                        0.7985,  0.8454, -0.6271,  0.8083, -0.0703,  0.2310,  0.3374,  0.7223,\n",
       "                        0.4290,  0.6106, -0.0186, -0.7208, -0.5856,  0.8839, -0.6936,  0.7261,\n",
       "                       -0.5238, -0.2077, -0.4005, -0.9467, -0.0159, -0.2294, -0.8327, -0.8178,\n",
       "                        0.4471, -0.5239, -0.0319,  0.3094, -0.1171, -0.0015, -0.9903,  0.2408,\n",
       "                        0.6613,  0.5797, -0.4104,  0.6322, -0.7361,  0.8464, -0.3049,  0.5982,\n",
       "                       -0.1537,  0.2086,  0.5773,  0.8168,  0.9739, -0.4168,  0.5612, -0.0815,\n",
       "                       -0.7732, -0.0077,  0.7330,  0.7261,  0.1814, -0.6230, -0.6786,  0.0310,\n",
       "                       -0.7130,  1.0236, -0.2938, -0.7463,  0.6415, -0.1202,  0.4466,  0.5648,\n",
       "                        0.7070, -0.8606, -0.7388,  0.7533,  0.7319, -0.7396, -0.7841,  0.8298,\n",
       "                       -0.4440,  0.6114, -0.4680,  0.1059,  0.0442,  0.3383,  0.0259,  0.7640,\n",
       "                       -0.9628,  0.4837,  0.7621, -0.4009,  0.5841,  0.1010,  0.3524,  0.0884,\n",
       "                       -0.0792, -0.6196,  0.2516,  0.3982,  0.7226, -0.7259,  0.5531,  0.7048,\n",
       "                        0.4890,  0.4585, -0.9787, -0.5732,  0.5222, -0.5443, -0.2158,  0.7595,\n",
       "                       -0.5173,  0.2871,  0.2129,  0.7144,  0.0518, -0.0215,  0.4639, -0.4501,\n",
       "                        0.7871, -0.7626,  0.5670,  0.4003, -0.6493, -0.6445, -0.2397,  0.6438,\n",
       "                       -0.3883, -0.0655, -0.5928,  0.7917], device='cuda:0')),\n",
       "              ('to_notes.weight',\n",
       "               tensor([[ 0.3264, -0.3199,  0.0398,  ..., -0.1633, -0.1324, -0.1690],\n",
       "                       [ 0.0098,  0.0943, -0.0536,  ..., -0.0477, -0.0768,  0.0073],\n",
       "                       [-0.0529,  0.0374,  0.0408,  ...,  0.0103,  0.0099,  0.1209],\n",
       "                       ...,\n",
       "                       [-0.0688,  0.0445, -0.0454,  ..., -0.0992, -0.0448,  0.0808],\n",
       "                       [-0.0023,  0.0179, -0.0478,  ..., -0.0490,  0.0286,  0.0734],\n",
       "                       [ 0.0137,  0.0552, -0.0418,  ..., -0.0020,  0.0419,  0.0564]],\n",
       "                      device='cuda:0')),\n",
       "              ('to_notes.bias',\n",
       "               tensor([-0.1189, -0.0514, -0.0669, -0.0486, -0.0344, -0.0842, -0.0602, -0.0904,\n",
       "                       -0.0612, -0.0809, -0.0600, -0.0878, -0.0309, -0.0795, -0.0673, -0.0413,\n",
       "                       -0.0927, -0.1021, -0.0367, -0.0699, -0.0956, -0.1147, -0.1790, -0.1449,\n",
       "                       -0.2080, -0.1739, -0.1845, -0.1764, -0.1948, -0.2101, -0.2100, -0.2567,\n",
       "                       -0.1790, -0.1731, -0.1994, -0.2511, -0.2587, -0.1510, -0.1871, -0.2197,\n",
       "                       -0.2023, -0.2735, -0.1962, -0.2361, -0.1938, -0.2359, -0.2702, -0.2281,\n",
       "                       -0.2304, -0.2839, -0.2398, -0.2827, -0.2252, -0.2339, -0.2245, -0.2670,\n",
       "                       -0.2689, -0.2385, -0.2201, -0.2163, -0.2685, -0.1457, -0.2814, -0.2321,\n",
       "                       -0.2825, -0.2176, -0.2031, -0.2859, -0.2043, -0.2659, -0.1843, -0.2591,\n",
       "                       -0.2239, -0.2577, -0.2408, -0.1963, -0.1892, -0.2539, -0.2782, -0.2387,\n",
       "                       -0.2093, -0.1871, -0.1794, -0.2475, -0.2714, -0.2696, -0.1934, -0.2811,\n",
       "                       -0.3017, -0.3244, -0.2128, -0.2739, -0.2354, -0.3035, -0.1475, -0.1495,\n",
       "                       -0.2294, -0.1493, -0.1481, -0.1558, -0.1736, -0.0850, -0.1604, -0.0422,\n",
       "                       -0.0981, -0.0316, -0.0838, -0.1069, -0.0852, -0.0971, -0.0382, -0.0827,\n",
       "                       -0.0523, -0.0786, -0.0999, -0.0760, -0.1082, -0.0460, -0.0523, -0.0635,\n",
       "                       -0.0872, -0.0923, -0.0530, -0.0722, -0.0558, -0.0427, -0.0593, -0.0872,\n",
       "                       -0.0717], device='cuda:0'))]),\n",
       " 'opt_dict': <bound method Optimizer.state_dict of Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     initial_lr: 0.001\n",
       "     lr: 0.0005000000000000002\n",
       "     weight_decay: 0\n",
       " )>}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_lstm.cuda()\n",
    "# state = torch.load('./temp_checkpoint.pth', map_location='cuda')\n",
    "# state = torch.load('./3temp_checkpoint.pth', map_location='cuda')\n",
    "# state = torch.load('./best.pth', map_location='cuda')\n",
    "state = torch.load('./3best_recall.pth', map_location='cuda')\n",
    "my_lstm.load_state_dict(state['state_dict'])\n",
    "# optimizer.load_state_dict(state['opt_dict'])\n",
    "epoch = state['epoch']\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0, 56, 63, 75, 87, 0]\n"
     ]
    }
   ],
   "source": [
    "generated_notes = [MyDataset(1, random_choice=False)[i][0].argmax().item() for i in range(500)]\n",
    "# generated_notes = [70, 70, 70, 0, 70, 70, 70, 0, 69, 80]\n",
    "start_len = len(generated_notes)\n",
    "\n",
    "my_lstm.eval()\n",
    "my_lstm.cuda()\n",
    "\n",
    "hidden = my_lstm.init_hidden(batch_size=1, cuda=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(1000):\n",
    "        input = torch.zeros(1, 1, 129, device='cuda')\n",
    "        input.data[0, 0, generated_notes[-1]] = 1\n",
    "\n",
    "        output, hidden = my_lstm(input, hidden, temperature=3.)\n",
    "#         print(output)\n",
    "\n",
    "        prediction = torch.multinomial(output.exp().view(1, 129), 1).item()\n",
    "        generated_notes.append(prediction)\n",
    "    torch.cuda.empty_cache()\n",
    "print(generated_notes[start_len:])\n",
    "# print(generated_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('./generated3.txt', 'w', encoding='utf-8').write(''.join([chr(ord('\\n') + num) for num in generated_notes[start_len:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68, 0, 0, 0, 0, 0, 0, 0, 54, 73, 0, 54, 73, 0, 54, 73, 0, 54, 73, 0, 54, 73, 0, 54, 73, 0, 54, 73, 0, 54, 73, 0, 54, 73, 0, 54, 73, 0, 54, 73, 0, 54, 73, 54, 0, 0, 54, 85, 0, 54, 85, 0, 54, 85, 0, 54, 85, 0, 54, 85, 0, 54, 85, 0, 54, 85, 0, 54, 85, 0, 54, 85, 0, 54, 85, 0, 54, 85, 0, 54, 85, 0, 54, 61, 80, 0, 58, 80, 0, 58, 80, 0, 58, 80, 0, 58, 80, 0, 58, 80]\n"
     ]
    }
   ],
   "source": [
    "my_lstm.eval()\n",
    "with torch.no_grad():\n",
    "    my_lstm.cuda()\n",
    "    predicted = []\n",
    "    hidden = my_lstm.init_hidden(1, True)\n",
    "    # count = 0\n",
    "    process_length = 10000\n",
    "    dataset_p = MyDataset(-1, total_batch=1, step_interval=1, random_choice=False)[0][0]\n",
    "    for data in [dataset_p[i * process_length : (i + 1) * process_length] for i in range(0, torch.ceil(torch.tensor(len(dataset_p) / process_length)).to(torch.int).item(), 1)]:\n",
    "        output, hidden = my_lstm(data.unsqueeze(0).cuda(), hidden)\n",
    "        predicted.extend(output.argmax(dim=2).view(-1).cpu().tolist())\n",
    "    #     count += 1\n",
    "    #     if count == 100:\n",
    "    #         break\n",
    "    my_lstm.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(predicted[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457497"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('./predicted3.txt', 'w', encoding='utf-8').write(''.join([chr(ord('\\n') + num) for num in predicted]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {1540034079624: {'momentum_buffer': tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "          device='cuda:0')},\n",
       "  1540034144368: {'momentum_buffer': tensor([[ 2.3477e-09, -1.0219e-08, -2.2433e-09,  ..., -1.3613e-08,\n",
       "            -3.7946e-09, -8.5903e-09],\n",
       "           [-3.0808e-09,  1.2355e-07,  5.8675e-08,  ...,  1.5693e-07,\n",
       "             5.4997e-08,  1.0782e-07],\n",
       "           [ 1.1489e-10,  5.5751e-08,  2.4654e-08,  ...,  7.2181e-08,\n",
       "             2.4321e-08,  5.2192e-08],\n",
       "           ...,\n",
       "           [-4.2990e-09,  1.7401e-07,  8.7198e-08,  ...,  2.2825e-07,\n",
       "             7.8105e-08,  1.4732e-07],\n",
       "           [-2.9213e-09,  3.4913e-09, -4.6304e-09,  ...,  3.0620e-09,\n",
       "             7.5239e-10,  1.4272e-09],\n",
       "           [ 2.7927e-09,  2.8372e-08,  1.9073e-08,  ...,  3.3195e-08,\n",
       "             1.1795e-08,  2.0385e-08]], device='cuda:0')},\n",
       "  1540034144440: {'momentum_buffer': tensor([ 3.5879e-07, -4.1968e-06, -1.8839e-06,  ..., -5.7946e-06,\n",
       "           -1.3709e-07, -7.9081e-07], device='cuda:0')},\n",
       "  1540034144512: {'momentum_buffer': tensor([ 3.5879e-07, -4.1968e-06, -1.8839e-06,  ..., -5.7946e-06,\n",
       "           -1.3709e-07, -7.9081e-07], device='cuda:0')},\n",
       "  1540034144584: {'momentum_buffer': tensor([[-6.1279e-08,  9.5652e-07,  4.3492e-07,  ...,  1.1748e-06,\n",
       "             4.1072e-07,  8.0328e-07],\n",
       "           [-6.1614e-10, -3.1824e-09, -6.3440e-09,  ..., -9.9709e-09,\n",
       "            -6.8312e-10, -7.9438e-11],\n",
       "           [-1.7365e-08,  7.4417e-07,  3.5070e-07,  ...,  9.5994e-07,\n",
       "             3.6514e-07,  6.0955e-07],\n",
       "           ...,\n",
       "           [ 3.0115e-09,  4.2092e-08,  1.8202e-08,  ...,  4.4674e-08,\n",
       "             2.0191e-08,  3.3122e-08],\n",
       "           [-4.1313e-08,  5.3741e-07,  3.6508e-07,  ...,  7.8714e-07,\n",
       "             2.9069e-07,  4.7668e-07],\n",
       "           [ 1.1679e-08,  2.3951e-07,  9.0523e-08,  ...,  3.0682e-07,\n",
       "             1.0992e-07,  1.8657e-07]], device='cuda:0')},\n",
       "  1540034144656: {'momentum_buffer': tensor([[-1.6514e-06,  3.8600e-07, -1.7369e-06,  ..., -1.7120e-07,\n",
       "            -1.8922e-06,  1.1764e-06],\n",
       "           [ 2.3872e-08, -1.3194e-08,  2.2020e-08,  ..., -8.4197e-10,\n",
       "             2.3992e-08, -8.9695e-09],\n",
       "           [-1.3415e-06,  3.1157e-07, -1.3888e-06,  ..., -1.3138e-07,\n",
       "            -1.5153e-06,  9.2883e-07],\n",
       "           ...,\n",
       "           [-7.7832e-08,  1.4234e-08, -8.0659e-08,  ..., -1.1092e-08,\n",
       "            -8.9286e-08,  5.3490e-08],\n",
       "           [-9.3802e-07,  2.6224e-07, -1.0065e-06,  ..., -5.8819e-08,\n",
       "            -1.0583e-06,  6.8873e-07],\n",
       "           [-4.5737e-07,  1.0546e-07, -4.7259e-07,  ..., -5.3526e-08,\n",
       "            -5.2152e-07,  3.1034e-07]], device='cuda:0')},\n",
       "  1540034144728: {'momentum_buffer': tensor([-3.1645e-05,  8.6143e-08, -2.4749e-05,  ..., -1.2263e-06,\n",
       "           -2.1293e-05, -7.9506e-06], device='cuda:0')},\n",
       "  1540034144800: {'momentum_buffer': tensor([-3.1645e-05,  8.6143e-08, -2.4749e-05,  ..., -1.2263e-06,\n",
       "           -2.1293e-05, -7.9506e-06], device='cuda:0')},\n",
       "  1540034144872: {'momentum_buffer': tensor([[ 1.7403e-07, -1.1285e-07,  2.0492e-07,  ...,  2.8439e-08,\n",
       "             2.5519e-07, -1.0123e-07],\n",
       "           [-1.0644e-05,  2.0596e-06, -1.0633e-05,  ..., -1.0069e-06,\n",
       "            -1.1892e-05,  6.9396e-06],\n",
       "           [-8.3317e-06,  1.4158e-06, -8.1119e-06,  ..., -8.6152e-07,\n",
       "            -9.0565e-06,  5.1324e-06],\n",
       "           ...,\n",
       "           [-1.2756e-05,  2.3380e-06, -1.2195e-05,  ..., -1.1748e-06,\n",
       "            -1.3663e-05,  7.7325e-06],\n",
       "           [-9.4461e-06,  1.3018e-06, -8.6726e-06,  ..., -6.4295e-07,\n",
       "            -9.8133e-06,  5.2800e-06],\n",
       "           [-1.5347e-06,  2.4663e-07, -1.3488e-06,  ..., -9.6329e-08,\n",
       "            -1.1895e-06,  7.9122e-07]], device='cuda:0')},\n",
       "  1540034144944: {'momentum_buffer': tensor([[-4.4538e-07, -2.0402e-06, -1.2360e-06,  ...,  1.6133e-06,\n",
       "             1.2827e-06,  8.0263e-07],\n",
       "           [ 1.0018e-05,  5.7473e-05,  3.2488e-05,  ..., -4.3058e-05,\n",
       "            -3.0004e-05, -2.2616e-05],\n",
       "           [ 9.1009e-06,  4.8479e-05,  2.8088e-05,  ..., -3.7341e-05,\n",
       "            -2.7302e-05, -1.9117e-05],\n",
       "           ...,\n",
       "           [ 1.3949e-05,  7.3513e-05,  4.2603e-05,  ..., -5.6826e-05,\n",
       "            -4.1790e-05, -2.9357e-05],\n",
       "           [ 1.0345e-05,  5.3722e-05,  3.1190e-05,  ..., -4.1592e-05,\n",
       "            -3.0889e-05, -2.1378e-05],\n",
       "           [ 9.4240e-07,  5.7216e-06,  3.1751e-06,  ..., -4.2114e-06,\n",
       "            -2.7809e-06, -2.2606e-06]], device='cuda:0')},\n",
       "  1540034145016: {'momentum_buffer': tensor([ 3.5084e-06, -1.6653e-04, -1.1427e-04,  ..., -1.7089e-04,\n",
       "           -1.1405e-04, -2.0594e-05], device='cuda:0')},\n",
       "  1540034145088: {'momentum_buffer': tensor([ 3.5084e-06, -1.6653e-04, -1.1427e-04,  ..., -1.7089e-04,\n",
       "           -1.1405e-04, -2.0594e-05], device='cuda:0')},\n",
       "  1540034145232: {'momentum_buffer': tensor([[ 0.0001,  0.0004,  0.0002,  ..., -0.0003, -0.0000, -0.0001],\n",
       "           [-0.0001, -0.0005, -0.0003,  ...,  0.0004,  0.0002,  0.0002],\n",
       "           [-0.0001, -0.0005, -0.0003,  ...,  0.0004,  0.0002,  0.0002],\n",
       "           ...,\n",
       "           [-0.0001, -0.0005, -0.0003,  ...,  0.0004,  0.0002,  0.0002],\n",
       "           [-0.0001, -0.0004, -0.0003,  ...,  0.0003,  0.0002,  0.0002],\n",
       "           [-0.0001, -0.0005, -0.0003,  ...,  0.0004,  0.0002,  0.0002]],\n",
       "          device='cuda:0')},\n",
       "  1540034145304: {'momentum_buffer': tensor([-1.4065e-02,  2.2390e-03,  2.1391e-03,  2.3432e-03,  2.1761e-03,\n",
       "            2.2698e-03,  2.1569e-03,  2.0372e-03,  2.1688e-03,  2.0679e-03,\n",
       "            2.2438e-03,  2.1413e-03,  2.0521e-03,  2.2113e-03,  2.2392e-03,\n",
       "            2.0433e-03,  2.1363e-03,  2.3010e-03,  2.2988e-03,  2.1854e-03,\n",
       "            2.1443e-03,  2.1638e-03,  2.2030e-03,  2.3096e-03,  2.3904e-03,\n",
       "            2.3556e-03,  2.3487e-03,  2.3335e-03,  2.3247e-03,  2.1023e-03,\n",
       "            2.3205e-03,  2.2292e-03,  2.0791e-03,  2.1108e-03,  1.5584e-03,\n",
       "            1.4728e-03,  2.3107e-03,  1.6325e-03,  2.1455e-03,  1.4094e-03,\n",
       "            2.3312e-03,  2.0802e-03,  2.3033e-03,  1.5248e-03,  1.3634e-03,\n",
       "            1.9553e-03, -3.2400e-04,  1.4680e-03,  7.8192e-04,  1.6456e-03,\n",
       "            1.5433e-03,  2.0372e-04,  2.3569e-03,  2.0097e-03, -4.6508e-03,\n",
       "            2.0680e-03, -3.1441e-03, -1.0174e-03, -2.1649e-03, -2.4773e-04,\n",
       "            2.4572e-04,  1.7494e-03,  1.5606e-03, -1.8033e-03, -1.9133e-03,\n",
       "           -1.4612e-04, -3.0065e-03, -3.3468e-03, -2.8975e-03,  1.0549e-03,\n",
       "           -3.0897e-03, -5.9114e-05, -1.0377e-03, -3.9867e-03, -2.5834e-03,\n",
       "            5.9910e-06,  1.0523e-03,  1.5559e-04, -3.9107e-03, -3.3816e-03,\n",
       "           -3.4894e-03,  1.7779e-03,  1.5191e-03,  7.1093e-04,  2.0766e-03,\n",
       "           -3.1008e-03,  1.5452e-03,  1.7224e-03,  1.0039e-03,  2.3285e-03,\n",
       "            2.1773e-03, -2.0132e-03, -1.8181e-03,  1.8549e-03,  2.2550e-03,\n",
       "            2.1494e-03,  2.2090e-03,  2.1559e-03,  2.0483e-03,  2.1471e-03,\n",
       "            2.3656e-03,  2.1305e-03,  2.0671e-03,  2.2113e-03,  2.3483e-03,\n",
       "            2.3731e-03,  2.3075e-03,  2.3093e-03,  2.1930e-03,  2.1000e-03,\n",
       "            2.2962e-03,  2.1785e-03,  2.2340e-03,  2.1595e-03,  2.2244e-03,\n",
       "            2.2972e-03,  2.2061e-03,  2.2369e-03,  2.1829e-03,  2.1940e-03,\n",
       "            2.1320e-03,  2.0711e-03,  2.4784e-03,  2.1928e-03,  2.1986e-03,\n",
       "            2.3252e-03,  2.2368e-03,  2.1255e-03,  2.2290e-03], device='cuda:0')}},\n",
       " 'param_groups': [{'lr': 1.0,\n",
       "   'momentum': 0.9,\n",
       "   'dampening': 0,\n",
       "   'weight_decay': 0,\n",
       "   'nesterov': True,\n",
       "   'params': [1540034079624,\n",
       "    1540034144368,\n",
       "    1540034144440,\n",
       "    1540034144512,\n",
       "    1540034144584,\n",
       "    1540034144656,\n",
       "    1540034144728,\n",
       "    1540034144800,\n",
       "    1540034144872,\n",
       "    1540034144944,\n",
       "    1540034145016,\n",
       "    1540034145088,\n",
       "    1540034145232,\n",
       "    1540034145304]}]}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipLSTM(\n",
      "  (layer_norm_0): LayerNorm(torch.Size([129]), eps=1e-05, elementwise_affine=True)\n",
      "  (lstm_1): LSTM(129, 300, batch_first=True)\n",
      "  (layer_norm_1): LayerNorm(torch.Size([300]), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout_1): Dropout(p=0.3)\n",
      "  (lstm_2): LSTM(429, 300, batch_first=True)\n",
      "  (layer_norm_2): LayerNorm(torch.Size([300]), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout_2): Dropout(p=0.3)\n",
      "  (lstm_3): LSTM(600, 300, batch_first=True)\n",
      "  (layer_norm_3): LayerNorm(torch.Size([300]), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout_3): Dropout(p=0.3)\n",
      "  (to_notes): Linear(in_features=600, out_features=129, bias=True)\n",
      "  (activation): Sigmoid()\n",
      "  (to_out): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(my_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
